training_config:
  seed: 1921
  output_dir: './output'
  training_iters: 10
  log_interval: 1
  eval_interval: null
  save_interval: 5
  loss_scale: 1
  loss_reduction: "mean"
  loss_func_kwargs:
    loss_func_type: "CrossEntropyLoss"
  resume_training: False
  crc_check: False
  ckpt_format: "ckpt"
  prefix: "network"
  load_checkpoint: ""
  enable_compile_cache: False
  compile_cache_path: "./output/compile_cache"
  keep_checkpoint_max: 3

dataset_config:
  batch_size: 1
  micro_batch_num: 1
  dataset_dir: 'data'
  shuffle: False
  drop_remainder: True
  pad_token: 0

parallel_config:
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 2
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: True

model_config:
  num_layers: 2
  seq_length: 4
  num_attention_heads: 16
  hidden_size: 32
  ffn_hidden_size: 128
  vocab_size: 64
  group_query_attention: True
  num_query_groups: 8
  attention_type: 'self_attn'
  qkv_has_bias: False
  out_proj_has_bias: False
  params_dtype: "float32"
  init_method: 'normal'
  compute_dtype: "float32"
  softmax_compute_dtype: "float32"
  hidden_dropout: 0.0
  attention_dropout: 0.0
  mask_func_type: "attn_mask_fill"
  mlp_has_bias: False
  gated_linear_unit: True
  hidden_act: 'silu'
  apply_residual_connection_post_norm: False
  normalization: 'FusedRMSNorm'
  norm_epsilon: 1.e-5
  untie_embeddings_and_output_weights: True
  position_embedding_type: 'rope'

moe_config:
  num_experts: 1
  moe_router_topk: 2
  moe_token_dispatcher_type: 'alltoall'
  moe_z_loss_coeff: 1.e-3
  moe_aux_loss_coeff: 1.e-2
  moe_router_load_balancing_type: 'aux_loss' # ['none', 'aux_loss']
  moe_input_noise_eps: null
  moe_expert_capacity_factor: -1
  moe_token_drop_policy: null
  moe_pad_expert_input_to_capacity: False
  use_self_defined_alltoall: False

optimizer_config:
  learning_rate: 0.0009
  lr_decay_iters: 7000
  lr_wsd_decay_iters: 7000
  lr_warmup_fraction: 0.0005
  lr_warmup_iters: 10000
  lr_decay_samples: 7000
  lr_wsd_decay_samples: 7000
  lr_warmup_samples: 10000
  lr_warmup_init: 0.0009
  min_lr: 0.00001
  lr_decay_style: "WSD"
  start_weight_decay: 0.00001
  end_weight_decay: 0.0009
  weight_decay_incr_style: "cosine"
  use_checkpoint_opt_param_scheduler: False
  override_opt_param_scheduler: True
  lr_wsd_decay_style: "exponential"
