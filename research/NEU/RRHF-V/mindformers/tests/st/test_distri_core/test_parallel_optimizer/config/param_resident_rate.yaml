training_config:
  seed: 2024

parallel_config:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: False
  zero_level: "z3"

model_config:
  vocab_size: 1
  num_layers: 1
  num_attention_heads: 32
  hidden_size: 2048
  attn_type: 'self_attn'
  qkv_has_bias: True
  out_proj_has_bias: False
  params_dtype: float32
  compute_dtype: float32
  softmax_compute_dtype: float32
  hidden_dropout: 0.0
  attention_dropout: 0.0
  mask_func_type: "attn_mask_add"
  mlp_has_bias: True
  ffn_hidden_size: 6144
  hidden_act: 'gelu'
  apply_residual_connection_post_norm: False
  normalization: 'FusedRMSNorm'
  norm_epsilon: 1.e-5

dataset_config:
  batch_size: 2
  dataset_size: 4
  seq_length: 16

optimizer_config:
  optimizer_type: "AdamW"
  beta1: 0.9
  beta2: 0.999
  eps: 1.e-6
  learning_rate: 1.e-3
  weight_decay: 0.0
  zero_config:
    param_resident: True
    param_resident_rate: 0.5

lora_config:
  use_lora: False