training_config:
  seed: 42
  output_dir: './output'
  training_iters: 10
  log_interval: 1
  eval_interval: 5
  save_interval: null
  loss_scale_value: 4096
  loss_scale_factor: 2
  loss_scale_window: 1000
  grad_clip_kwargs:
    grad_clip_type: "GradClipByValue"
    clip_value: 1.0
  loss_func_kwargs:
    loss_func_type: "CrossEntropyLoss"
    reduction: "mean"
  eval_metric: "perplexity"

parallel_config:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: False

model_config:
  vocab_size: 50257
  hidden_size: 2560
  num_layers: 2
  num_attention_heads: 32
  num_experts: null
  group_query_attention: False
  ffn_hidden_size: 10240
  apply_residual_connection_post_norm: False
  use_flash_attention: False
  qkv_has_bias: True
  out_proj_has_bias: True
  mask_func_type: "attn_mask_fill"
  normalization: "LayerNorm"
  norm_epsilon: 1.e-5
  hidden_act: "fast_gelu"
  hidden_dropout_prob: 0.0
  attention_probs_dropout_prob: 0.0
  mlp_has_bias: True

dataset_config:
  batch_size: 1
  micro_batch_num: 1
  dataset_dir: 'dataset'
  shuffle: False
  drop_remainder: True
  eod_id: 50256

lora_config:
  use_lora: False
  lora_rank: 8
  lora_alpha: 32
  target_cells: [
                  {'target_cells':[
                      '.*.qkv_proj',
                      '.*.out_proj',
                      '.*.mapping',
                      '.*.projection',
                      '.*.q_proj',
                      '.*.kv_proj',
                  ]},
              ]
  checkpoint_dir: 'path/to/ckpt'
  checkpoint_prefix: 'checkpoint_'

optimizer_config:
  optimizer_type: "AdamWeightDecay"
  beta1: 0.9
  beta2: 0.95
  eps: 1.e-8
  learning_rate: 1.e-4
  weight_decay: 1.e-1
  learning_rate_scheduler_kwargs:
    warmup_steps: 200
    decay_steps: 2000
    use_cosine: True
    end_learning_rate: 1.e-6
