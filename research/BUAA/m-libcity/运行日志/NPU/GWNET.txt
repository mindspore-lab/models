time="2023-10-12T20:10:36+08:00" level=info msg="init logger successful" file="init.go:49" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="current user 1000:1000" file="init.go:51" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=warning msg="report event InitStart failed: send training-event info to algorancher failed, err: Post \"https://modelarts.cn-northwest-229.myhuaweicloud.com/v2/0c3767f69c62473b8fde445bb6d570f5/training-jobs/f828aa20-b6f6-40b5-a502-f91ed3515371/tasks/worker-0/reports/training-event\": dial tcp: lookup modelarts.cn-northwest-229.myhuaweicloud.com on 10.247.3.10:53: no such host" file="event.go:51" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="init command: bash /home/ma-user/training/init.sh ''" file="init.go:75" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="scc is already installed, skipping this step..." Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="[init] toolkit_obs_upload_pid = 57" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="[init] running at 2023-10-12-20:10:36" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="[init] ip of the pod: 172.16.0.92" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:36+08:00" level=info msg="local dir = /home/ma-user/modelarts/log/" file="upload.go:193" Command=obs/upload Component=ma-training-toolkit Platform=ModelArts-Service Task=
time="2023-10-12T20:10:36+08:00" level=info msg="obs dir = s3://modelarts-train-log-cn-northwest-229/f828aa20-b6f6-40b5-a502-f91ed3515371/worker-0" file="upload.go:196" Command=obs/upload Component=ma-training-toolkit Platform=ModelArts-Service Task=
time="2023-10-12T20:10:36+08:00" level=info msg="start the periodic upload task, upload Period = 5 seconds " file="upload.go:206" Command=obs/upload Component=ma-training-toolkit Platform=ModelArts-Service Task=
time="2023-10-12T20:10:37+08:00" level=info msg="[task]Detect item: disk-size shm" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:37+08:00" level=info msg="[detect] code: 0, message: ok, item: disk-size shm" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:37+08:00" level=info msg="[task]Detect item: dns" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:38+08:00" level=info msg="[detect] code: 0, message: ok, item: dns" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:38+08:00" level=info msg="[task]Detect item: disk-size root" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:38+08:00" level=info msg="[detect] code: 0, message: ok, item: disk-size root" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:38+08:00" level=info msg="[task]Detect item: disk-size cache" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:39+08:00" level=info msg="[detect] code: 0, message: ok, item: disk-size cache" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:39+08:00" level=warning msg="report event DetectFinish failed: send training-event info to algorancher failed, err: Post \"https://modelarts.cn-northwest-229.myhuaweicloud.com/v2/0c3767f69c62473b8fde445bb6d570f5/training-jobs/f828aa20-b6f6-40b5-a502-f91ed3515371/tasks/worker-0/reports/training-event\": dial tcp: lookup modelarts.cn-northwest-229.myhuaweicloud.com on 10.247.3.10:53: no such host" file="event.go:51" Command=report Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:39+08:00" level=info msg="[init] autosearch_path is empty, skip the autosearch download" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:39+08:00" level=info msg="[init] code_url is empty, skip the code download." Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:39+08:00" level=info msg="[init] record_dir is empty, skip the code upload" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:39+08:00" level=info msg="[init] inputs_handler_job_pid = 270" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:40+08:00" level=info msg="env MA_INPUTS is empty, skip the inputs handler" Component=PythonScripts Platform=ModelArts-Service
time="2023-10-12T20:10:41+08:00" level=info msg="[init] exiting at 2023-10-12-20:10:41" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:41+08:00" level=info msg="[init] upload_metrics_pid = 413" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:43+08:00" level=info msg="[init] stop toolkit_obs_upload_pid = 57 by signal SIGTERM" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:43+08:00" level=info msg="the periodic upload task exiting..." file="upload.go:216" Command=obs/upload Component=ma-training-toolkit Platform=ModelArts-Service Task=
time="2023-10-12T20:10:43+08:00" level=info msg="[init] toolkit_obs_upload 57 ret_code is 0" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:43+08:00" level=info msg="[init] exit with 0" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:43+08:00" level=info msg="local dir = /home/ma-user/modelarts/log/" file="upload.go:193" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service Task=
time="2023-10-12T20:10:43+08:00" level=info msg="obs dir = s3://modelarts-train-log-cn-northwest-229/f828aa20-b6f6-40b5-a502-f91ed3515371/worker-0" file="upload.go:196" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service Task=
time="2023-10-12T20:10:43+08:00" level=warning msg="report event InitExit failed: send training-event info to algorancher failed, err: Post \"https://modelarts.cn-northwest-229.myhuaweicloud.com/v2/0c3767f69c62473b8fde445bb6d570f5/training-jobs/f828aa20-b6f6-40b5-a502-f91ed3515371/tasks/worker-0/reports/training-event\": dial tcp: lookup modelarts.cn-northwest-229.myhuaweicloud.com on 10.247.3.10:53: no such host" file="event.go:51" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:43+08:00" level=info msg="bootstrap is exiting with exit code 0" file="bootstrap.go:241" Command=bootstrap/init Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:45+08:00" level=info msg="init logger successful" file="run_train.go:86" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:45+08:00" level=info msg="Waiting for SCC server start." file="run_train.go:439" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:45+08:00" level=info msg="init logger successful" file="upload.go:39" Command=bootstrap/upload Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:45+08:00" level=info msg="current user 1000:1000" file="upload.go:41" Command=bootstrap/upload Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=warning msg="report event SidecarStart failed: send training-event info to algorancher failed, err: Post \"https://modelarts.cn-northwest-229.myhuaweicloud.com/v2/0c3767f69c62473b8fde445bb6d570f5/training-jobs/f828aa20-b6f6-40b5-a502-f91ed3515371/tasks/worker-0/reports/training-event\": dial tcp: lookup modelarts.cn-northwest-229.myhuaweicloud.com on 10.247.3.10:53: no such host" file="event.go:51" Command=bootstrap/upload Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="upload command: /home/ma-user/training/sidecar.sh" file="upload.go:57" Command=bootstrap/upload Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="scc is already installed, skipping this step..." Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="[sidecar] running at 2023-10-12-20:10:46" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="[sidecar] toolkit_obs_upload_by_channels_pid = 51" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="[sidecar] waiting for training complete" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="[sidecar] scc server pid = 59" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="MA_OUTPUTS environment variable is empty, skip creating upload tasks." file="upload_by_channels.go:52" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="Starting SCC server on 127.0.0.1:39692" file="server.go:53" Command=scc-server Component=ma-scc-server Platform=ModelArts-Service
time="2023-10-12T20:10:46+08:00" level=info msg="local dir = /home/ma-user/modelarts/log/" file="upload.go:193" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=srt_log_collection
time="2023-10-12T20:10:46+08:00" level=info msg="obs dir = s3://modelarts-train-log-cn-northwest-229/f828aa20-b6f6-40b5-a502-f91ed3515371/worker-0" file="upload.go:196" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=srt_log_collection
time="2023-10-12T20:10:46+08:00" level=info msg="enable append upload mode" file="upload.go:199" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=srt_log_collection
time="2023-10-12T20:10:46+08:00" level=info msg="start the periodic upload task, upload Period = 5 seconds " file="upload.go:206" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=srt_log_collection
time="2023-10-12T20:10:46+08:00" level=info msg="local dir = /home/ma-user/modelarts/log/" file="upload.go:193" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=log_url
time="2023-10-12T20:10:46+08:00" level=info msg="obs dir = obs://grampus/log/" file="upload.go:196" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=log_url
time="2023-10-12T20:10:46+08:00" level=info msg="start the periodic upload task, upload Period = 30 seconds " file="upload.go:206" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=log_url
time="2023-10-12T20:10:50+08:00" level=info msg="SCC server has been init, training continue." file="run_train.go:457" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=warning msg="report event TrainingStart failed: send training-event info to algorancher failed, err: Post \"https://modelarts.cn-northwest-229.myhuaweicloud.com/v2/0c3767f69c62473b8fde445bb6d570f5/training-jobs/f828aa20-b6f6-40b5-a502-f91ed3515371/tasks/worker-0/reports/training-event\": dial tcp: lookup modelarts.cn-northwest-229.myhuaweicloud.com on 10.247.3.10:53: no such host" file="event.go:51" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="Skip hang detect" file="run_train.go:474" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="pre train command: /modelarts-job-f828aa20-b6f6-40b5-a502-f91ed3515371/ma-training-toolkit detect image-check; mkdir -p ~/.pip; echo -e '[global]\\ntrusted-host = pip.modelarts.private.com\\nindex-url = http://pip.modelarts.private.com:8888/repository/pypi/simple' > ~/.pip/pip.conf;  " file="run_train.go:510" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="runUser name: ma-user" file="image_check.go:54" Command=image-check Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="runUser uid: 1000" file="image_check.go:55" Command=image-check Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="runUser gid: 100" file="image_check.go:56" Command=image-check Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="run command: mkdir -p /cache/code;mkdir -p /cache/pretrainmodel;mkdir -p /cache/dataset;mkdir -p /cache/output;export bucket=PROD && export remote_path=job/zhang2023101220t075157625/output/;echo 'start to exec code';source /home/ma-user/.bashrc;export GLOG_v=3;export ASCEND_GLOBAL_LOG_LEVEL=3;export ASCEND_SLOG_PRINT_TO_STDOUT=0 ;export HCCL_CONNECT_TIMEOUT=3600;export HCCL_EXEC_TIMEOUT=1800;export PIPELINE_SLICE_SKIP_REDISTRIBUTION=1;export MS_DEV_REDUNDANCY_TASK_NUM=4;export MS_DEV_CELL_REUSE=2;python /home/ma-user/davinci/train/davincirun.py python /home/ma-user/grampus.py  --'rank_size'='1' --multi_data_url='[{\"dataset_url\":\"s3:///urchincache/attachment/8/1/81cd7def-5d35-46cc-b440-3a506446e51c/NYC_RISK.zip\",\"dataset_name\":\"NYC_RISK.zip\",\"containerPath\":\"/cache/dataset/NYC_RISK.zip\",\"readOnly\":true}]' --pretrain_url='[]' --code_url='s3:///urchincache/job/zhang2023101220t075157625/code/master.zip' --code_name='m-libcity' --boot_file='M_libcity/test_pipeline.py' --model_url='s3:///grampus/job/zhang2023101220t075157625/output/' --grampus_code_url='s3:///grampus/system_code/' --grampus_code_file_name='pre_and_suf.py';result=$?;bash -c \"[[ $result -eq 0 ]] && exit 0 || exit -1\"; " file="run_train.go:322" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:10:50+08:00" level=info msg="zombie process cleaner is start running, childPid=31" file="cleaner_unix.go:31" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
start to exec code
INFO:root:Using MoXing-v2.0.1.rc0.ffd1c0c8-ffd1c0c8
INFO:root:Using OBS-Python-SDK-3.20.9.1
[ModelArts Service Log]2023-10-12 20:10:51,012 - INFO - Ascend Driver: Version=22.0.0.3
[ModelArts Service Log]2023-10-12 20:10:51,013 - INFO - you are advised to use ASCEND_DEVICE_ID env instead of DEVICE_ID, as the DEVICE_ID env will be discarded in later versions
[ModelArts Service Log]2023-10-12 20:10:51,013 - INFO - particularly, ${ASCEND_DEVICE_ID} == ${DEVICE_ID}, it's the logical device id
[ModelArts Service Log]2023-10-12 20:10:51,013 - INFO - Davinci training command
[ModelArts Service Log]2023-10-12 20:10:51,013 - INFO - ['python', '/home/ma-user/grampus.py', '--rank_size=1', '--multi_data_url=[{"dataset_url":"s3:///urchincache/attachment/8/1/81cd7def-5d35-46cc-b440-3a506446e51c/NYC_RISK.zip","dataset_name":"NYC_RISK.zip","containerPath":"/cache/dataset/NYC_RISK.zip","readOnly":true}]', '--pretrain_url=[]', '--code_url=s3:///urchincache/job/zhang2023101220t075157625/code/master.zip', '--code_name=m-libcity', '--boot_file=M_libcity/test_pipeline.py', '--model_url=s3:///grampus/job/zhang2023101220t075157625/output/', '--grampus_code_url=s3:///grampus/system_code/', '--grampus_code_file_name=pre_and_suf.py']
[ModelArts Service Log]2023-10-12 20:10:51,013 - INFO - Wait for Rank table file ready
[ModelArts Service Log]2023-10-12 20:10:51,013 - INFO - Rank table file (K8S generated) is ready for read
[ModelArts Service Log]2023-10-12 20:10:51,014 - INFO - 
{
    "status": "completed",
    "group_count": "1",
    "group_list": [
        {
            "group_name": "worker",
            "device_count": "1",
            "instance_count": "1",
            "instance_list": [
                {
                    "pod_name": "ma-job-f828aa20-b6f6-40b5-a502-f91ed3515371-worker-0",
                    "server_id": "192.168.6.102",
                    "devices": [
                        {
                            "device_id": "2",
                            "device_ip": "192.3.0.184"
                        }
                    ]
                }
            ]
        }
    ]
}
[ModelArts Service Log]2023-10-12 20:10:51,015 - INFO - Rank table file (V1)
[ModelArts Service Log]2023-10-12 20:10:51,015 - INFO - 
{
    "status": "completed",
    "version": "1.0",
    "server_count": "1",
    "server_list": [
        {
            "server_id": "192.168.6.102",
            "device": [
                {
                    "device_id": "2",
                    "device_ip": "192.3.0.184",
                    "rank_id": "0"
                }
            ]
        }
    ]
}
[ModelArts Service Log]2023-10-12 20:10:51,015 - INFO - Rank table file (V1) is generated
[ModelArts Service Log]2023-10-12 20:10:51,015 - INFO - Current server
[ModelArts Service Log]2023-10-12 20:10:51,016 - INFO - 
{
    "server_id": "192.168.6.102",
    "device": [
        {
            "device_id": "2",
            "device_ip": "192.3.0.184",
            "rank_id": "0"
        }
    ]
}
[ModelArts Service Log]2023-10-12 20:10:51,016 - INFO - Route plan ends for env ROUTE_PLAN = false. Route plan acceleration service may not be available in this region
[ModelArts Service Log]2023-10-12 20:10:51,016 - INFO - env already exists. env_name: HCCL_CONNECT_TIMEOUT, env_value: 1800 
[ModelArts Service Log]2023-10-12 20:10:51,017 - INFO - bootstrap proc-rank-0-device-0
[ModelArts Service Log]2023-10-12 20:10:51,026 - INFO - proc-rank-0-device-0 (pid: 124)
INFO:root:Listing OBS: 1000
INFO:root:Log directory: /cache/code/m-libcity/M_libcity/log
Successfully Download /cache/code/m-libcity/M_libcity/data/ to /cache/code/m-libcity/M_libcity/raw_data
download_input succeed
2023-10-12 20:11:27,614 - INFO - Log directory: /cache/code/m-libcity/M_libcity/log
INFO:root:Begin pipeline, task=traffic_state_pred, model_name=GWNET, dataset_name=NYCBike20140409, exp_id=1916
2023-10-12 20:11:27,615 - INFO - Begin pipeline, task=traffic_state_pred, model_name=GWNET, dataset_name=NYCBike20140409, exp_id=1916
INFO:root:{'task': 'traffic_state_pred', 'model': 'GWNET', 'dataset': 'NYCBike20140409', 'saved_model': True, 'train': True, 'rank_size': 1, 'batch_size': 64, 'max_epoch': 30, 'dataset_class': 'TrafficStateGridDataset', 'executor': 'TrafficStateExecutor', 'evaluator': 'TrafficStateEvaluator', 'dropout': 0.3, 'blocks': 4, 'layers': 2, 'gcn_bool': True, 'addaptadj': True, 'adjtype': 'doubletransition', 'bidir_adj_mx': False, 'randomadj': True, 'aptonly': True, 'kernel_size': 2, 'nhid': 32, 'residual_channels': 32, 'dilation_channels': 32, 'skip_channels': 256, 'end_channels': 512, 'scaler': 'standard', 'load_external': False, 'normal_external': False, 'ext_scaler': 'none', 'add_time_in_day': True, 'add_day_in_week': False, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': False, 'cache_dataset': True, 'num_workers': 1, 'pad_with_last_sample': True, 'train_rate': 0.8, 'eval_rate': 0.1, 'input_window': 12, 'output_window': 12, 'use_row_column': False, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0.0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [50, 200, 400, 700], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'patience': 50, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE'], 'evaluator_mode': 'single', 'save_mode': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'row_id': 'num', 'column_id': 'num'}}, 'grid': {'including_types': ['state'], 'state': {'row_id': 16, 'column_id': 8, 'new_flow': 'num', 'end_flow': 'num'}}, 'data_col': ['new_flow', 'end_flow'], 'data_files': ['NYCBIKE20140409'], 'geo_file': 'NYCBIKE20140409', 'output_dim': 2, 'time_intervals': 3600, 'init_weight_inf_or_zero': 'inf', 'set_weight_link_or_dist': 'dist', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'exp_id': 1916}
2023-10-12 20:11:27,615 - INFO - {'task': 'traffic_state_pred', 'model': 'GWNET', 'dataset': 'NYCBike20140409', 'saved_model': True, 'train': True, 'rank_size': 1, 'batch_size': 64, 'max_epoch': 30, 'dataset_class': 'TrafficStateGridDataset', 'executor': 'TrafficStateExecutor', 'evaluator': 'TrafficStateEvaluator', 'dropout': 0.3, 'blocks': 4, 'layers': 2, 'gcn_bool': True, 'addaptadj': True, 'adjtype': 'doubletransition', 'bidir_adj_mx': False, 'randomadj': True, 'aptonly': True, 'kernel_size': 2, 'nhid': 32, 'residual_channels': 32, 'dilation_channels': 32, 'skip_channels': 256, 'end_channels': 512, 'scaler': 'standard', 'load_external': False, 'normal_external': False, 'ext_scaler': 'none', 'add_time_in_day': True, 'add_day_in_week': False, 'learner': 'adam', 'learning_rate': 0.001, 'lr_decay': False, 'clip_grad_norm': True, 'max_grad_norm': 5, 'use_early_stop': False, 'cache_dataset': True, 'num_workers': 1, 'pad_with_last_sample': True, 'train_rate': 0.8, 'eval_rate': 0.1, 'input_window': 12, 'output_window': 12, 'use_row_column': False, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0.0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [50, 200, 400, 700], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'patience': 50, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'MSE', 'RMSE', 'masked_MAE', 'masked_MAPE', 'masked_MSE', 'masked_RMSE'], 'evaluator_mode': 'single', 'save_mode': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'row_id': 'num', 'column_id': 'num'}}, 'grid': {'including_types': ['state'], 'state': {'row_id': 16, 'column_id': 8, 'new_flow': 'num', 'end_flow': 'num'}}, 'data_col': ['new_flow', 'end_flow'], 'data_files': ['NYCBIKE20140409'], 'geo_file': 'NYCBIKE20140409', 'output_dim': 2, 'time_intervals': 3600, 'init_weight_inf_or_zero': 'inf', 'set_weight_link_or_dist': 'dist', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1, 'exp_id': 1916}
INFO:root:Loaded file NYCBIKE20140409.geo, num_grids=128, grid_size=(16, 8)
2023-10-12 20:11:27,657 - INFO - Loaded file NYCBIKE20140409.geo, num_grids=128, grid_size=(16, 8)
INFO:root:Generate grid rel file, shape=(128, 128)
2023-10-12 20:11:27,659 - INFO - Generate grid rel file, shape=(128, 128)
INFO:root:Loading file NYCBIKE20140409.grid
2023-10-12 20:11:27,659 - INFO - Loading file NYCBIKE20140409.grid
INFO:root:Loaded file NYCBIKE20140409.grid, shape=(4392, 128, 2)
2023-10-12 20:11:28,394 - INFO - Loaded file NYCBIKE20140409.grid, shape=(4392, 128, 2)
INFO:root:Dataset created
2023-10-12 20:11:28,912 - INFO - Dataset created
INFO:root:x shape: (4369, 12, 128, 2), y shape: (4369, 12, 128, 2)
2023-10-12 20:11:28,913 - INFO - x shape: (4369, 12, 128, 2), y shape: (4369, 12, 128, 2)
INFO:root:train	x: (3495, 12, 128, 2), y: (3495, 12, 128, 2)
2023-10-12 20:11:28,914 - INFO - train	x: (3495, 12, 128, 2), y: (3495, 12, 128, 2)
INFO:root:eval	x: (437, 12, 128, 2), y: (437, 12, 128, 2)
2023-10-12 20:11:28,915 - INFO - eval	x: (437, 12, 128, 2), y: (437, 12, 128, 2)
INFO:root:test	x: (437, 12, 128, 2), y: (437, 12, 128, 2)
2023-10-12 20:11:28,915 - INFO - test	x: (437, 12, 128, 2), y: (437, 12, 128, 2)
INFO:root:Saved at /cache/code/m-libcity/M_libcity/cache/dataset_cache/grid_based_NYCBike20140409_12_12_0.8_0.1_standard_64_False_True_False_True_False.npz
2023-10-12 20:11:33,220 - INFO - Saved at /cache/code/m-libcity/M_libcity/cache/dataset_cache/grid_based_NYCBike20140409_12_12_0.8_0.1_standard_64_False_True_False_True_False.npz
INFO:root:StandardScaler mean: 9.371765561665475, std: 18.27140197139823
!!! 2 0 2
2023-10-12 20:11:33,359 - INFO - StandardScaler mean: 9.371765561665475, std: 18.27140197139823
INFO:root:NoneScaler
2023-10-12 20:11:33,360 - INFO - NoneScaler
INFO:root:GWNET<
  (network): gwnet<
    (filter_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (gate_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (residual_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (skip_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (bn): CellList<
      (0): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.0.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.0.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.0.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.0.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (1): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.1.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.1.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.1.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.1.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (2): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.2.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.2.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.2.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.2.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (3): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.3.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.3.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.3.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.3.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (4): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.4.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.4.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.4.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.4.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (5): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.5.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.5.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.5.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.5.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (6): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.6.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.6.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.6.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.6.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (7): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.7.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.7.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.7.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.7.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      >
    (gconv): CellList<
      (0): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (1): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (2): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (3): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (4): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (5): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (6): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (7): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      >
    (start_conv): Conv2d<input_channels=1, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
    (end_conv_1): Conv2d<input_channels=256, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
    (end_conv_2): Conv2d<input_channels=512, output_channels=12, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
    >
  >
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!data/utils
! 0.3 True True 1 1 32 32 256 512 128
2023-10-12 20:11:34,903 - INFO - GWNET<
  (network): gwnet<
    (filter_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (gate_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 2), stride=(1, 1), pad_mode=valid, padding=0, dilation=(2, 2), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (residual_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (skip_convs): CellList<
      (0): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (1): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (2): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (3): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (4): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (5): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (6): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      (7): Conv2d<input_channels=32, output_channels=256, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
      >
    (bn): CellList<
      (0): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.0.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.0.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.0.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.0.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (1): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.1.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.1.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.1.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.1.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (2): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.2.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.2.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.2.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.2.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (3): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.3.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.3.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.3.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.3.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (4): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.4.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.4.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.4.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.4.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (5): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.5.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.5.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.5.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.5.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (6): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.6.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.6.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.6.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.6.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      (7): BatchNorm2d<num_features=32, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=network.bn.7.gamma, shape=(32,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.bn.7.beta, shape=(32,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.bn.7.moving_mean, shape=(32,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.bn.7.moving_variance, shape=(32,), dtype=Float32, requires_grad=False)>
      >
    (gconv): CellList<
      (0): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (1): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (2): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (3): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (4): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (5): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (6): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      (7): gcn<
        (mlp): Conv2d<input_channels=224, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
        >
      >
    (start_conv): Conv2d<input_channels=1, output_channels=32, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
    (end_conv_1): Conv2d<input_channels=256, output_channels=512, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
    (end_conv_2): Conv2d<input_channels=512, output_channels=12, kernel_size=(1, 1), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=he_uniform, bias_init=zeros, format=NCHW>
    >
  >
INFO:root:network.nodevec1	(128, 10)	True
2023-10-12 20:11:34,910 - INFO - network.nodevec1	(128, 10)	True
INFO:root:network.nodevec2	(10, 128)	True
2023-10-12 20:11:34,910 - INFO - network.nodevec2	(10, 128)	True
INFO:root:network.filter_convs.0.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,910 - INFO - network.filter_convs.0.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.1.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,910 - INFO - network.filter_convs.1.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.2.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,910 - INFO - network.filter_convs.2.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.3.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,910 - INFO - network.filter_convs.3.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.4.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,910 - INFO - network.filter_convs.4.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.5.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.filter_convs.5.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.6.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.filter_convs.6.weight	(32, 32, 1, 2)	True
INFO:root:network.filter_convs.7.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.filter_convs.7.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.0.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.gate_convs.0.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.1.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.gate_convs.1.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.2.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.gate_convs.2.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.3.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.gate_convs.3.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.4.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,911 - INFO - network.gate_convs.4.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.5.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,912 - INFO - network.gate_convs.5.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.6.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,912 - INFO - network.gate_convs.6.weight	(32, 32, 1, 2)	True
INFO:root:network.gate_convs.7.weight	(32, 32, 1, 2)	True
2023-10-12 20:11:34,912 - INFO - network.gate_convs.7.weight	(32, 32, 1, 2)	True
INFO:root:network.residual_convs.0.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,912 - INFO - network.residual_convs.0.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.1.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,912 - INFO - network.residual_convs.1.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.2.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,912 - INFO - network.residual_convs.2.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.3.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,912 - INFO - network.residual_convs.3.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.4.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,912 - INFO - network.residual_convs.4.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.5.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,912 - INFO - network.residual_convs.5.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.6.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.residual_convs.6.weight	(32, 32, 1, 1)	True
INFO:root:network.residual_convs.7.weight	(32, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.residual_convs.7.weight	(32, 32, 1, 1)	True
INFO:root:network.skip_convs.0.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.0.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.1.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.1.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.2.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.2.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.3.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.3.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.4.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.4.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.5.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.5.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.6.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,913 - INFO - network.skip_convs.6.weight	(256, 32, 1, 1)	True
INFO:root:network.skip_convs.7.weight	(256, 32, 1, 1)	True
2023-10-12 20:11:34,914 - INFO - network.skip_convs.7.weight	(256, 32, 1, 1)	True
INFO:root:network.bn.0.gamma	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.0.gamma	(32,)	True
INFO:root:network.bn.0.beta	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.0.beta	(32,)	True
INFO:root:network.bn.1.gamma	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.1.gamma	(32,)	True
INFO:root:network.bn.1.beta	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.1.beta	(32,)	True
INFO:root:network.bn.2.gamma	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.2.gamma	(32,)	True
INFO:root:network.bn.2.beta	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.2.beta	(32,)	True
INFO:root:network.bn.3.gamma	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.3.gamma	(32,)	True
INFO:root:network.bn.3.beta	(32,)	True
2023-10-12 20:11:34,914 - INFO - network.bn.3.beta	(32,)	True
INFO:root:network.bn.4.gamma	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.4.gamma	(32,)	True
INFO:root:network.bn.4.beta	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.4.beta	(32,)	True
INFO:root:network.bn.5.gamma	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.5.gamma	(32,)	True
INFO:root:network.bn.5.beta	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.5.beta	(32,)	True
INFO:root:network.bn.6.gamma	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.6.gamma	(32,)	True
INFO:root:network.bn.6.beta	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.6.beta	(32,)	True
INFO:root:network.bn.7.gamma	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.7.gamma	(32,)	True
INFO:root:network.bn.7.beta	(32,)	True
2023-10-12 20:11:34,915 - INFO - network.bn.7.beta	(32,)	True
INFO:root:network.gconv.0.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,915 - INFO - network.gconv.0.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.1.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.1.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.2.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.2.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.3.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.3.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.4.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.4.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.5.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.5.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.6.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.6.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.gconv.7.mlp.weight	(32, 224, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.gconv.7.mlp.weight	(32, 224, 1, 1)	True
INFO:root:network.start_conv.weight	(32, 1, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.start_conv.weight	(32, 1, 1, 1)	True
INFO:root:network.end_conv_1.weight	(512, 256, 1, 1)	True
2023-10-12 20:11:34,916 - INFO - network.end_conv_1.weight	(512, 256, 1, 1)	True
INFO:root:network.end_conv_2.weight	(12, 512, 1, 1)	True
2023-10-12 20:11:34,917 - INFO - network.end_conv_2.weight	(12, 512, 1, 1)	True
INFO:root:Total parameter numbers: 304160
2023-10-12 20:11:34,917 - INFO - Total parameter numbers: 304160
INFO:root:You select `adam` optimizer.
2023-10-12 20:11:34,918 - INFO - You select `adam` optimizer.
WARNING:root:Received none train loss func and will use the loss func defined in the model.
2023-10-12 20:11:35,026 - WARNING - Received none train loss func and will use the loss func defined in the model.
INFO:root:Start training ...
2023-10-12 20:11:35,027 - INFO - Start training ...
INFO:root:num_batches:54
2023-10-12 20:11:35,027 - INFO - num_batches:54
[INFO] PARALLEL(383,ffff9e8d4ac0,python):2023-10-12-20:11:35.901.335 [mindspore/ccsrc/frontend/parallel/costmodel_context.cc:30] GetInstance] Create costmodel_context
[INFO] CORE(383,ffff9e8d4ac0,python):2023-10-12-20:11:36.260.358 [mindspore/core/utils/ms_context.h:234] set_param<std::basic_string<char> >] ms set context device target:Ascend
[INFO] CORE(383,ffff9e8d4ac0,python):2023-10-12-20:11:36.497.090 [mindspore/core/utils/ms_context.cc:136] set_backend_policy] ms set context backend policy:ms
[INFO] ME(383:281473341803200,MainProcess):2023-10-12-20:11:40.208.397 [mindspore/_extends/remote/kernel_build_server_ascend.py:32] [TRACE] Ascend Messager init...
epoch: 1 step: 1, loss is 31.365619659423828
epoch: 1 step: 2, loss is 21.98418617248535
epoch: 1 step: 3, loss is 20.165775299072266
epoch: 1 step: 4, loss is 20.812719345092773
epoch: 1 step: 5, loss is 20.955242156982422
epoch: 1 step: 6, loss is 20.039466857910156
epoch: 1 step: 7, loss is 18.69673728942871
epoch: 1 step: 8, loss is 18.057254791259766
epoch: 1 step: 9, loss is 16.32050895690918
epoch: 1 step: 10, loss is 15.923330307006836
epoch: 1 step: 11, loss is 16.665542602539062
epoch: 1 step: 12, loss is 15.644715309143066
epoch: 1 step: 13, loss is 15.822312355041504
epoch: 1 step: 14, loss is 15.786364555358887
epoch: 1 step: 15, loss is 14.665026664733887
epoch: 1 step: 16, loss is 14.742877006530762
epoch: 1 step: 17, loss is 14.941022872924805
epoch: 1 step: 18, loss is 14.266192436218262
epoch: 1 step: 19, loss is 14.637389183044434
epoch: 1 step: 20, loss is 14.193988800048828
epoch: 1 step: 21, loss is 14.036397933959961
epoch: 1 step: 22, loss is 13.73413372039795
epoch: 1 step: 23, loss is 13.508381843566895
epoch: 1 step: 24, loss is 13.388361930847168
epoch: 1 step: 25, loss is 13.208587646484375
epoch: 1 step: 26, loss is 13.408041954040527
epoch: 1 step: 27, loss is 12.74887466430664
epoch: 1 step: 28, loss is 13.06338882446289
epoch: 1 step: 29, loss is 13.270590782165527
epoch: 1 step: 30, loss is 13.539472579956055
epoch: 1 step: 31, loss is 12.921371459960938
epoch: 1 step: 32, loss is 12.475810050964355
epoch: 1 step: 33, loss is 11.98361587524414
epoch: 1 step: 34, loss is 12.793244361877441
epoch: 1 step: 35, loss is 12.835285186767578
epoch: 1 step: 36, loss is 12.758373260498047
epoch: 1 step: 37, loss is 12.12855339050293
epoch: 1 step: 38, loss is 12.562183380126953
epoch: 1 step: 39, loss is 12.221635818481445
epoch: 1 step: 40, loss is 12.317578315734863
epoch: 1 step: 41, loss is 11.487407684326172
epoch: 1 step: 42, loss is 12.168630599975586
epoch: 1 step: 43, loss is 11.967243194580078
epoch: 1 step: 44, loss is 12.055900573730469
epoch: 1 step: 45, loss is 11.869123458862305
epoch: 1 step: 46, loss is 11.904102325439453
epoch: 1 step: 47, loss is 11.771782875061035
epoch: 1 step: 48, loss is 11.636275291442871
epoch: 1 step: 49, loss is 11.465808868408203
epoch: 1 step: 50, loss is 11.15285587310791
epoch: 1 step: 51, loss is 10.90425968170166
epoch: 1 step: 52, loss is 11.50505542755127
epoch: 1 step: 53, loss is 11.618399620056152
epoch: 1 step: 54, loss is 11.821990013122559
Train epoch time: 320790.889 ms, per step time: 5940.572 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:16:55,825 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 11.666847229003906
2023-10-12 20:16:58,060 - INFO - valid loss : 11.666847229003906
epoch: 2 step: 1, loss is 11.44784164428711
epoch: 2 step: 2, loss is 10.864991188049316
epoch: 2 step: 3, loss is 11.139593124389648
epoch: 2 step: 4, loss is 10.752947807312012
epoch: 2 step: 5, loss is 11.369037628173828
epoch: 2 step: 6, loss is 10.554584503173828
epoch: 2 step: 7, loss is 11.386159896850586
epoch: 2 step: 8, loss is 10.557933807373047
epoch: 2 step: 9, loss is 10.345793724060059
epoch: 2 step: 10, loss is 10.826766967773438
epoch: 2 step: 11, loss is 10.94467544555664
epoch: 2 step: 12, loss is 10.910356521606445
epoch: 2 step: 13, loss is 10.891507148742676
epoch: 2 step: 14, loss is 10.842304229736328
epoch: 2 step: 15, loss is 10.25299072265625
epoch: 2 step: 16, loss is 10.554747581481934
epoch: 2 step: 17, loss is 10.79035472869873
epoch: 2 step: 18, loss is 10.466767311096191
epoch: 2 step: 19, loss is 10.937813758850098
epoch: 2 step: 20, loss is 10.185297966003418
epoch: 2 step: 21, loss is 10.1295804977417
epoch: 2 step: 22, loss is 10.278977394104004
epoch: 2 step: 23, loss is 10.280170440673828
epoch: 2 step: 24, loss is 10.49713134765625
epoch: 2 step: 25, loss is 10.665642738342285
epoch: 2 step: 26, loss is 9.839642524719238
epoch: 2 step: 27, loss is 10.301819801330566
epoch: 2 step: 28, loss is 9.9164457321167
epoch: 2 step: 29, loss is 10.108346939086914
epoch: 2 step: 30, loss is 9.692481994628906
epoch: 2 step: 31, loss is 10.110825538635254
epoch: 2 step: 32, loss is 10.009345054626465
epoch: 2 step: 33, loss is 9.865828514099121
epoch: 2 step: 34, loss is 9.667180061340332
epoch: 2 step: 35, loss is 9.888981819152832
epoch: 2 step: 36, loss is 9.860394477844238
epoch: 2 step: 37, loss is 10.23341178894043
epoch: 2 step: 38, loss is 9.980711936950684
epoch: 2 step: 39, loss is 9.394888877868652
epoch: 2 step: 40, loss is 9.488011360168457
epoch: 2 step: 41, loss is 9.385354995727539
epoch: 2 step: 42, loss is 9.161042213439941
epoch: 2 step: 43, loss is 9.369810104370117
epoch: 2 step: 44, loss is 9.810614585876465
epoch: 2 step: 45, loss is 9.717179298400879
epoch: 2 step: 46, loss is 9.649103164672852
epoch: 2 step: 47, loss is 9.557929039001465
epoch: 2 step: 48, loss is 9.417567253112793
epoch: 2 step: 49, loss is 9.259477615356445
epoch: 2 step: 50, loss is 9.0970458984375
epoch: 2 step: 51, loss is 9.49374771118164
epoch: 2 step: 52, loss is 9.225129127502441
epoch: 2 step: 53, loss is 9.921001434326172
epoch: 2 step: 54, loss is 9.60561466217041
Train epoch time: 38497.431 ms, per step time: 712.915 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:17:36,559 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 9.560175895690918
2023-10-12 20:17:38,481 - INFO - valid loss : 9.560175895690918
epoch: 3 step: 1, loss is 8.972677230834961
epoch: 3 step: 2, loss is 9.287656784057617
epoch: 3 step: 3, loss is 9.4183931350708
epoch: 3 step: 4, loss is 8.983609199523926
epoch: 3 step: 5, loss is 9.429880142211914
epoch: 3 step: 6, loss is 9.293556213378906
epoch: 3 step: 7, loss is 9.069100379943848
epoch: 3 step: 8, loss is 9.506610870361328
epoch: 3 step: 9, loss is 9.187337875366211
epoch: 3 step: 10, loss is 9.164734840393066
epoch: 3 step: 11, loss is 9.107638359069824
epoch: 3 step: 12, loss is 9.384698867797852
epoch: 3 step: 13, loss is 8.893223762512207
epoch: 3 step: 14, loss is 9.409295082092285
epoch: 3 step: 15, loss is 9.292265892028809
epoch: 3 step: 16, loss is 8.856311798095703
epoch: 3 step: 17, loss is 9.501861572265625
epoch: 3 step: 18, loss is 8.674827575683594
epoch: 3 step: 19, loss is 8.635437965393066
epoch: 3 step: 20, loss is 9.046712875366211
epoch: 3 step: 21, loss is 9.097199440002441
epoch: 3 step: 22, loss is 8.581028938293457
epoch: 3 step: 23, loss is 8.87691593170166
epoch: 3 step: 24, loss is 8.657076835632324
epoch: 3 step: 25, loss is 8.77267074584961
epoch: 3 step: 26, loss is 8.656682014465332
epoch: 3 step: 27, loss is 8.979111671447754
epoch: 3 step: 28, loss is 8.426168441772461
epoch: 3 step: 29, loss is 8.946229934692383
epoch: 3 step: 30, loss is 9.160192489624023
epoch: 3 step: 31, loss is 8.655529975891113
epoch: 3 step: 32, loss is 8.590747833251953
epoch: 3 step: 33, loss is 8.75412654876709
epoch: 3 step: 34, loss is 8.770731925964355
epoch: 3 step: 35, loss is 8.547074317932129
epoch: 3 step: 36, loss is 8.387274742126465
epoch: 3 step: 37, loss is 8.432865142822266
epoch: 3 step: 38, loss is 8.74067211151123
epoch: 3 step: 39, loss is 8.307022094726562
epoch: 3 step: 40, loss is 8.63976764678955
epoch: 3 step: 41, loss is 8.933172225952148
epoch: 3 step: 42, loss is 8.765685081481934
epoch: 3 step: 43, loss is 8.888482093811035
epoch: 3 step: 44, loss is 9.177461624145508
epoch: 3 step: 45, loss is 8.668429374694824
epoch: 3 step: 46, loss is 8.801847457885742
epoch: 3 step: 47, loss is 8.830594062805176
epoch: 3 step: 48, loss is 9.094978332519531
epoch: 3 step: 49, loss is 8.404126167297363
epoch: 3 step: 50, loss is 8.737089157104492
epoch: 3 step: 51, loss is 9.161231994628906
epoch: 3 step: 52, loss is 8.96715259552002
epoch: 3 step: 53, loss is 8.404257774353027
epoch: 3 step: 54, loss is 8.15347671508789
Train epoch time: 38203.574 ms, per step time: 707.474 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:18:16,686 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 8.656584739685059
2023-10-12 20:18:18,718 - INFO - valid loss : 8.656584739685059
epoch: 4 step: 1, loss is 8.864309310913086
epoch: 4 step: 2, loss is 8.553848266601562
epoch: 4 step: 3, loss is 8.383862495422363
epoch: 4 step: 4, loss is 8.688652038574219
epoch: 4 step: 5, loss is 8.788360595703125
epoch: 4 step: 6, loss is 8.869406700134277
epoch: 4 step: 7, loss is 8.419751167297363
epoch: 4 step: 8, loss is 8.396628379821777
epoch: 4 step: 9, loss is 8.57660961151123
epoch: 4 step: 10, loss is 8.644455909729004
epoch: 4 step: 11, loss is 8.147122383117676
epoch: 4 step: 12, loss is 8.910697937011719
epoch: 4 step: 13, loss is 8.560839653015137
epoch: 4 step: 14, loss is 8.345158576965332
epoch: 4 step: 15, loss is 8.426823616027832
epoch: 4 step: 16, loss is 8.019905090332031
epoch: 4 step: 17, loss is 8.10638427734375
epoch: 4 step: 18, loss is 7.984184265136719
epoch: 4 step: 19, loss is 8.357316017150879
epoch: 4 step: 20, loss is 7.9272356033325195
epoch: 4 step: 21, loss is 8.646649360656738
epoch: 4 step: 22, loss is 8.505833625793457
epoch: 4 step: 23, loss is 8.589277267456055
epoch: 4 step: 24, loss is 8.325448036193848
epoch: 4 step: 25, loss is 8.704524040222168
epoch: 4 step: 26, loss is 8.432318687438965
epoch: 4 step: 27, loss is 8.279895782470703
epoch: 4 step: 28, loss is 8.291507720947266
epoch: 4 step: 29, loss is 8.38209342956543
epoch: 4 step: 30, loss is 8.05509090423584
epoch: 4 step: 31, loss is 8.136933326721191
epoch: 4 step: 32, loss is 8.275604248046875
epoch: 4 step: 33, loss is 8.239651679992676
epoch: 4 step: 34, loss is 8.089043617248535
epoch: 4 step: 35, loss is 7.449929714202881
epoch: 4 step: 36, loss is 7.7003679275512695
epoch: 4 step: 37, loss is 8.142292022705078
epoch: 4 step: 38, loss is 8.18183708190918
epoch: 4 step: 39, loss is 7.95177698135376
epoch: 4 step: 40, loss is 7.7889556884765625
epoch: 4 step: 41, loss is 8.00363540649414
epoch: 4 step: 42, loss is 8.14069938659668
epoch: 4 step: 43, loss is 8.342514991760254
epoch: 4 step: 44, loss is 8.980090141296387
epoch: 4 step: 45, loss is 8.213645935058594
epoch: 4 step: 46, loss is 7.810452938079834
epoch: 4 step: 47, loss is 8.106273651123047
epoch: 4 step: 48, loss is 8.639361381530762
epoch: 4 step: 49, loss is 8.055049896240234
epoch: 4 step: 50, loss is 7.869606971740723
epoch: 4 step: 51, loss is 8.129799842834473
epoch: 4 step: 52, loss is 8.575751304626465
epoch: 4 step: 53, loss is 8.00241756439209
epoch: 4 step: 54, loss is 8.041521072387695
Train epoch time: 38623.304 ms, per step time: 715.246 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:18:57,343 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 8.18234920501709
2023-10-12 20:18:59,164 - INFO - valid loss : 8.18234920501709
epoch: 5 step: 1, loss is 8.079147338867188
epoch: 5 step: 2, loss is 8.236709594726562
epoch: 5 step: 3, loss is 7.767871379852295
epoch: 5 step: 4, loss is 7.693342685699463
epoch: 5 step: 5, loss is 7.680731296539307
epoch: 5 step: 6, loss is 7.774990558624268
epoch: 5 step: 7, loss is 8.031774520874023
epoch: 5 step: 8, loss is 7.997488975524902
epoch: 5 step: 9, loss is 8.088692665100098
epoch: 5 step: 10, loss is 8.082930564880371
epoch: 5 step: 11, loss is 7.746207237243652
epoch: 5 step: 12, loss is 7.712547779083252
epoch: 5 step: 13, loss is 7.813769817352295
epoch: 5 step: 14, loss is 7.78244161605835
epoch: 5 step: 15, loss is 7.7724761962890625
epoch: 5 step: 16, loss is 8.36106014251709
epoch: 5 step: 17, loss is 8.354976654052734
epoch: 5 step: 18, loss is 8.311121940612793
epoch: 5 step: 19, loss is 8.169427871704102
epoch: 5 step: 20, loss is 8.14927864074707
epoch: 5 step: 21, loss is 7.737691879272461
epoch: 5 step: 22, loss is 7.555844306945801
epoch: 5 step: 23, loss is 7.526138782501221
epoch: 5 step: 24, loss is 7.977028846740723
epoch: 5 step: 25, loss is 8.295358657836914
epoch: 5 step: 26, loss is 8.382734298706055
epoch: 5 step: 27, loss is 8.12836742401123
epoch: 5 step: 28, loss is 7.87399959564209
epoch: 5 step: 29, loss is 7.8796892166137695
epoch: 5 step: 30, loss is 7.713324069976807
epoch: 5 step: 31, loss is 8.025016784667969
epoch: 5 step: 32, loss is 7.804694175720215
epoch: 5 step: 33, loss is 7.587346076965332
epoch: 5 step: 34, loss is 7.898241996765137
epoch: 5 step: 35, loss is 7.495070457458496
epoch: 5 step: 36, loss is 8.009428024291992
epoch: 5 step: 37, loss is 7.8039231300354
epoch: 5 step: 38, loss is 7.369538307189941
epoch: 5 step: 39, loss is 7.777726650238037
epoch: 5 step: 40, loss is 7.703427314758301
epoch: 5 step: 41, loss is 7.3031182289123535
epoch: 5 step: 42, loss is 7.568367958068848
epoch: 5 step: 43, loss is 7.6059064865112305
epoch: 5 step: 44, loss is 8.162764549255371
epoch: 5 step: 45, loss is 7.759323596954346
epoch: 5 step: 46, loss is 7.888858795166016
epoch: 5 step: 47, loss is 7.510933876037598
epoch: 5 step: 48, loss is 7.936014175415039
epoch: 5 step: 49, loss is 8.244688034057617
epoch: 5 step: 50, loss is 7.4198079109191895
epoch: 5 step: 51, loss is 7.653848648071289
epoch: 5 step: 52, loss is 7.756627559661865
epoch: 5 step: 53, loss is 8.075139045715332
epoch: 5 step: 54, loss is 7.706898212432861
Train epoch time: 38615.839 ms, per step time: 715.108 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:19:37,782 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 7.645761489868164
2023-10-12 20:19:39,658 - INFO - valid loss : 7.645761489868164
epoch: 6 step: 1, loss is 7.617931842803955
epoch: 6 step: 2, loss is 7.372895240783691
epoch: 6 step: 3, loss is 7.685610771179199
epoch: 6 step: 4, loss is 8.033866882324219
epoch: 6 step: 5, loss is 7.775828838348389
epoch: 6 step: 6, loss is 7.490601062774658
epoch: 6 step: 7, loss is 7.597011566162109
epoch: 6 step: 8, loss is 7.615015506744385
epoch: 6 step: 9, loss is 7.5389933586120605
epoch: 6 step: 10, loss is 7.648386478424072
epoch: 6 step: 11, loss is 7.891703128814697
epoch: 6 step: 12, loss is 7.3784661293029785
epoch: 6 step: 13, loss is 7.510451316833496
epoch: 6 step: 14, loss is 7.318192481994629
epoch: 6 step: 15, loss is 7.578034400939941
epoch: 6 step: 16, loss is 7.891499996185303
epoch: 6 step: 17, loss is 7.821313381195068
epoch: 6 step: 18, loss is 7.435796737670898
epoch: 6 step: 19, loss is 7.232096195220947
epoch: 6 step: 20, loss is 7.390242099761963
epoch: 6 step: 21, loss is 7.434047222137451
epoch: 6 step: 22, loss is 7.541781425476074
epoch: 6 step: 23, loss is 8.122702598571777
epoch: 6 step: 24, loss is 7.311728477478027
epoch: 6 step: 25, loss is 7.743657112121582
epoch: 6 step: 26, loss is 7.771580696105957
epoch: 6 step: 27, loss is 7.7375617027282715
epoch: 6 step: 28, loss is 7.430846691131592
epoch: 6 step: 29, loss is 7.847331523895264
epoch: 6 step: 30, loss is 7.526339530944824
epoch: 6 step: 31, loss is 7.623087406158447
epoch: 6 step: 32, loss is 8.104804992675781
epoch: 6 step: 33, loss is 7.365051746368408
epoch: 6 step: 34, loss is 7.537569999694824
epoch: 6 step: 35, loss is 7.8130574226379395
epoch: 6 step: 36, loss is 7.762817859649658
epoch: 6 step: 37, loss is 7.5449442863464355
epoch: 6 step: 38, loss is 7.256789684295654
epoch: 6 step: 39, loss is 7.4669575691223145
epoch: 6 step: 40, loss is 8.346582412719727
epoch: 6 step: 41, loss is 7.459830284118652
epoch: 6 step: 42, loss is 7.645936489105225
epoch: 6 step: 43, loss is 7.39641809463501
epoch: 6 step: 44, loss is 7.9173736572265625
epoch: 6 step: 45, loss is 7.264552593231201
epoch: 6 step: 46, loss is 7.627570629119873
epoch: 6 step: 47, loss is 7.867918491363525
epoch: 6 step: 48, loss is 7.739865303039551
epoch: 6 step: 49, loss is 6.913938045501709
epoch: 6 step: 50, loss is 7.383306503295898
epoch: 6 step: 51, loss is 7.247859001159668
epoch: 6 step: 52, loss is 7.413907051086426
epoch: 6 step: 53, loss is 7.472166538238525
epoch: 6 step: 54, loss is 7.783607006072998
Train epoch time: 37577.502 ms, per step time: 695.880 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:20:17,238 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 7.477756023406982
2023-10-12 20:20:19,075 - INFO - valid loss : 7.477756023406982
epoch: 7 step: 1, loss is 7.385829448699951
epoch: 7 step: 2, loss is 7.478312969207764
epoch: 7 step: 3, loss is 7.23651123046875
epoch: 7 step: 4, loss is 7.2637810707092285
epoch: 7 step: 5, loss is 7.204868793487549
epoch: 7 step: 6, loss is 7.5331621170043945
epoch: 7 step: 7, loss is 7.726349353790283
epoch: 7 step: 8, loss is 7.4080491065979
epoch: 7 step: 9, loss is 7.410004138946533
epoch: 7 step: 10, loss is 7.553353786468506
epoch: 7 step: 11, loss is 7.771908760070801
epoch: 7 step: 12, loss is 7.414999008178711
epoch: 7 step: 13, loss is 7.848221778869629
epoch: 7 step: 14, loss is 7.473295211791992
epoch: 7 step: 15, loss is 7.093082904815674
epoch: 7 step: 16, loss is 7.376153945922852
epoch: 7 step: 17, loss is 7.611060619354248
epoch: 7 step: 18, loss is 7.431787014007568
epoch: 7 step: 19, loss is 7.547155380249023
epoch: 7 step: 20, loss is 7.36929178237915
epoch: 7 step: 21, loss is 7.597452640533447
epoch: 7 step: 22, loss is 7.463926792144775
epoch: 7 step: 23, loss is 7.299785614013672
epoch: 7 step: 24, loss is 7.548026084899902
epoch: 7 step: 25, loss is 7.088337421417236
epoch: 7 step: 26, loss is 7.582447528839111
epoch: 7 step: 27, loss is 7.22873067855835
epoch: 7 step: 28, loss is 7.113057613372803
epoch: 7 step: 29, loss is 7.5656890869140625
epoch: 7 step: 30, loss is 7.634610652923584
epoch: 7 step: 31, loss is 7.355456829071045
epoch: 7 step: 32, loss is 7.161290645599365
epoch: 7 step: 33, loss is 7.204294204711914
epoch: 7 step: 34, loss is 7.752207279205322
epoch: 7 step: 35, loss is 7.559131622314453
epoch: 7 step: 36, loss is 7.321256160736084
epoch: 7 step: 37, loss is 7.7508158683776855
epoch: 7 step: 38, loss is 7.202685832977295
epoch: 7 step: 39, loss is 7.274704456329346
epoch: 7 step: 40, loss is 7.233537673950195
epoch: 7 step: 41, loss is 7.269650459289551
epoch: 7 step: 42, loss is 7.464442253112793
epoch: 7 step: 43, loss is 7.249857425689697
epoch: 7 step: 44, loss is 7.506312370300293
epoch: 7 step: 45, loss is 7.3770670890808105
epoch: 7 step: 46, loss is 7.205636501312256
epoch: 7 step: 47, loss is 7.379324436187744
epoch: 7 step: 48, loss is 7.381019592285156
epoch: 7 step: 49, loss is 7.3148603439331055
epoch: 7 step: 50, loss is 7.329853057861328
epoch: 7 step: 51, loss is 6.9951605796813965
epoch: 7 step: 52, loss is 7.4164628982543945
epoch: 7 step: 53, loss is 7.415098667144775
epoch: 7 step: 54, loss is 7.242337703704834
Train epoch time: 38316.449 ms, per step time: 709.564 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:20:57,394 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 7.304636001586914
2023-10-12 20:20:59,303 - INFO - valid loss : 7.304636001586914
epoch: 8 step: 1, loss is 7.44380521774292
epoch: 8 step: 2, loss is 7.448319911956787
epoch: 8 step: 3, loss is 7.5525312423706055
epoch: 8 step: 4, loss is 7.350233554840088
epoch: 8 step: 5, loss is 7.273766994476318
epoch: 8 step: 6, loss is 7.123786449432373
epoch: 8 step: 7, loss is 7.421503067016602
epoch: 8 step: 8, loss is 7.296041488647461
epoch: 8 step: 9, loss is 7.046701431274414
epoch: 8 step: 10, loss is 7.044322490692139
epoch: 8 step: 11, loss is 6.963561534881592
epoch: 8 step: 12, loss is 7.255180835723877
epoch: 8 step: 13, loss is 7.563171863555908
epoch: 8 step: 14, loss is 7.201094150543213
epoch: 8 step: 15, loss is 7.102550029754639
epoch: 8 step: 16, loss is 7.75563383102417
epoch: 8 step: 17, loss is 7.569814205169678
epoch: 8 step: 18, loss is 7.563553810119629
epoch: 8 step: 19, loss is 7.378049373626709
epoch: 8 step: 20, loss is 7.114625453948975
epoch: 8 step: 21, loss is 6.9933977127075195
epoch: 8 step: 22, loss is 7.653440952301025
epoch: 8 step: 23, loss is 7.364957332611084
epoch: 8 step: 24, loss is 7.478671550750732
epoch: 8 step: 25, loss is 7.609166145324707
epoch: 8 step: 26, loss is 7.435502052307129
epoch: 8 step: 27, loss is 7.268736839294434
epoch: 8 step: 28, loss is 7.106651306152344
epoch: 8 step: 29, loss is 7.648746967315674
epoch: 8 step: 30, loss is 7.403767108917236
epoch: 8 step: 31, loss is 6.977295875549316
epoch: 8 step: 32, loss is 7.337608814239502
epoch: 8 step: 33, loss is 7.103080749511719
epoch: 8 step: 34, loss is 7.253144264221191
epoch: 8 step: 35, loss is 7.096096515655518
epoch: 8 step: 36, loss is 6.680420398712158
epoch: 8 step: 37, loss is 7.124092102050781
epoch: 8 step: 38, loss is 7.409824371337891
epoch: 8 step: 39, loss is 7.054147720336914
epoch: 8 step: 40, loss is 7.890987873077393
epoch: 8 step: 41, loss is 7.456092357635498
epoch: 8 step: 42, loss is 7.010928630828857
epoch: 8 step: 43, loss is 7.183663845062256
epoch: 8 step: 44, loss is 7.153930187225342
epoch: 8 step: 45, loss is 6.9939117431640625
epoch: 8 step: 46, loss is 7.303350925445557
epoch: 8 step: 47, loss is 7.263342380523682
epoch: 8 step: 48, loss is 6.789510726928711
epoch: 8 step: 49, loss is 6.847687721252441
epoch: 8 step: 50, loss is 7.332851886749268
epoch: 8 step: 51, loss is 7.652993679046631
epoch: 8 step: 52, loss is 7.005451202392578
epoch: 8 step: 53, loss is 7.308408737182617
epoch: 8 step: 54, loss is 7.5270819664001465
Train epoch time: 37236.396 ms, per step time: 689.563 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:21:36,541 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 7.243745803833008
2023-10-12 20:21:38,534 - INFO - valid loss : 7.243745803833008
epoch: 9 step: 1, loss is 6.992976188659668
epoch: 9 step: 2, loss is 7.236241340637207
epoch: 9 step: 3, loss is 7.379847526550293
epoch: 9 step: 4, loss is 7.110860824584961
epoch: 9 step: 5, loss is 7.2980217933654785
epoch: 9 step: 6, loss is 6.779952526092529
epoch: 9 step: 7, loss is 7.152667045593262
epoch: 9 step: 8, loss is 7.175887584686279
epoch: 9 step: 9, loss is 7.251562595367432
epoch: 9 step: 10, loss is 7.261019229888916
epoch: 9 step: 11, loss is 7.55038595199585
epoch: 9 step: 12, loss is 7.019010543823242
epoch: 9 step: 13, loss is 7.367728233337402
epoch: 9 step: 14, loss is 6.997184753417969
epoch: 9 step: 15, loss is 7.178244113922119
epoch: 9 step: 16, loss is 7.329763412475586
epoch: 9 step: 17, loss is 7.440556526184082
epoch: 9 step: 18, loss is 7.236996650695801
epoch: 9 step: 19, loss is 7.89151668548584
epoch: 9 step: 20, loss is 7.136260509490967
epoch: 9 step: 21, loss is 7.194216728210449
epoch: 9 step: 22, loss is 6.650700569152832
epoch: 9 step: 23, loss is 7.498691558837891
epoch: 9 step: 24, loss is 7.354504108428955
epoch: 9 step: 25, loss is 7.182528018951416
epoch: 9 step: 26, loss is 6.910128116607666
epoch: 9 step: 27, loss is 7.245319843292236
epoch: 9 step: 28, loss is 6.828212261199951
epoch: 9 step: 29, loss is 6.809816837310791
epoch: 9 step: 30, loss is 6.99151611328125
epoch: 9 step: 31, loss is 7.336790084838867
epoch: 9 step: 32, loss is 7.073194980621338
epoch: 9 step: 33, loss is 6.9292755126953125
epoch: 9 step: 34, loss is 6.925429344177246
epoch: 9 step: 35, loss is 7.154074192047119
epoch: 9 step: 36, loss is 6.95938777923584
epoch: 9 step: 37, loss is 7.487527847290039
epoch: 9 step: 38, loss is 7.259362697601318
epoch: 9 step: 39, loss is 7.357304096221924
epoch: 9 step: 40, loss is 6.820856094360352
epoch: 9 step: 41, loss is 7.159708023071289
epoch: 9 step: 42, loss is 6.967753887176514
epoch: 9 step: 43, loss is 6.772100448608398
epoch: 9 step: 44, loss is 6.964624881744385
epoch: 9 step: 45, loss is 6.960268974304199
epoch: 9 step: 46, loss is 7.1603593826293945
epoch: 9 step: 47, loss is 6.9037675857543945
epoch: 9 step: 48, loss is 7.3094611167907715
epoch: 9 step: 49, loss is 7.5903000831604
epoch: 9 step: 50, loss is 6.977324962615967
epoch: 9 step: 51, loss is 6.91650915145874
epoch: 9 step: 52, loss is 7.010514736175537
epoch: 9 step: 53, loss is 6.961702823638916
epoch: 9 step: 54, loss is 7.072963714599609
Train epoch time: 38096.663 ms, per step time: 705.494 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:22:16,633 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.966192245483398
2023-10-12 20:22:18,522 - INFO - valid loss : 6.966192245483398
epoch: 10 step: 1, loss is 6.992568492889404
epoch: 10 step: 2, loss is 6.764569282531738
epoch: 10 step: 3, loss is 6.924123287200928
epoch: 10 step: 4, loss is 7.398072719573975
epoch: 10 step: 5, loss is 6.911404132843018
epoch: 10 step: 6, loss is 7.124386310577393
epoch: 10 step: 7, loss is 6.741494178771973
epoch: 10 step: 8, loss is 6.810711860656738
epoch: 10 step: 9, loss is 7.42039155960083
epoch: 10 step: 10, loss is 7.544308185577393
epoch: 10 step: 11, loss is 7.182380676269531
epoch: 10 step: 12, loss is 7.2450714111328125
epoch: 10 step: 13, loss is 7.049781322479248
epoch: 10 step: 14, loss is 7.024161338806152
epoch: 10 step: 15, loss is 7.409441947937012
epoch: 10 step: 16, loss is 7.135304927825928
epoch: 10 step: 17, loss is 7.10892391204834
epoch: 10 step: 18, loss is 6.784002304077148
epoch: 10 step: 19, loss is 7.34152889251709
epoch: 10 step: 20, loss is 7.332141876220703
epoch: 10 step: 21, loss is 7.133143424987793
epoch: 10 step: 22, loss is 6.6152191162109375
epoch: 10 step: 23, loss is 7.0744147300720215
epoch: 10 step: 24, loss is 6.83482551574707
epoch: 10 step: 25, loss is 6.959458827972412
epoch: 10 step: 26, loss is 7.262436389923096
epoch: 10 step: 27, loss is 6.804682731628418
epoch: 10 step: 28, loss is 6.935133457183838
epoch: 10 step: 29, loss is 7.375248908996582
epoch: 10 step: 30, loss is 7.022972583770752
epoch: 10 step: 31, loss is 6.605961799621582
epoch: 10 step: 32, loss is 7.021910667419434
epoch: 10 step: 33, loss is 7.458634376525879
epoch: 10 step: 34, loss is 6.8094096183776855
epoch: 10 step: 35, loss is 7.036351203918457
epoch: 10 step: 36, loss is 7.1727118492126465
epoch: 10 step: 37, loss is 6.8814311027526855
epoch: 10 step: 38, loss is 6.850740432739258
epoch: 10 step: 39, loss is 7.33591890335083
epoch: 10 step: 40, loss is 7.333143711090088
epoch: 10 step: 41, loss is 6.555464267730713
epoch: 10 step: 42, loss is 7.07162618637085
epoch: 10 step: 43, loss is 6.847894191741943
epoch: 10 step: 44, loss is 7.225320339202881
epoch: 10 step: 45, loss is 7.099512100219727
epoch: 10 step: 46, loss is 7.026671886444092
epoch: 10 step: 47, loss is 6.761423110961914
epoch: 10 step: 48, loss is 6.809295654296875
epoch: 10 step: 49, loss is 6.819698333740234
epoch: 10 step: 50, loss is 6.64652681350708
epoch: 10 step: 51, loss is 6.774234771728516
epoch: 10 step: 52, loss is 6.9595112800598145
epoch: 10 step: 53, loss is 6.913786888122559
epoch: 10 step: 54, loss is 7.615412712097168
Train epoch time: 36644.083 ms, per step time: 678.594 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:22:55,167 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.8354926109313965
2023-10-12 20:22:56,927 - INFO - valid loss : 6.8354926109313965
epoch: 11 step: 1, loss is 6.626369953155518
epoch: 11 step: 2, loss is 6.602534770965576
epoch: 11 step: 3, loss is 6.959140777587891
epoch: 11 step: 4, loss is 6.913422107696533
epoch: 11 step: 5, loss is 7.104621410369873
epoch: 11 step: 6, loss is 6.893721103668213
epoch: 11 step: 7, loss is 7.0662431716918945
epoch: 11 step: 8, loss is 7.017930030822754
epoch: 11 step: 9, loss is 7.187949180603027
epoch: 11 step: 10, loss is 6.972894191741943
epoch: 11 step: 11, loss is 6.688939094543457
epoch: 11 step: 12, loss is 7.093591690063477
epoch: 11 step: 13, loss is 7.744901180267334
epoch: 11 step: 14, loss is 6.730879306793213
epoch: 11 step: 15, loss is 7.038264274597168
epoch: 11 step: 16, loss is 6.832080364227295
epoch: 11 step: 17, loss is 6.873806953430176
epoch: 11 step: 18, loss is 6.768243312835693
epoch: 11 step: 19, loss is 7.377261638641357
epoch: 11 step: 20, loss is 7.059689998626709
epoch: 11 step: 21, loss is 6.984720230102539
epoch: 11 step: 22, loss is 6.7244696617126465
epoch: 11 step: 23, loss is 6.713461399078369
epoch: 11 step: 24, loss is 7.208734512329102
epoch: 11 step: 25, loss is 6.731110095977783
epoch: 11 step: 26, loss is 7.25575065612793
epoch: 11 step: 27, loss is 7.080244541168213
epoch: 11 step: 28, loss is 6.941829204559326
epoch: 11 step: 29, loss is 6.837716102600098
epoch: 11 step: 30, loss is 6.83834171295166
epoch: 11 step: 31, loss is 7.299693584442139
epoch: 11 step: 32, loss is 6.702186107635498
epoch: 11 step: 33, loss is 6.768247604370117
epoch: 11 step: 34, loss is 6.71088981628418
epoch: 11 step: 35, loss is 7.031575679779053
epoch: 11 step: 36, loss is 6.80733585357666
epoch: 11 step: 37, loss is 7.328396797180176
epoch: 11 step: 38, loss is 6.83767032623291
epoch: 11 step: 39, loss is 6.961645126342773
epoch: 11 step: 40, loss is 6.702249526977539
epoch: 11 step: 41, loss is 6.843602180480957
epoch: 11 step: 42, loss is 6.4791059494018555
epoch: 11 step: 43, loss is 7.163495063781738
epoch: 11 step: 44, loss is 6.823270797729492
epoch: 11 step: 45, loss is 6.7519850730896
epoch: 11 step: 46, loss is 7.525843620300293
epoch: 11 step: 47, loss is 6.9473795890808105
epoch: 11 step: 48, loss is 6.627195358276367
epoch: 11 step: 49, loss is 7.0479736328125
epoch: 11 step: 50, loss is 6.87061071395874
epoch: 11 step: 51, loss is 7.149096488952637
epoch: 11 step: 52, loss is 6.807055473327637
epoch: 11 step: 53, loss is 6.911965847015381
epoch: 11 step: 54, loss is 6.5357465744018555
Train epoch time: 36618.795 ms, per step time: 678.126 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:23:33,547 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.69456672668457
2023-10-12 20:23:35,282 - INFO - valid loss : 6.69456672668457
epoch: 12 step: 1, loss is 6.903303146362305
epoch: 12 step: 2, loss is 6.729106903076172
epoch: 12 step: 3, loss is 6.586357593536377
epoch: 12 step: 4, loss is 6.632967472076416
epoch: 12 step: 5, loss is 7.49479341506958
epoch: 12 step: 6, loss is 6.5374298095703125
epoch: 12 step: 7, loss is 6.5013298988342285
epoch: 12 step: 8, loss is 6.768321514129639
epoch: 12 step: 9, loss is 7.169239044189453
epoch: 12 step: 10, loss is 6.730010032653809
epoch: 12 step: 11, loss is 6.8982157707214355
epoch: 12 step: 12, loss is 6.784670352935791
epoch: 12 step: 13, loss is 7.381432056427002
epoch: 12 step: 14, loss is 7.158964157104492
epoch: 12 step: 15, loss is 7.00555944442749
epoch: 12 step: 16, loss is 6.774969100952148
epoch: 12 step: 17, loss is 7.2515363693237305
epoch: 12 step: 18, loss is 6.978579998016357
epoch: 12 step: 19, loss is 6.978289604187012
epoch: 12 step: 20, loss is 6.704297065734863
epoch: 12 step: 21, loss is 6.873713970184326
epoch: 12 step: 22, loss is 6.788905143737793
epoch: 12 step: 23, loss is 6.8206586837768555
epoch: 12 step: 24, loss is 6.503073215484619
epoch: 12 step: 25, loss is 6.486245155334473
epoch: 12 step: 26, loss is 6.886432647705078
epoch: 12 step: 27, loss is 6.987966537475586
epoch: 12 step: 28, loss is 6.977773666381836
epoch: 12 step: 29, loss is 6.595491409301758
epoch: 12 step: 30, loss is 6.738743305206299
epoch: 12 step: 31, loss is 6.515329837799072
epoch: 12 step: 32, loss is 6.848297119140625
epoch: 12 step: 33, loss is 6.441060543060303
epoch: 12 step: 34, loss is 6.8812456130981445
epoch: 12 step: 35, loss is 7.311133861541748
epoch: 12 step: 36, loss is 6.896135330200195
epoch: 12 step: 37, loss is 6.716055393218994
epoch: 12 step: 38, loss is 7.059834003448486
epoch: 12 step: 39, loss is 6.639327526092529
epoch: 12 step: 40, loss is 6.649866580963135
epoch: 12 step: 41, loss is 6.583224296569824
epoch: 12 step: 42, loss is 6.7694783210754395
epoch: 12 step: 43, loss is 6.662524700164795
epoch: 12 step: 44, loss is 6.718611717224121
epoch: 12 step: 45, loss is 6.6547417640686035
epoch: 12 step: 46, loss is 6.491028785705566
epoch: 12 step: 47, loss is 6.894412517547607
epoch: 12 step: 48, loss is 7.085431098937988
epoch: 12 step: 49, loss is 6.45732307434082
epoch: 12 step: 50, loss is 6.882043361663818
epoch: 12 step: 51, loss is 6.686884880065918
epoch: 12 step: 52, loss is 6.914848804473877
epoch: 12 step: 53, loss is 6.709589004516602
epoch: 12 step: 54, loss is 6.704400062561035
Train epoch time: 36465.418 ms, per step time: 675.286 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:24:11,748 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.569364547729492
2023-10-12 20:24:13,505 - INFO - valid loss : 6.569364547729492
epoch: 13 step: 1, loss is 6.476687431335449
epoch: 13 step: 2, loss is 7.139662265777588
epoch: 13 step: 3, loss is 6.4098381996154785
epoch: 13 step: 4, loss is 6.545338153839111
epoch: 13 step: 5, loss is 6.534449100494385
epoch: 13 step: 6, loss is 6.76846981048584
epoch: 13 step: 7, loss is 6.745970249176025
epoch: 13 step: 8, loss is 6.42573881149292
epoch: 13 step: 9, loss is 6.975111484527588
epoch: 13 step: 10, loss is 6.865988731384277
epoch: 13 step: 11, loss is 6.840433597564697
epoch: 13 step: 12, loss is 6.556094646453857
epoch: 13 step: 13, loss is 6.696738243103027
epoch: 13 step: 14, loss is 6.855438232421875
epoch: 13 step: 15, loss is 6.758394718170166
epoch: 13 step: 16, loss is 6.397845268249512
epoch: 13 step: 17, loss is 6.58638334274292
epoch: 13 step: 18, loss is 7.043918609619141
epoch: 13 step: 19, loss is 6.804766654968262
epoch: 13 step: 20, loss is 6.4709014892578125
epoch: 13 step: 21, loss is 7.0192646980285645
epoch: 13 step: 22, loss is 7.164614677429199
epoch: 13 step: 23, loss is 6.476987838745117
epoch: 13 step: 24, loss is 6.364500522613525
epoch: 13 step: 25, loss is 6.762333393096924
epoch: 13 step: 26, loss is 6.534797191619873
epoch: 13 step: 27, loss is 6.584484100341797
epoch: 13 step: 28, loss is 6.62271785736084
epoch: 13 step: 29, loss is 6.465158462524414
epoch: 13 step: 30, loss is 6.648797988891602
epoch: 13 step: 31, loss is 6.669477462768555
epoch: 13 step: 32, loss is 7.186878204345703
epoch: 13 step: 33, loss is 6.844332695007324
epoch: 13 step: 34, loss is 6.488759994506836
epoch: 13 step: 35, loss is 6.899694919586182
epoch: 13 step: 36, loss is 6.624330520629883
epoch: 13 step: 37, loss is 6.706816673278809
epoch: 13 step: 38, loss is 6.5401787757873535
epoch: 13 step: 39, loss is 7.027608871459961
epoch: 13 step: 40, loss is 6.818607330322266
epoch: 13 step: 41, loss is 6.680553436279297
epoch: 13 step: 42, loss is 6.605740070343018
epoch: 13 step: 43, loss is 6.2638163566589355
epoch: 13 step: 44, loss is 7.017753601074219
epoch: 13 step: 45, loss is 6.687193393707275
epoch: 13 step: 46, loss is 7.125051021575928
epoch: 13 step: 47, loss is 6.645818710327148
epoch: 13 step: 48, loss is 6.582324028015137
epoch: 13 step: 49, loss is 6.697516918182373
epoch: 13 step: 50, loss is 6.74608039855957
epoch: 13 step: 51, loss is 6.508124828338623
epoch: 13 step: 52, loss is 6.604115009307861
epoch: 13 step: 53, loss is 6.620907306671143
epoch: 13 step: 54, loss is 6.80634069442749
Train epoch time: 36805.036 ms, per step time: 681.575 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:24:50,311 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.528419494628906
2023-10-12 20:24:52,194 - INFO - valid loss : 6.528419494628906
epoch: 14 step: 1, loss is 6.658041954040527
epoch: 14 step: 2, loss is 6.851271152496338
epoch: 14 step: 3, loss is 6.7937541007995605
epoch: 14 step: 4, loss is 6.913626194000244
epoch: 14 step: 5, loss is 6.421189308166504
epoch: 14 step: 6, loss is 6.413821220397949
epoch: 14 step: 7, loss is 6.59080696105957
epoch: 14 step: 8, loss is 6.766383171081543
epoch: 14 step: 9, loss is 6.407045364379883
epoch: 14 step: 10, loss is 6.893259525299072
epoch: 14 step: 11, loss is 6.277446746826172
epoch: 14 step: 12, loss is 6.554478168487549
epoch: 14 step: 13, loss is 6.670222282409668
epoch: 14 step: 14, loss is 6.624922275543213
epoch: 14 step: 15, loss is 6.646831035614014
epoch: 14 step: 16, loss is 6.962406158447266
epoch: 14 step: 17, loss is 6.798367500305176
epoch: 14 step: 18, loss is 6.5652570724487305
epoch: 14 step: 19, loss is 6.416892051696777
epoch: 14 step: 20, loss is 6.532224655151367
epoch: 14 step: 21, loss is 6.617467880249023
epoch: 14 step: 22, loss is 6.783783435821533
epoch: 14 step: 23, loss is 6.614276885986328
epoch: 14 step: 24, loss is 6.6011505126953125
epoch: 14 step: 25, loss is 6.352001667022705
epoch: 14 step: 26, loss is 7.328566551208496
epoch: 14 step: 27, loss is 6.783761978149414
epoch: 14 step: 28, loss is 6.6464152336120605
epoch: 14 step: 29, loss is 6.866846084594727
epoch: 14 step: 30, loss is 6.858292102813721
epoch: 14 step: 31, loss is 6.60875129699707
epoch: 14 step: 32, loss is 6.5799736976623535
epoch: 14 step: 33, loss is 6.541537284851074
epoch: 14 step: 34, loss is 6.682750225067139
epoch: 14 step: 35, loss is 7.036771297454834
epoch: 14 step: 36, loss is 6.838868618011475
epoch: 14 step: 37, loss is 7.078546524047852
epoch: 14 step: 38, loss is 6.67603874206543
epoch: 14 step: 39, loss is 6.464710712432861
epoch: 14 step: 40, loss is 6.555166244506836
epoch: 14 step: 41, loss is 6.647922515869141
epoch: 14 step: 42, loss is 6.866737365722656
epoch: 14 step: 43, loss is 6.526810169219971
epoch: 14 step: 44, loss is 6.286515712738037
epoch: 14 step: 45, loss is 6.78773307800293
epoch: 14 step: 46, loss is 6.689509391784668
epoch: 14 step: 47, loss is 6.2559285163879395
epoch: 14 step: 48, loss is 6.319526672363281
epoch: 14 step: 49, loss is 6.269524097442627
epoch: 14 step: 50, loss is 6.802303791046143
epoch: 14 step: 51, loss is 6.688586235046387
epoch: 14 step: 52, loss is 7.126763820648193
epoch: 14 step: 53, loss is 6.636536598205566
epoch: 14 step: 54, loss is 6.829959392547607
Train epoch time: 37595.616 ms, per step time: 696.215 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:25:29,791 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.333675861358643
2023-10-12 20:25:31,740 - INFO - valid loss : 6.333675861358643
epoch: 15 step: 1, loss is 6.350687026977539
epoch: 15 step: 2, loss is 6.840076923370361
epoch: 15 step: 3, loss is 7.150173664093018
epoch: 15 step: 4, loss is 6.933947563171387
epoch: 15 step: 5, loss is 6.815803527832031
epoch: 15 step: 6, loss is 6.439925670623779
epoch: 15 step: 7, loss is 6.58305549621582
epoch: 15 step: 8, loss is 6.582615375518799
epoch: 15 step: 9, loss is 6.364387512207031
epoch: 15 step: 10, loss is 6.714790344238281
epoch: 15 step: 11, loss is 6.959675312042236
epoch: 15 step: 12, loss is 6.744219779968262
epoch: 15 step: 13, loss is 6.449304580688477
epoch: 15 step: 14, loss is 6.861676216125488
epoch: 15 step: 15, loss is 6.384237289428711
epoch: 15 step: 16, loss is 6.3997697830200195
epoch: 15 step: 17, loss is 6.834084510803223
epoch: 15 step: 18, loss is 6.726828098297119
epoch: 15 step: 19, loss is 6.397698879241943
epoch: 15 step: 20, loss is 6.71062707901001
epoch: 15 step: 21, loss is 6.2750654220581055
epoch: 15 step: 22, loss is 6.322662353515625
epoch: 15 step: 23, loss is 6.4486589431762695
epoch: 15 step: 24, loss is 6.417052268981934
epoch: 15 step: 25, loss is 6.458359241485596
epoch: 15 step: 26, loss is 6.6216630935668945
epoch: 15 step: 27, loss is 6.896815776824951
epoch: 15 step: 28, loss is 6.434189796447754
epoch: 15 step: 29, loss is 6.562119960784912
epoch: 15 step: 30, loss is 6.420119762420654
epoch: 15 step: 31, loss is 6.830101490020752
epoch: 15 step: 32, loss is 6.66605806350708
epoch: 15 step: 33, loss is 6.685078144073486
epoch: 15 step: 34, loss is 6.585237503051758
epoch: 15 step: 35, loss is 6.501883029937744
epoch: 15 step: 36, loss is 6.736658573150635
epoch: 15 step: 37, loss is 6.685450077056885
epoch: 15 step: 38, loss is 6.470541000366211
epoch: 15 step: 39, loss is 6.68151330947876
epoch: 15 step: 40, loss is 6.649686813354492
epoch: 15 step: 41, loss is 6.4802680015563965
epoch: 15 step: 42, loss is 6.0973310470581055
epoch: 15 step: 43, loss is 6.351661682128906
epoch: 15 step: 44, loss is 6.594788551330566
epoch: 15 step: 45, loss is 6.401230812072754
epoch: 15 step: 46, loss is 6.148377895355225
epoch: 15 step: 47, loss is 6.553150177001953
epoch: 15 step: 48, loss is 6.376481056213379
epoch: 15 step: 49, loss is 6.529397964477539
epoch: 15 step: 50, loss is 6.373569965362549
epoch: 15 step: 51, loss is 6.32316780090332
epoch: 15 step: 52, loss is 6.269443035125732
epoch: 15 step: 53, loss is 6.886465549468994
epoch: 15 step: 54, loss is 6.754458427429199
Train epoch time: 37286.319 ms, per step time: 690.487 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:26:09,027 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.150813579559326
2023-10-12 20:26:10,697 - INFO - valid loss : 6.150813579559326
epoch: 16 step: 1, loss is 6.714790344238281
epoch: 16 step: 2, loss is 6.336946964263916
epoch: 16 step: 3, loss is 6.521486759185791
epoch: 16 step: 4, loss is 6.317291259765625
epoch: 16 step: 5, loss is 6.565937519073486
epoch: 16 step: 6, loss is 6.689184665679932
epoch: 16 step: 7, loss is 6.810742378234863
epoch: 16 step: 8, loss is 6.300434589385986
epoch: 16 step: 9, loss is 6.5768961906433105
epoch: 16 step: 10, loss is 6.455508232116699
epoch: 16 step: 11, loss is 6.8407135009765625
epoch: 16 step: 12, loss is 6.639385223388672
epoch: 16 step: 13, loss is 6.03000545501709
epoch: 16 step: 14, loss is 6.497176170349121
epoch: 16 step: 15, loss is 6.1738080978393555
epoch: 16 step: 16, loss is 6.339679718017578
epoch: 16 step: 17, loss is 5.9766154289245605
epoch: 16 step: 18, loss is 7.100326061248779
epoch: 16 step: 19, loss is 6.298289775848389
epoch: 16 step: 20, loss is 7.21343469619751
epoch: 16 step: 21, loss is 6.474837779998779
epoch: 16 step: 22, loss is 6.178498268127441
epoch: 16 step: 23, loss is 6.499588966369629
epoch: 16 step: 24, loss is 6.658115863800049
epoch: 16 step: 25, loss is 6.348974227905273
epoch: 16 step: 26, loss is 6.509647369384766
epoch: 16 step: 27, loss is 6.462730407714844
epoch: 16 step: 28, loss is 6.323890209197998
epoch: 16 step: 29, loss is 7.0063347816467285
epoch: 16 step: 30, loss is 6.6278977394104
epoch: 16 step: 31, loss is 6.318556785583496
epoch: 16 step: 32, loss is 6.299543380737305
epoch: 16 step: 33, loss is 6.62004280090332
epoch: 16 step: 34, loss is 6.155547618865967
epoch: 16 step: 35, loss is 6.398938179016113
epoch: 16 step: 36, loss is 6.343288421630859
epoch: 16 step: 37, loss is 6.203182220458984
epoch: 16 step: 38, loss is 6.360650539398193
epoch: 16 step: 39, loss is 5.7027177810668945
epoch: 16 step: 40, loss is 6.918122291564941
epoch: 16 step: 41, loss is 6.219529151916504
epoch: 16 step: 42, loss is 6.782296657562256
epoch: 16 step: 43, loss is 6.21107292175293
epoch: 16 step: 44, loss is 6.8660149574279785
epoch: 16 step: 45, loss is 6.332513332366943
epoch: 16 step: 46, loss is 6.698914051055908
epoch: 16 step: 47, loss is 6.734240531921387
epoch: 16 step: 48, loss is 6.29193115234375
epoch: 16 step: 49, loss is 6.590725421905518
epoch: 16 step: 50, loss is 6.953389644622803
epoch: 16 step: 51, loss is 6.5874104499816895
epoch: 16 step: 52, loss is 6.639695644378662
epoch: 16 step: 53, loss is 6.40449333190918
epoch: 16 step: 54, loss is 6.42117977142334
Train epoch time: 37266.878 ms, per step time: 690.127 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:26:47,965 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.35614538192749
2023-10-12 20:26:49,940 - INFO - valid loss : 6.35614538192749
epoch: 17 step: 1, loss is 6.3578619956970215
epoch: 17 step: 2, loss is 6.464558124542236
epoch: 17 step: 3, loss is 6.674339294433594
epoch: 17 step: 4, loss is 6.670577049255371
epoch: 17 step: 5, loss is 6.271424293518066
epoch: 17 step: 6, loss is 6.372744560241699
epoch: 17 step: 7, loss is 6.3018083572387695
epoch: 17 step: 8, loss is 6.9198431968688965
epoch: 17 step: 9, loss is 6.334317207336426
epoch: 17 step: 10, loss is 6.507555961608887
epoch: 17 step: 11, loss is 6.3276190757751465
epoch: 17 step: 12, loss is 6.66475248336792
epoch: 17 step: 13, loss is 6.321518898010254
epoch: 17 step: 14, loss is 6.271668910980225
epoch: 17 step: 15, loss is 6.123641490936279
epoch: 17 step: 16, loss is 6.364686965942383
epoch: 17 step: 17, loss is 6.39111328125
epoch: 17 step: 18, loss is 6.377889633178711
epoch: 17 step: 19, loss is 6.773590087890625
epoch: 17 step: 20, loss is 6.611174583435059
epoch: 17 step: 21, loss is 6.620893955230713
epoch: 17 step: 22, loss is 6.321400165557861
epoch: 17 step: 23, loss is 6.697454929351807
epoch: 17 step: 24, loss is 6.403944492340088
epoch: 17 step: 25, loss is 6.205564022064209
epoch: 17 step: 26, loss is 6.714166164398193
epoch: 17 step: 27, loss is 5.934329509735107
epoch: 17 step: 28, loss is 6.861615180969238
epoch: 17 step: 29, loss is 6.661628246307373
epoch: 17 step: 30, loss is 6.622136116027832
epoch: 17 step: 31, loss is 6.544443607330322
epoch: 17 step: 32, loss is 6.73947286605835
epoch: 17 step: 33, loss is 6.192052841186523
epoch: 17 step: 34, loss is 6.373286247253418
epoch: 17 step: 35, loss is 6.143448352813721
epoch: 17 step: 36, loss is 6.163373947143555
epoch: 17 step: 37, loss is 6.4624505043029785
epoch: 17 step: 38, loss is 6.478655815124512
epoch: 17 step: 39, loss is 6.889939785003662
epoch: 17 step: 40, loss is 6.392573356628418
epoch: 17 step: 41, loss is 6.315326690673828
epoch: 17 step: 42, loss is 6.356142997741699
epoch: 17 step: 43, loss is 6.031983852386475
epoch: 17 step: 44, loss is 6.52076530456543
epoch: 17 step: 45, loss is 6.313815116882324
epoch: 17 step: 46, loss is 6.779392242431641
epoch: 17 step: 47, loss is 6.333386421203613
epoch: 17 step: 48, loss is 6.227137088775635
epoch: 17 step: 49, loss is 6.1682538986206055
epoch: 17 step: 50, loss is 6.348509788513184
epoch: 17 step: 51, loss is 6.476740837097168
epoch: 17 step: 52, loss is 6.51315450668335
epoch: 17 step: 53, loss is 6.498287677764893
epoch: 17 step: 54, loss is 6.060624122619629
Train epoch time: 36996.498 ms, per step time: 685.120 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:27:26,938 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.995727062225342
2023-10-12 20:27:28,816 - INFO - valid loss : 5.995727062225342
epoch: 18 step: 1, loss is 6.468539714813232
epoch: 18 step: 2, loss is 6.61689567565918
epoch: 18 step: 3, loss is 6.608294486999512
epoch: 18 step: 4, loss is 6.539455890655518
epoch: 18 step: 5, loss is 5.996088981628418
epoch: 18 step: 6, loss is 6.287692070007324
epoch: 18 step: 7, loss is 6.57034969329834
epoch: 18 step: 8, loss is 6.480055809020996
epoch: 18 step: 9, loss is 6.397393226623535
epoch: 18 step: 10, loss is 6.067737102508545
epoch: 18 step: 11, loss is 6.267462730407715
epoch: 18 step: 12, loss is 6.391353130340576
epoch: 18 step: 13, loss is 6.184274196624756
epoch: 18 step: 14, loss is 6.358085632324219
epoch: 18 step: 15, loss is 6.073939323425293
epoch: 18 step: 16, loss is 6.356328964233398
epoch: 18 step: 17, loss is 6.11010217666626
epoch: 18 step: 18, loss is 6.403726100921631
epoch: 18 step: 19, loss is 6.54070520401001
epoch: 18 step: 20, loss is 6.295559406280518
epoch: 18 step: 21, loss is 6.359496116638184
epoch: 18 step: 22, loss is 6.347583770751953
epoch: 18 step: 23, loss is 6.184436798095703
epoch: 18 step: 24, loss is 6.498035430908203
epoch: 18 step: 25, loss is 6.204635143280029
epoch: 18 step: 26, loss is 6.211095809936523
epoch: 18 step: 27, loss is 6.090019702911377
epoch: 18 step: 28, loss is 6.246660232543945
epoch: 18 step: 29, loss is 6.202751636505127
epoch: 18 step: 30, loss is 6.611245155334473
epoch: 18 step: 31, loss is 6.199468612670898
epoch: 18 step: 32, loss is 6.487008571624756
epoch: 18 step: 33, loss is 6.5350236892700195
epoch: 18 step: 34, loss is 6.817458152770996
epoch: 18 step: 35, loss is 6.212360382080078
epoch: 18 step: 36, loss is 6.517723083496094
epoch: 18 step: 37, loss is 6.40700626373291
epoch: 18 step: 38, loss is 6.841566562652588
epoch: 18 step: 39, loss is 6.138383865356445
epoch: 18 step: 40, loss is 6.668727397918701
epoch: 18 step: 41, loss is 5.907680511474609
epoch: 18 step: 42, loss is 6.473925590515137
epoch: 18 step: 43, loss is 6.73148250579834
epoch: 18 step: 44, loss is 6.550154209136963
epoch: 18 step: 45, loss is 6.787139892578125
epoch: 18 step: 46, loss is 6.237832546234131
epoch: 18 step: 47, loss is 6.095809459686279
epoch: 18 step: 48, loss is 6.242498874664307
epoch: 18 step: 49, loss is 6.022250652313232
epoch: 18 step: 50, loss is 6.525790214538574
epoch: 18 step: 51, loss is 6.12044095993042
epoch: 18 step: 52, loss is 6.437076091766357
epoch: 18 step: 53, loss is 6.15054988861084
epoch: 18 step: 54, loss is 6.3703532218933105
Train epoch time: 36916.525 ms, per step time: 683.639 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:28:05,734 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.114757537841797
2023-10-12 20:28:07,606 - INFO - valid loss : 6.114757537841797
epoch: 19 step: 1, loss is 6.44056510925293
epoch: 19 step: 2, loss is 6.424010753631592
epoch: 19 step: 3, loss is 6.411559104919434
epoch: 19 step: 4, loss is 6.263321399688721
epoch: 19 step: 5, loss is 6.499269485473633
epoch: 19 step: 6, loss is 6.522408962249756
epoch: 19 step: 7, loss is 6.468239784240723
epoch: 19 step: 8, loss is 6.213237762451172
epoch: 19 step: 9, loss is 6.5283074378967285
epoch: 19 step: 10, loss is 6.339231014251709
epoch: 19 step: 11, loss is 6.69251012802124
epoch: 19 step: 12, loss is 6.408397674560547
epoch: 19 step: 13, loss is 6.259822368621826
epoch: 19 step: 14, loss is 6.010613918304443
epoch: 19 step: 15, loss is 6.689537525177002
epoch: 19 step: 16, loss is 5.9519805908203125
epoch: 19 step: 17, loss is 6.185837745666504
epoch: 19 step: 18, loss is 6.45396614074707
epoch: 19 step: 19, loss is 6.192357063293457
epoch: 19 step: 20, loss is 6.249295711517334
epoch: 19 step: 21, loss is 5.8153510093688965
epoch: 19 step: 22, loss is 5.973586082458496
epoch: 19 step: 23, loss is 6.454810619354248
epoch: 19 step: 24, loss is 5.960076808929443
epoch: 19 step: 25, loss is 6.1177473068237305
epoch: 19 step: 26, loss is 6.687448024749756
epoch: 19 step: 27, loss is 6.28708028793335
epoch: 19 step: 28, loss is 6.062269687652588
epoch: 19 step: 29, loss is 5.848447322845459
epoch: 19 step: 30, loss is 6.959417343139648
epoch: 19 step: 31, loss is 6.524460315704346
epoch: 19 step: 32, loss is 6.463508605957031
epoch: 19 step: 33, loss is 6.809046745300293
epoch: 19 step: 34, loss is 6.069031715393066
epoch: 19 step: 35, loss is 6.181681156158447
epoch: 19 step: 36, loss is 6.420766353607178
epoch: 19 step: 37, loss is 6.327169418334961
epoch: 19 step: 38, loss is 6.49032735824585
epoch: 19 step: 39, loss is 6.386591911315918
epoch: 19 step: 40, loss is 6.388608455657959
epoch: 19 step: 41, loss is 6.741323947906494
epoch: 19 step: 42, loss is 6.445708751678467
epoch: 19 step: 43, loss is 6.442729949951172
epoch: 19 step: 44, loss is 6.01715087890625
epoch: 19 step: 45, loss is 5.98728084564209
epoch: 19 step: 46, loss is 6.150468349456787
epoch: 19 step: 47, loss is 6.11221981048584
epoch: 19 step: 48, loss is 6.401205539703369
epoch: 19 step: 49, loss is 6.306710243225098
epoch: 19 step: 50, loss is 6.1488447189331055
epoch: 19 step: 51, loss is 6.081420421600342
epoch: 19 step: 52, loss is 6.4747700691223145
epoch: 19 step: 53, loss is 6.412173748016357
epoch: 19 step: 54, loss is 6.470241546630859
Train epoch time: 37057.897 ms, per step time: 686.257 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:28:44,666 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.044375896453857
2023-10-12 20:28:46,520 - INFO - valid loss : 6.044375896453857
epoch: 20 step: 1, loss is 6.365716457366943
epoch: 20 step: 2, loss is 6.027296543121338
epoch: 20 step: 3, loss is 6.364866733551025
epoch: 20 step: 4, loss is 6.13821268081665
epoch: 20 step: 5, loss is 6.444007396697998
epoch: 20 step: 6, loss is 6.363622665405273
epoch: 20 step: 7, loss is 6.247406005859375
epoch: 20 step: 8, loss is 6.359405994415283
epoch: 20 step: 9, loss is 6.309250354766846
epoch: 20 step: 10, loss is 6.289229869842529
epoch: 20 step: 11, loss is 6.385068893432617
epoch: 20 step: 12, loss is 6.243441104888916
epoch: 20 step: 13, loss is 6.356793403625488
epoch: 20 step: 14, loss is 6.351975917816162
epoch: 20 step: 15, loss is 6.078941345214844
epoch: 20 step: 16, loss is 5.9149580001831055
epoch: 20 step: 17, loss is 6.174276351928711
epoch: 20 step: 18, loss is 6.457519054412842
epoch: 20 step: 19, loss is 6.338501930236816
epoch: 20 step: 20, loss is 6.138636589050293
epoch: 20 step: 21, loss is 6.083484172821045
epoch: 20 step: 22, loss is 6.542252540588379
epoch: 20 step: 23, loss is 6.466395854949951
epoch: 20 step: 24, loss is 6.204922676086426
epoch: 20 step: 25, loss is 5.993321895599365
epoch: 20 step: 26, loss is 6.249134063720703
epoch: 20 step: 27, loss is 6.464798927307129
epoch: 20 step: 28, loss is 6.260896682739258
epoch: 20 step: 29, loss is 6.252195358276367
epoch: 20 step: 30, loss is 6.161586284637451
epoch: 20 step: 31, loss is 6.220673084259033
epoch: 20 step: 32, loss is 6.49980354309082
epoch: 20 step: 33, loss is 5.9833760261535645
epoch: 20 step: 34, loss is 6.376189708709717
epoch: 20 step: 35, loss is 6.236976623535156
epoch: 20 step: 36, loss is 5.952239036560059
epoch: 20 step: 37, loss is 6.179742813110352
epoch: 20 step: 38, loss is 6.1428399085998535
epoch: 20 step: 39, loss is 6.332143306732178
epoch: 20 step: 40, loss is 6.362541198730469
epoch: 20 step: 41, loss is 6.449821472167969
epoch: 20 step: 42, loss is 6.327462673187256
epoch: 20 step: 43, loss is 6.285547733306885
epoch: 20 step: 44, loss is 6.040374279022217
epoch: 20 step: 45, loss is 6.370156764984131
epoch: 20 step: 46, loss is 6.103520393371582
epoch: 20 step: 47, loss is 6.351859092712402
epoch: 20 step: 48, loss is 6.253407001495361
epoch: 20 step: 49, loss is 6.69454288482666
epoch: 20 step: 50, loss is 6.015639781951904
epoch: 20 step: 51, loss is 6.363215446472168
epoch: 20 step: 52, loss is 6.362278461456299
epoch: 20 step: 53, loss is 6.51154088973999
epoch: 20 step: 54, loss is 6.440922737121582
Train epoch time: 37206.989 ms, per step time: 689.018 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:29:23,728 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.179179668426514
2023-10-12 20:29:25,572 - INFO - valid loss : 6.179179668426514
epoch: 21 step: 1, loss is 5.945062160491943
epoch: 21 step: 2, loss is 6.527883529663086
epoch: 21 step: 3, loss is 6.232166767120361
epoch: 21 step: 4, loss is 6.022388935089111
epoch: 21 step: 5, loss is 6.74244499206543
epoch: 21 step: 6, loss is 6.244504451751709
epoch: 21 step: 7, loss is 6.213157653808594
epoch: 21 step: 8, loss is 5.94697380065918
epoch: 21 step: 9, loss is 6.278414726257324
epoch: 21 step: 10, loss is 6.197351455688477
epoch: 21 step: 11, loss is 6.419535160064697
epoch: 21 step: 12, loss is 6.3142194747924805
epoch: 21 step: 13, loss is 6.357494354248047
epoch: 21 step: 14, loss is 5.9993510246276855
epoch: 21 step: 15, loss is 6.1133012771606445
epoch: 21 step: 16, loss is 6.063859462738037
epoch: 21 step: 17, loss is 5.97832727432251
epoch: 21 step: 18, loss is 6.230721473693848
epoch: 21 step: 19, loss is 6.086595058441162
epoch: 21 step: 20, loss is 6.091910362243652
epoch: 21 step: 21, loss is 6.1768903732299805
epoch: 21 step: 22, loss is 6.50150203704834
epoch: 21 step: 23, loss is 6.126916885375977
epoch: 21 step: 24, loss is 6.123474597930908
epoch: 21 step: 25, loss is 6.199539661407471
epoch: 21 step: 26, loss is 6.238208770751953
epoch: 21 step: 27, loss is 6.42879056930542
epoch: 21 step: 28, loss is 6.574108600616455
epoch: 21 step: 29, loss is 6.1997222900390625
epoch: 21 step: 30, loss is 6.215411186218262
epoch: 21 step: 31, loss is 6.3113017082214355
epoch: 21 step: 32, loss is 6.323816299438477
epoch: 21 step: 33, loss is 6.480456829071045
epoch: 21 step: 34, loss is 6.262692928314209
epoch: 21 step: 35, loss is 5.831626892089844
epoch: 21 step: 36, loss is 5.908125877380371
epoch: 21 step: 37, loss is 5.998058319091797
epoch: 21 step: 38, loss is 6.62085485458374
epoch: 21 step: 39, loss is 5.896600723266602
epoch: 21 step: 40, loss is 6.041100978851318
epoch: 21 step: 41, loss is 6.622936248779297
epoch: 21 step: 42, loss is 5.918341636657715
epoch: 21 step: 43, loss is 6.274134159088135
epoch: 21 step: 44, loss is 6.512429714202881
epoch: 21 step: 45, loss is 6.55197286605835
epoch: 21 step: 46, loss is 6.198071002960205
epoch: 21 step: 47, loss is 6.203580379486084
epoch: 21 step: 48, loss is 6.13258171081543
epoch: 21 step: 49, loss is 6.291946887969971
epoch: 21 step: 50, loss is 6.045382976531982
epoch: 21 step: 51, loss is 6.089716911315918
epoch: 21 step: 52, loss is 6.005898475646973
epoch: 21 step: 53, loss is 5.937492847442627
epoch: 21 step: 54, loss is 5.969790935516357
Train epoch time: 36585.283 ms, per step time: 677.505 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:30:02,161 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 6.052206039428711
2023-10-12 20:30:04,090 - INFO - valid loss : 6.052206039428711
epoch: 22 step: 1, loss is 6.198758125305176
epoch: 22 step: 2, loss is 6.2063069343566895
epoch: 22 step: 3, loss is 5.7756805419921875
epoch: 22 step: 4, loss is 6.261481285095215
epoch: 22 step: 5, loss is 6.352803707122803
epoch: 22 step: 6, loss is 5.772329330444336
epoch: 22 step: 7, loss is 6.138835906982422
epoch: 22 step: 8, loss is 6.342520236968994
epoch: 22 step: 9, loss is 6.078042030334473
epoch: 22 step: 10, loss is 5.896842956542969
epoch: 22 step: 11, loss is 6.119592666625977
epoch: 22 step: 12, loss is 6.481599807739258
epoch: 22 step: 13, loss is 6.539538860321045
epoch: 22 step: 14, loss is 6.173289775848389
epoch: 22 step: 15, loss is 6.1720805168151855
epoch: 22 step: 16, loss is 6.300742149353027
epoch: 22 step: 17, loss is 6.010499000549316
epoch: 22 step: 18, loss is 5.926196575164795
epoch: 22 step: 19, loss is 6.2381439208984375
epoch: 22 step: 20, loss is 6.501761436462402
epoch: 22 step: 21, loss is 6.652648448944092
epoch: 22 step: 22, loss is 5.890376091003418
epoch: 22 step: 23, loss is 6.461972713470459
epoch: 22 step: 24, loss is 6.501613616943359
epoch: 22 step: 25, loss is 6.073480606079102
epoch: 22 step: 26, loss is 6.014742374420166
epoch: 22 step: 27, loss is 6.281296730041504
epoch: 22 step: 28, loss is 6.2619123458862305
epoch: 22 step: 29, loss is 6.223811626434326
epoch: 22 step: 30, loss is 6.263546466827393
epoch: 22 step: 31, loss is 6.205641269683838
epoch: 22 step: 32, loss is 6.046396255493164
epoch: 22 step: 33, loss is 5.819913864135742
epoch: 22 step: 34, loss is 6.245415687561035
epoch: 22 step: 35, loss is 6.043827533721924
epoch: 22 step: 36, loss is 5.983207702636719
epoch: 22 step: 37, loss is 6.154849052429199
epoch: 22 step: 38, loss is 6.348051071166992
epoch: 22 step: 39, loss is 5.864744186401367
epoch: 22 step: 40, loss is 6.343221664428711
epoch: 22 step: 41, loss is 5.9806342124938965
epoch: 22 step: 42, loss is 6.764158248901367
epoch: 22 step: 43, loss is 6.684301376342773
epoch: 22 step: 44, loss is 6.334264278411865
epoch: 22 step: 45, loss is 5.974850654602051
epoch: 22 step: 46, loss is 6.357706069946289
epoch: 22 step: 47, loss is 5.963621139526367
epoch: 22 step: 48, loss is 6.150290012359619
epoch: 22 step: 49, loss is 6.393994331359863
epoch: 22 step: 50, loss is 6.72471284866333
epoch: 22 step: 51, loss is 6.264407157897949
epoch: 22 step: 52, loss is 6.221858024597168
epoch: 22 step: 53, loss is 6.294203758239746
epoch: 22 step: 54, loss is 5.9438676834106445
Train epoch time: 38229.127 ms, per step time: 707.947 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:30:42,320 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.880239963531494
2023-10-12 20:30:44,719 - INFO - valid loss : 5.880239963531494
epoch: 23 step: 1, loss is 6.038939476013184
epoch: 23 step: 2, loss is 6.219186782836914
epoch: 23 step: 3, loss is 6.172480583190918
epoch: 23 step: 4, loss is 5.860164165496826
epoch: 23 step: 5, loss is 6.470196723937988
epoch: 23 step: 6, loss is 5.942653656005859
epoch: 23 step: 7, loss is 6.01414155960083
epoch: 23 step: 8, loss is 6.288692951202393
epoch: 23 step: 9, loss is 6.028837203979492
epoch: 23 step: 10, loss is 5.975269794464111
epoch: 23 step: 11, loss is 5.960903167724609
epoch: 23 step: 12, loss is 5.998280048370361
epoch: 23 step: 13, loss is 6.274147987365723
epoch: 23 step: 14, loss is 6.324609756469727
epoch: 23 step: 15, loss is 6.06228494644165
epoch: 23 step: 16, loss is 6.580418586730957
epoch: 23 step: 17, loss is 5.985384464263916
epoch: 23 step: 18, loss is 5.8678741455078125
epoch: 23 step: 19, loss is 6.291680335998535
epoch: 23 step: 20, loss is 5.827223777770996
epoch: 23 step: 21, loss is 6.2881011962890625
epoch: 23 step: 22, loss is 5.9686431884765625
epoch: 23 step: 23, loss is 5.918210029602051
epoch: 23 step: 24, loss is 5.857368469238281
epoch: 23 step: 25, loss is 6.008206367492676
epoch: 23 step: 26, loss is 5.876038074493408
epoch: 23 step: 27, loss is 5.875179290771484
epoch: 23 step: 28, loss is 5.848738670349121
epoch: 23 step: 29, loss is 5.974601745605469
epoch: 23 step: 30, loss is 6.2465434074401855
epoch: 23 step: 31, loss is 6.39256477355957
epoch: 23 step: 32, loss is 6.5439910888671875
epoch: 23 step: 33, loss is 6.080933570861816
epoch: 23 step: 34, loss is 6.082922458648682
epoch: 23 step: 35, loss is 6.175331115722656
epoch: 23 step: 36, loss is 5.873401641845703
epoch: 23 step: 37, loss is 5.922475337982178
epoch: 23 step: 38, loss is 6.159086227416992
epoch: 23 step: 39, loss is 5.956555366516113
epoch: 23 step: 40, loss is 5.978106498718262
epoch: 23 step: 41, loss is 6.447702407836914
epoch: 23 step: 42, loss is 6.361494541168213
epoch: 23 step: 43, loss is 6.1081109046936035
epoch: 23 step: 44, loss is 6.027024269104004
epoch: 23 step: 45, loss is 6.116119384765625
epoch: 23 step: 46, loss is 5.786468029022217
epoch: 23 step: 47, loss is 6.066899299621582
epoch: 23 step: 48, loss is 6.03422737121582
epoch: 23 step: 49, loss is 6.011777400970459
epoch: 23 step: 50, loss is 6.297283172607422
epoch: 23 step: 51, loss is 6.531027317047119
epoch: 23 step: 52, loss is 5.798030853271484
epoch: 23 step: 53, loss is 5.972631931304932
epoch: 23 step: 54, loss is 6.070233345031738
Train epoch time: 36808.360 ms, per step time: 681.636 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:31:21,528 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.943419933319092
2023-10-12 20:31:23,327 - INFO - valid loss : 5.943419933319092
epoch: 24 step: 1, loss is 6.064089775085449
epoch: 24 step: 2, loss is 6.566555500030518
epoch: 24 step: 3, loss is 6.240484237670898
epoch: 24 step: 4, loss is 6.044161796569824
epoch: 24 step: 5, loss is 6.327435493469238
epoch: 24 step: 6, loss is 6.251559734344482
epoch: 24 step: 7, loss is 5.709962844848633
epoch: 24 step: 8, loss is 6.006933212280273
epoch: 24 step: 9, loss is 5.751983165740967
epoch: 24 step: 10, loss is 5.9539079666137695
epoch: 24 step: 11, loss is 6.110958099365234
epoch: 24 step: 12, loss is 6.197689533233643
epoch: 24 step: 13, loss is 5.836910247802734
epoch: 24 step: 14, loss is 5.864040374755859
epoch: 24 step: 15, loss is 6.063030242919922
epoch: 24 step: 16, loss is 6.2042555809021
epoch: 24 step: 17, loss is 6.100830554962158
epoch: 24 step: 18, loss is 5.779796123504639
epoch: 24 step: 19, loss is 6.00366735458374
epoch: 24 step: 20, loss is 6.240570068359375
epoch: 24 step: 21, loss is 6.117888450622559
epoch: 24 step: 22, loss is 6.030216693878174
epoch: 24 step: 23, loss is 6.229406356811523
epoch: 24 step: 24, loss is 6.1850080490112305
epoch: 24 step: 25, loss is 6.152263641357422
epoch: 24 step: 26, loss is 5.720306873321533
epoch: 24 step: 27, loss is 5.972879409790039
epoch: 24 step: 28, loss is 5.926186561584473
epoch: 24 step: 29, loss is 6.727609634399414
epoch: 24 step: 30, loss is 6.278097629547119
epoch: 24 step: 31, loss is 5.53737735748291
epoch: 24 step: 32, loss is 5.806987762451172
epoch: 24 step: 33, loss is 5.944304943084717
epoch: 24 step: 34, loss is 6.448444366455078
epoch: 24 step: 35, loss is 6.305776596069336
epoch: 24 step: 36, loss is 6.140196323394775
epoch: 24 step: 37, loss is 5.844437599182129
epoch: 24 step: 38, loss is 6.327926158905029
epoch: 24 step: 39, loss is 5.920405387878418
epoch: 24 step: 40, loss is 6.016407012939453
epoch: 24 step: 41, loss is 6.1021833419799805
epoch: 24 step: 42, loss is 6.077700614929199
epoch: 24 step: 43, loss is 6.073566436767578
epoch: 24 step: 44, loss is 6.312413215637207
epoch: 24 step: 45, loss is 5.959029674530029
epoch: 24 step: 46, loss is 6.046660900115967
epoch: 24 step: 47, loss is 5.983524799346924
epoch: 24 step: 48, loss is 6.333526611328125
epoch: 24 step: 49, loss is 6.110834121704102
epoch: 24 step: 50, loss is 6.103974342346191
epoch: 24 step: 51, loss is 5.995616436004639
epoch: 24 step: 52, loss is 6.37401819229126
epoch: 24 step: 53, loss is 5.677942752838135
epoch: 24 step: 54, loss is 5.928608417510986
Train epoch time: 37707.613 ms, per step time: 698.289 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:32:01,036 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.918618679046631
2023-10-12 20:32:02,857 - INFO - valid loss : 5.918618679046631
epoch: 25 step: 1, loss is 6.189490795135498
epoch: 25 step: 2, loss is 6.025225639343262
epoch: 25 step: 3, loss is 6.021330833435059
epoch: 25 step: 4, loss is 6.295112133026123
epoch: 25 step: 5, loss is 6.072338581085205
epoch: 25 step: 6, loss is 6.313989162445068
epoch: 25 step: 7, loss is 6.144275188446045
epoch: 25 step: 8, loss is 6.0169997215271
epoch: 25 step: 9, loss is 6.398379802703857
epoch: 25 step: 10, loss is 5.707563877105713
epoch: 25 step: 11, loss is 6.6147589683532715
epoch: 25 step: 12, loss is 5.876389026641846
epoch: 25 step: 13, loss is 6.055866241455078
epoch: 25 step: 14, loss is 6.01402473449707
epoch: 25 step: 15, loss is 6.274450302124023
epoch: 25 step: 16, loss is 5.951414108276367
epoch: 25 step: 17, loss is 6.036051273345947
epoch: 25 step: 18, loss is 5.988776683807373
epoch: 25 step: 19, loss is 5.80181884765625
epoch: 25 step: 20, loss is 6.277252674102783
epoch: 25 step: 21, loss is 5.79472017288208
epoch: 25 step: 22, loss is 6.272096157073975
epoch: 25 step: 23, loss is 5.921350002288818
epoch: 25 step: 24, loss is 5.881921768188477
epoch: 25 step: 25, loss is 5.957657814025879
epoch: 25 step: 26, loss is 6.131820201873779
epoch: 25 step: 27, loss is 6.43478536605835
epoch: 25 step: 28, loss is 6.038607597351074
epoch: 25 step: 29, loss is 5.958656311035156
epoch: 25 step: 30, loss is 5.874902725219727
epoch: 25 step: 31, loss is 6.154556751251221
epoch: 25 step: 32, loss is 6.16085147857666
epoch: 25 step: 33, loss is 6.290423393249512
epoch: 25 step: 34, loss is 5.854786396026611
epoch: 25 step: 35, loss is 5.777444362640381
epoch: 25 step: 36, loss is 6.081421852111816
epoch: 25 step: 37, loss is 6.12993049621582
epoch: 25 step: 38, loss is 6.16766357421875
epoch: 25 step: 39, loss is 6.283713340759277
epoch: 25 step: 40, loss is 6.671003341674805
epoch: 25 step: 41, loss is 5.984734535217285
epoch: 25 step: 42, loss is 6.098828315734863
epoch: 25 step: 43, loss is 6.163628578186035
epoch: 25 step: 44, loss is 5.959165573120117
epoch: 25 step: 45, loss is 6.2175164222717285
epoch: 25 step: 46, loss is 6.170518398284912
epoch: 25 step: 47, loss is 6.329911231994629
epoch: 25 step: 48, loss is 6.428038597106934
epoch: 25 step: 49, loss is 5.83297061920166
epoch: 25 step: 50, loss is 6.226374626159668
epoch: 25 step: 51, loss is 5.961703300476074
epoch: 25 step: 52, loss is 6.210860729217529
epoch: 25 step: 53, loss is 5.702192783355713
epoch: 25 step: 54, loss is 5.992331027984619
Train epoch time: 37454.727 ms, per step time: 693.606 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:32:40,313 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.727062225341797
2023-10-12 20:32:42,042 - INFO - valid loss : 5.727062225341797
epoch: 26 step: 1, loss is 5.982330799102783
epoch: 26 step: 2, loss is 6.242702484130859
epoch: 26 step: 3, loss is 6.147409915924072
epoch: 26 step: 4, loss is 6.319634914398193
epoch: 26 step: 5, loss is 5.67900276184082
epoch: 26 step: 6, loss is 5.834986686706543
epoch: 26 step: 7, loss is 5.977543354034424
epoch: 26 step: 8, loss is 5.553383827209473
epoch: 26 step: 9, loss is 5.928605079650879
epoch: 26 step: 10, loss is 6.23490047454834
epoch: 26 step: 11, loss is 6.0953850746154785
epoch: 26 step: 12, loss is 5.810668468475342
epoch: 26 step: 13, loss is 5.824113368988037
epoch: 26 step: 14, loss is 6.382355690002441
epoch: 26 step: 15, loss is 6.3092265129089355
epoch: 26 step: 16, loss is 6.241557598114014
epoch: 26 step: 17, loss is 5.938816547393799
epoch: 26 step: 18, loss is 6.322638988494873
epoch: 26 step: 19, loss is 5.938557147979736
epoch: 26 step: 20, loss is 6.355527400970459
epoch: 26 step: 21, loss is 5.909938812255859
epoch: 26 step: 22, loss is 6.104696273803711
epoch: 26 step: 23, loss is 5.97736120223999
epoch: 26 step: 24, loss is 5.910264015197754
epoch: 26 step: 25, loss is 5.8698410987854
epoch: 26 step: 26, loss is 6.140702247619629
epoch: 26 step: 27, loss is 6.030374050140381
epoch: 26 step: 28, loss is 5.983297348022461
epoch: 26 step: 29, loss is 5.7448883056640625
epoch: 26 step: 30, loss is 5.968697547912598
epoch: 26 step: 31, loss is 5.846837997436523
epoch: 26 step: 32, loss is 6.225769996643066
epoch: 26 step: 33, loss is 6.102991104125977
epoch: 26 step: 34, loss is 5.993708610534668
epoch: 26 step: 35, loss is 5.790004253387451
epoch: 26 step: 36, loss is 5.932779312133789
epoch: 26 step: 37, loss is 6.328376293182373
epoch: 26 step: 38, loss is 5.853940010070801
epoch: 26 step: 39, loss is 6.37800407409668
epoch: 26 step: 40, loss is 6.04397439956665
epoch: 26 step: 41, loss is 6.276179313659668
epoch: 26 step: 42, loss is 6.192696571350098
epoch: 26 step: 43, loss is 6.373356819152832
epoch: 26 step: 44, loss is 6.099090576171875
epoch: 26 step: 45, loss is 6.380525588989258
epoch: 26 step: 46, loss is 5.610778331756592
epoch: 26 step: 47, loss is 5.911942005157471
epoch: 26 step: 48, loss is 6.132498741149902
epoch: 26 step: 49, loss is 6.076775550842285
epoch: 26 step: 50, loss is 6.0094709396362305
epoch: 26 step: 51, loss is 6.310256481170654
epoch: 26 step: 52, loss is 5.716379642486572
epoch: 26 step: 53, loss is 5.973299503326416
epoch: 26 step: 54, loss is 6.068987846374512
Train epoch time: 36263.895 ms, per step time: 671.554 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:33:18,307 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.719184398651123
2023-10-12 20:33:19,972 - INFO - valid loss : 5.719184398651123
epoch: 27 step: 1, loss is 6.301013946533203
epoch: 27 step: 2, loss is 5.986937046051025
epoch: 27 step: 3, loss is 6.182980060577393
epoch: 27 step: 4, loss is 6.054936408996582
epoch: 27 step: 5, loss is 6.000668048858643
epoch: 27 step: 6, loss is 6.304832935333252
epoch: 27 step: 7, loss is 6.255098819732666
epoch: 27 step: 8, loss is 6.46046257019043
epoch: 27 step: 9, loss is 5.8138861656188965
epoch: 27 step: 10, loss is 6.439103126525879
epoch: 27 step: 11, loss is 5.96164608001709
epoch: 27 step: 12, loss is 5.837606906890869
epoch: 27 step: 13, loss is 5.62062406539917
epoch: 27 step: 14, loss is 6.14253568649292
epoch: 27 step: 15, loss is 5.8715972900390625
epoch: 27 step: 16, loss is 6.0410637855529785
epoch: 27 step: 17, loss is 5.960043907165527
epoch: 27 step: 18, loss is 5.763047695159912
epoch: 27 step: 19, loss is 6.103229999542236
epoch: 27 step: 20, loss is 6.2533111572265625
epoch: 27 step: 21, loss is 6.129068851470947
epoch: 27 step: 22, loss is 6.178195476531982
epoch: 27 step: 23, loss is 6.18310546875
epoch: 27 step: 24, loss is 5.555657863616943
epoch: 27 step: 25, loss is 5.999453067779541
epoch: 27 step: 26, loss is 5.9215803146362305
epoch: 27 step: 27, loss is 5.925081729888916
epoch: 27 step: 28, loss is 5.838499546051025
epoch: 27 step: 29, loss is 6.224714279174805
epoch: 27 step: 30, loss is 6.197046756744385
epoch: 27 step: 31, loss is 5.8402605056762695
epoch: 27 step: 32, loss is 6.0091938972473145
epoch: 27 step: 33, loss is 5.601455211639404
epoch: 27 step: 34, loss is 6.062049865722656
epoch: 27 step: 35, loss is 5.949449062347412
epoch: 27 step: 36, loss is 5.849994659423828
epoch: 27 step: 37, loss is 5.822732925415039
epoch: 27 step: 38, loss is 5.7453155517578125
epoch: 27 step: 39, loss is 6.33457612991333
epoch: 27 step: 40, loss is 6.317769527435303
epoch: 27 step: 41, loss is 5.9126200675964355
epoch: 27 step: 42, loss is 6.099507808685303
epoch: 27 step: 43, loss is 5.785576343536377
epoch: 27 step: 44, loss is 6.138045310974121
epoch: 27 step: 45, loss is 6.200737476348877
epoch: 27 step: 46, loss is 5.475058078765869
epoch: 27 step: 47, loss is 5.653162956237793
epoch: 27 step: 48, loss is 6.043389320373535
epoch: 27 step: 49, loss is 6.184359073638916
epoch: 27 step: 50, loss is 6.165914535522461
epoch: 27 step: 51, loss is 6.146401882171631
epoch: 27 step: 52, loss is 5.886582851409912
epoch: 27 step: 53, loss is 5.962400913238525
epoch: 27 step: 54, loss is 6.062649726867676
Train epoch time: 37275.926 ms, per step time: 690.295 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:33:57,249 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.718421459197998
2023-10-12 20:33:59,189 - INFO - valid loss : 5.718421459197998
epoch: 28 step: 1, loss is 6.145387172698975
epoch: 28 step: 2, loss is 6.16718864440918
epoch: 28 step: 3, loss is 6.072519302368164
epoch: 28 step: 4, loss is 5.949609279632568
epoch: 28 step: 5, loss is 6.258413791656494
epoch: 28 step: 6, loss is 5.959960460662842
epoch: 28 step: 7, loss is 5.833212375640869
epoch: 28 step: 8, loss is 5.963595390319824
epoch: 28 step: 9, loss is 6.363508224487305
epoch: 28 step: 10, loss is 6.0213623046875
epoch: 28 step: 11, loss is 5.878183364868164
epoch: 28 step: 12, loss is 6.074885368347168
epoch: 28 step: 13, loss is 6.323327541351318
epoch: 28 step: 14, loss is 5.913051605224609
epoch: 28 step: 15, loss is 5.958860397338867
epoch: 28 step: 16, loss is 5.488766670227051
epoch: 28 step: 17, loss is 6.223026275634766
epoch: 28 step: 18, loss is 5.997201919555664
epoch: 28 step: 19, loss is 6.119012355804443
epoch: 28 step: 20, loss is 5.640244483947754
epoch: 28 step: 21, loss is 6.055324077606201
epoch: 28 step: 22, loss is 6.0338873863220215
epoch: 28 step: 23, loss is 6.059541702270508
epoch: 28 step: 24, loss is 6.125415802001953
epoch: 28 step: 25, loss is 6.048224449157715
epoch: 28 step: 26, loss is 6.208393096923828
epoch: 28 step: 27, loss is 5.780621528625488
epoch: 28 step: 28, loss is 5.958744525909424
epoch: 28 step: 29, loss is 6.401737689971924
epoch: 28 step: 30, loss is 6.122570514678955
epoch: 28 step: 31, loss is 5.800851821899414
epoch: 28 step: 32, loss is 6.099153995513916
epoch: 28 step: 33, loss is 5.625809669494629
epoch: 28 step: 34, loss is 5.848757743835449
epoch: 28 step: 35, loss is 6.229137897491455
epoch: 28 step: 36, loss is 6.199174880981445
epoch: 28 step: 37, loss is 6.281978130340576
epoch: 28 step: 38, loss is 5.943558692932129
epoch: 28 step: 39, loss is 5.70773458480835
epoch: 28 step: 40, loss is 6.3911848068237305
epoch: 28 step: 41, loss is 6.207468509674072
epoch: 28 step: 42, loss is 5.881191730499268
epoch: 28 step: 43, loss is 6.259958267211914
epoch: 28 step: 44, loss is 5.730899333953857
epoch: 28 step: 45, loss is 5.665409564971924
epoch: 28 step: 46, loss is 5.672171115875244
epoch: 28 step: 47, loss is 6.784759044647217
epoch: 28 step: 48, loss is 6.0097479820251465
epoch: 28 step: 49, loss is 6.046337127685547
epoch: 28 step: 50, loss is 5.823253631591797
epoch: 28 step: 51, loss is 6.2415242195129395
epoch: 28 step: 52, loss is 5.838781833648682
epoch: 28 step: 53, loss is 5.813597679138184
epoch: 28 step: 54, loss is 5.743435859680176
Train epoch time: 38033.139 ms, per step time: 704.317 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:34:37,224 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.8835368156433105
2023-10-12 20:34:39,187 - INFO - valid loss : 5.8835368156433105
epoch: 29 step: 1, loss is 5.6472296714782715
epoch: 29 step: 2, loss is 5.965156078338623
epoch: 29 step: 3, loss is 6.032262802124023
epoch: 29 step: 4, loss is 5.852875709533691
epoch: 29 step: 5, loss is 6.032058238983154
epoch: 29 step: 6, loss is 5.955988883972168
epoch: 29 step: 7, loss is 5.572246074676514
epoch: 29 step: 8, loss is 6.34758186340332
epoch: 29 step: 9, loss is 6.168067932128906
epoch: 29 step: 10, loss is 5.760854244232178
epoch: 29 step: 11, loss is 5.775284767150879
epoch: 29 step: 12, loss is 6.070286750793457
epoch: 29 step: 13, loss is 5.643377304077148
epoch: 29 step: 14, loss is 5.8633317947387695
epoch: 29 step: 15, loss is 6.008371353149414
epoch: 29 step: 16, loss is 5.982742786407471
epoch: 29 step: 17, loss is 5.711139678955078
epoch: 29 step: 18, loss is 6.093406677246094
epoch: 29 step: 19, loss is 6.339993000030518
epoch: 29 step: 20, loss is 6.10950231552124
epoch: 29 step: 21, loss is 6.225078105926514
epoch: 29 step: 22, loss is 5.96575927734375
epoch: 29 step: 23, loss is 5.675228595733643
epoch: 29 step: 24, loss is 5.746394157409668
epoch: 29 step: 25, loss is 5.984043598175049
epoch: 29 step: 26, loss is 5.880921840667725
epoch: 29 step: 27, loss is 6.507437705993652
epoch: 29 step: 28, loss is 5.438975811004639
epoch: 29 step: 29, loss is 5.897228240966797
epoch: 29 step: 30, loss is 5.75475549697876
epoch: 29 step: 31, loss is 5.668198585510254
epoch: 29 step: 32, loss is 5.962351322174072
epoch: 29 step: 33, loss is 5.757096290588379
epoch: 29 step: 34, loss is 6.038390159606934
epoch: 29 step: 35, loss is 6.185178279876709
epoch: 29 step: 36, loss is 5.892565727233887
epoch: 29 step: 37, loss is 5.938673496246338
epoch: 29 step: 38, loss is 6.018458843231201
epoch: 29 step: 39, loss is 5.8646440505981445
epoch: 29 step: 40, loss is 5.753222942352295
epoch: 29 step: 41, loss is 5.549378871917725
epoch: 29 step: 42, loss is 5.798458099365234
epoch: 29 step: 43, loss is 6.298289775848389
epoch: 29 step: 44, loss is 6.409572601318359
epoch: 29 step: 45, loss is 6.109945774078369
epoch: 29 step: 46, loss is 5.958587169647217
epoch: 29 step: 47, loss is 6.356137275695801
epoch: 29 step: 48, loss is 5.960943698883057
epoch: 29 step: 49, loss is 5.987335205078125
epoch: 29 step: 50, loss is 6.119053363800049
epoch: 29 step: 51, loss is 5.966526031494141
epoch: 29 step: 52, loss is 5.589588642120361
epoch: 29 step: 53, loss is 6.06504487991333
epoch: 29 step: 54, loss is 5.844891548156738
Train epoch time: 37203.248 ms, per step time: 688.949 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:35:16,392 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.865980625152588
2023-10-12 20:35:18,209 - INFO - valid loss : 5.865980625152588
epoch: 30 step: 1, loss is 6.386822700500488
epoch: 30 step: 2, loss is 5.519195079803467
epoch: 30 step: 3, loss is 5.531222820281982
epoch: 30 step: 4, loss is 5.703614234924316
epoch: 30 step: 5, loss is 5.568638324737549
epoch: 30 step: 6, loss is 5.566432952880859
epoch: 30 step: 7, loss is 6.023918628692627
epoch: 30 step: 8, loss is 5.721592426300049
epoch: 30 step: 9, loss is 5.6723952293396
epoch: 30 step: 10, loss is 5.660406589508057
epoch: 30 step: 11, loss is 5.864541530609131
epoch: 30 step: 12, loss is 5.543296813964844
epoch: 30 step: 13, loss is 6.12221097946167
epoch: 30 step: 14, loss is 5.634847164154053
epoch: 30 step: 15, loss is 6.096116542816162
epoch: 30 step: 16, loss is 5.9148993492126465
epoch: 30 step: 17, loss is 5.790459632873535
epoch: 30 step: 18, loss is 6.220341205596924
epoch: 30 step: 19, loss is 6.088827133178711
epoch: 30 step: 20, loss is 5.666992664337158
epoch: 30 step: 21, loss is 5.777677059173584
epoch: 30 step: 22, loss is 5.779082775115967
epoch: 30 step: 23, loss is 5.701025485992432
epoch: 30 step: 24, loss is 6.389467239379883
epoch: 30 step: 25, loss is 5.622065544128418
epoch: 30 step: 26, loss is 6.143272399902344
epoch: 30 step: 27, loss is 6.026328086853027
epoch: 30 step: 28, loss is 5.481566905975342
epoch: 30 step: 29, loss is 5.820777416229248
epoch: 30 step: 30, loss is 5.890691757202148
epoch: 30 step: 31, loss is 5.595683574676514
epoch: 30 step: 32, loss is 6.135684013366699
epoch: 30 step: 33, loss is 6.059799671173096
epoch: 30 step: 34, loss is 6.028968334197998
epoch: 30 step: 35, loss is 5.880079746246338
epoch: 30 step: 36, loss is 6.181391716003418
epoch: 30 step: 37, loss is 6.139490127563477
epoch: 30 step: 38, loss is 5.9831862449646
epoch: 30 step: 39, loss is 6.287394046783447
epoch: 30 step: 40, loss is 5.788058280944824
epoch: 30 step: 41, loss is 5.6970109939575195
epoch: 30 step: 42, loss is 6.042738437652588
epoch: 30 step: 43, loss is 6.122098445892334
epoch: 30 step: 44, loss is 6.091020107269287
epoch: 30 step: 45, loss is 5.905202865600586
epoch: 30 step: 46, loss is 5.7896223068237305
epoch: 30 step: 47, loss is 5.578094959259033
epoch: 30 step: 48, loss is 5.827899932861328
epoch: 30 step: 49, loss is 6.010318756103516
epoch: 30 step: 50, loss is 6.110100746154785
epoch: 30 step: 51, loss is 6.195318222045898
epoch: 30 step: 52, loss is 5.867280006408691
epoch: 30 step: 53, loss is 5.871345520019531
epoch: 30 step: 54, loss is 5.718070030212402
Train epoch time: 37287.823 ms, per step time: 690.515 ms
INFO:root:current lr 0.0010000000474974513
2023-10-12 20:35:55,498 - INFO - current lr 0.0010000000474974513
INFO:root:valid loss : 5.6730523109436035
2023-10-12 20:35:57,284 - INFO - valid loss : 5.6730523109436035
INFO:root:Loaded model at /cache/code/m-libcity/M_libcity/cache/1916/model_cache/GWNET_NYCBike20140409.ckpt
2023-10-12 20:35:57,286 - INFO - Loaded model at /cache/code/m-libcity/M_libcity/cache/1916/model_cache/GWNET_NYCBike20140409.ckpt
INFO:root:Start evaluating ...
2023-10-12 20:35:57,385 - INFO - Start evaluating ...
INFO:root:Note that you select the single mode to evaluate!
(384, 12, 128, 1)
2023-10-12 20:37:21,174 - INFO - Note that you select the single mode to evaluate!
INFO:root:Evaluate result is saved at /cache/code/m-libcity/M_libcity/cache/1916/evaluate_cache/2023_10_12_20_37_21_GWNET_NYCBike20140409.csv
2023-10-12 20:37:22,959 - INFO - Evaluate result is saved at /cache/code/m-libcity/M_libcity/cache/1916/evaluate_cache/2023_10_12_20_37_21_GWNET_NYCBike20140409.csv
INFO:root:
        MAE        MAPE        MSE     RMSE
1  3.846395  0.25592875  42.840565  6.54527

*********************
/cache/code/m-libcity/M_libcity/cache/
Successfully Upload /cache/code/m-libcity/M_libcity/cache/ to /cache/output

download code successfully
unzip code successfully
[INFO] ME(383:281473341803200,MainProcess):2023-10-12-20:37:29.259.517 [mindspore/_extends/remote/kernel_build_server_ascend.py:65] [TRACE] Ascend Messager Exit...
[INFO] PIPELINE(383,ffff9e8d4ac0,python):2023-10-12-20:37:29.261.662 [mindspore/ccsrc/pipeline/jit/init.cc:376] operator()] Start releasing dataset handles...
[INFO] PIPELINE(383,ffff9e8d4ac0,python):2023-10-12-20:37:29.261.765 [mindspore/ccsrc/pipeline/jit/init.cc:379] operator()] End release dataset handles.
[INFO] PIPELINE(383,ffff9e8d4ac0,python):2023-10-12-20:37:29.261.784 [mindspore/ccsrc/pipeline/jit/pipeline.cc:1818] FinalizeCluster] Start finalize the cluster instance.
[INFO] PIPELINE(383,ffff9e8d4ac0,python):2023-10-12-20:37:29.261.803 [mindspore/ccsrc/pipeline/jit/pipeline.cc:1825] FinalizeCluster] End finalize the cluster instance.
upload model successfully
download system code successfully
time="2023-10-12T20:37:31+08:00" level=info msg="clean up child process succeed, pid=383, wstatus=0, exit_status=0" file="cleaner_unix.go:75" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:31+08:00" level=info msg="clean up child process succeed, pid=394, wstatus=0, exit_status=0" file="cleaner_unix.go:75" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:31+08:00" level=info msg="clean up child process succeed, pid=401, wstatus=0, exit_status=0" file="cleaner_unix.go:75" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
[ModelArts Service Log]2023-10-12 20:37:31,630 - INFO - Begin destroy training processes
[ModelArts Service Log]2023-10-12 20:37:31,631 - INFO - proc-rank-0-device-0 (pid: 124) has exited
[ModelArts Service Log]2023-10-12 20:37:31,631 - INFO - End destroy training processes
time="2023-10-12T20:37:31+08:00" level=info msg="clean up child process succeed, pid=125, wstatus=0, exit_status=0" file="cleaner_unix.go:75" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:31+08:00" level=warning msg="waiting for cmd process, but got ECHILD error" file="run_train.go:367" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:32+08:00" level=warning msg="report event TrainingExit failed: send training-event info to algorancher failed, err: Post \"https://modelarts.cn-northwest-229.myhuaweicloud.com/v2/0c3767f69c62473b8fde445bb6d570f5/training-jobs/f828aa20-b6f6-40b5-a502-f91ed3515371/tasks/worker-0/reports/training-event\": dial tcp: lookup modelarts.cn-northwest-229.myhuaweicloud.com on 10.247.3.10:53: no such host" file="event.go:51" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:32+08:00" level=info msg="bootstrap is exiting with exit code 0" file="bootstrap.go:241" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:32+08:00" level=info msg="retCode 0 has been written to the retCode file /home/ma-user/modelarts/retCode" file="bootstrap.go:221" Command=bootstrap/run Component=ma-training-toolkit Platform=ModelArts-Service
time="2023-10-12T20:37:32+08:00" level=info msg="[sidecar] training is completed" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:37:32+08:00" level=info msg="[sidecar] stop toolkit_obs_upload_by_channels_pid = 51 by signal SIGTERM" Component=ShellScripts Platform=ModelArts-Service
time="2023-10-12T20:37:32+08:00" level=info msg="the periodic upload task exiting..." file="upload.go:216" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=srt_log_collection
time="2023-10-12T20:37:32+08:00" level=info msg="the periodic upload task exiting..." file="upload.go:216" Command=obs/upload_by_channels Component=ma-training-toolkit Platform=ModelArts-Service Task=log_url
success