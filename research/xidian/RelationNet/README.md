# Contents

[查看中文](./README_CN.md)

- [Contents](#contents)
- [RelationNet Description](#RelationNet-description)
- [Model Architecture](#model-architecture)
- [Dataset](#dataset)
- [Features](#features)
    - [Mixed Precision](#mixed-precision)
- [Environment Requirements](#environment-requirements)
- [Quick Start](#quick-start)
- [Script Description](#script-description)
    - [Script and Sample Code](#script-and-sample-code)
        - [Script Parameters](#script-parameters)
        - [Training Process](#training-process)
            - [Training](#training)
            - [Distributed Training](#distributed-training)
        - [Evaluation Process](#evaluation-process)
            - [Evaluation](#evaluation)
        - [Export Process](#export-process)
            - [Export](#export)
- [Model Description](#model-description)
    - [Performance](#performance)
        - [Training Performance](#training-performance)      
        - [Evaluation Performance](#evaluation-performance)
- [Description of Random Situation](#description-of-random-situation)
- [ModelZoo Homepage](#modelzoo-homepage)

# [RelationNet Description](#contents)

RelativeNet is a neural network model used to solve relational reasoning problems, particularly suitable for tasks that require understanding the interactions between objects, such as object relationship recognition in images. It was proposed by researchers from Facebook's Artificial Intelligence Research Laboratory (FAIR) in 2017.

The core idea of RelationNet design is to enable machine learning systems to understand and process the relationships between different parts of an image like humans. Traditional methods typically rely on direct comparison of features or pre-defined relationship models, while RelationNet learns these relationships by introducing a new mechanism that allows the model to understand complex scenes without explicit programming.


A key feature of RelationNet is its ability to be applied to various tasks, such as Visual Question Answering (VQA), object recognition, and relational reasoning. Through training, RelativeNet can learn how to recognize objects in images and their relative positions and other relationships, thereby better answering questions about images.

In summary, RelationNet provides an effective way for deep learning models to handle more complex relational inference tasks, which is an important advancement in the field of visual understanding. With the development of research, such models are expected to further promote the development of fields such as computer vision and natural language processing.


[Paper](https://arxiv.org/abs/1711.06025)：Sung F, Yang Y, Zhang L, et al. Learning to compare: Relation network for few-shot learning[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 1199-1208.

# [Model Architecture](#contents)

This model consists of two main parts:

- feature Detector: This part is responsible for extracting possible relationship pairs from input data and generating a vector representation for each relationship pair.
- Relationship Classifier: This part receives vectors generated by relationship detectors and predicts the importance or relevance of these relationships.

# [Dataset](#contents)

Dataset used: [Omnigram]（ https://gitcode.net/mirrors/brendenlake/omniglot?utm_source=csdn_github_accelerator )
Please download the dataset and unzip it and place it in the '. data/' folder. After successful decompression, the '. data/mniglot_resized' folder will be used for training and testing

The Omniglot dataset consists of 50 alphabets (each with a different number of characters), each containing different characters. For example, the common Latin alphabet, abcdefg, has a total of 26 letters. There are also 1623 characters in Korean and Japanese, each with 20 different writing styles. Each writing style is a 108 * 108 image, which means the size of the dataset is 1623 * 20

# [Features](#contents)

## Mixed Precision

The mixed precision training method accelerates the deep learning neural network training process by using both the single-precision and half-precision data formats, and maintains the network precision achieved by the single-precision training at the same time. Mixed precision training can accelerate the computation process, reduce memory usage, and enable a larger model or batch size to be trained on specific hardware.
For FP16 operators, if the input data type is FP32, the backend of MindSpore will automatically handle it with reduced precision. Users could check the reduced-precision operators by enabling INFO log and then searching ‘reduce precision’.

# [Environment Requirements](#contents)

- Hardware（Ascend/GPU/CPU）
    - Prepare hardware environment with Ascend/GPU/CPU processor.
- Framework
    - [MindSpore](https://www.mindspore.cn/install/en)
- For more information, please check the resources below：
    - [MindSpore Tutorials](https://www.mindspore.cn/tutorials/en/master/index.html)
    - [MindSpore Python API](https://www.mindspore.cn/docs/api/en/master/index.html)

# [Quick Start](#contents)

After installing MindSpore via the official website, you can start training and evaluation as follows:

- running on Ascend

  ```python
  # Run training example
  python omniglot_train_few_shot.py > omniglot_train_few_shot.log 2>&1 &

  # Run distributed training example
  bash run_train.sh [RANK_TABLE_FILE]
  # example: bash run_train.sh ~/hccl_8p.json

  #Run evaluation example
  python eval.py > eval.log 2>&1 &
  OR
  bash run_eval.sh
  ```

  For distributed training, a hccl configuration file with JSON format needs to be created in advance.

  Please follow the instructions in the link below:

  <https://gitee.com/mindspore/models/tree/master/utils/hccl_tools>.

- running on GPU

  For running on GPU, please change `device_target` from `Ascend` to `GPU` in configuration file default_config.yaml

  ```python
  # run training example
  export CUDA_VISIBLE_DEVICES=0
  python omniglot_train_few_shot.py > omniglot_train_few_shot.log 2>&1 &

  # run distributed training example
  bash scripts/run_train_gpu.sh 8 0,1,2,3,4,5,6,7

  # run evaluation example
  python eval.py > eval.log 2>&1 &
  OR
  bash run_eval_gpu.sh
  ```

- running on CPU

  For running on CPU, please change `device_target` from `Ascend` to `CPU` in configuration file default_config.yaml

  ```python
  # run training example
  bash run_train_cpu.sh
  OR
  python omniglot_train_few_shot.py > omniglot_train_few_shot.log 2>&1 &

  # run evaluation example
  bash run_eval_cpu.sh
  OR
  python eval.py > eval.log 2>&1 &
  ```

- ModelArts (If you want to run in modelarts, please check the official documentation of [modelarts](https://support.huaweicloud.com/modelarts/), and you can start training as follows)

    - Train 8p on ModelArts

    ```python
      # (1) Add "config_path='./default_config.yaml'" on the website UI interface.
      # (2) Perform a or b.
      #       a. Set "enable_modelarts=True" on default_config.yaml file.
      #          Set other parameters on default_config.yaml file you need.
      #       b. Add "enable_modelarts=True" on the website UI interface.
      #          Add other parameters on the website UI interface.
      # (3) Upload a zip dataset to S3 bucket.
      # (4) Set the code directory to "/path/RelationNet" on the website UI interface.
      # (5) Set the startup file to "omniglot_train_few_shot.py" on the website UI interface.
      # (6) Set the "Dataset path" and "Output file path" and "Job log path" to your path on the website UI interface.
      # (7) Create your job.
    ```

    - Eval on ModelArts

    ```python
      # (1) Add "config_path='./default_config.yaml'" on the website UI interface.
      # (2) Perform a or b.
      #       a. Set "enable_modelarts=True" on default_config.yaml file.
      #          Set other parameters on default_config.yaml file you need.
      #       b. Add "enable_modelarts=True" on the website UI interface.
      #          Add other parameters on the website UI interface.
      # (3) Upload or copy your pretrained model to S3 bucket.
      # (4) Upload a zip dataset to S3 bucket.
      # (5) Set the code directory to "/path/RelationNet" on the website UI interface.
      # (6) Set the startup file to "eval.py" on the website UI interface.
      # (7) Set the "Dataset path" and "Output file path" and "Job log path" to your path on the website UI interface.
      # (8) Create your job.
    ```

    - Export on ModelArts

    ```python
      # (1) Add "config_path='./default_config.yaml'" on the website UI interface.
      # (2) Perform a or b.
      #       a. Set "enable_modelarts=True" on default_config.yaml file.
      #          Set other parameters on cifar10_config.yaml file you need.
      #       b. Add "enable_modelarts=True" on the website UI interface.
      #          Add other parameters on the website UI interface.
      # (3) Upload or copy your trained model to S3 bucket.
      # (4) Set the code directory to "/path/RelationNet" on the website UI interface.
      # (5) Set the startup file to "export.py" on the website UI interface.
      # (6) Set the "Dataset path" and "Output file path" and "Job log path" to your path on the website UI interface.
      # (7) Create your job.
    ```

# [Script Description](#contents)

## [Script and Sample Code](#contents)

```text
├── RelationNet  
  ├── README_CN.md                  # RelationNet相关中文说明
  ├── README.md                     # RelationNet相关英文说明
  ├── checkpoints
  │   ├── omniglot_feature_encoder.ckpt
  │   └── omniglot_relation_network.ckpt
  ├── data
  │   ├── omniglot_resized          # 数据集
  │   └── __MACOSX
  ├── default_config.yaml           # 参数配置文件
  ├── eval.py                       # 评估脚本
  ├── export.py                     # 将checkpoint文件导出到air/mindir
  ├── model_utils
  │   ├── config.py                 # 处理配置参数
  │   ├── device_adapter.py         # 获取云ID
  │   ├── local_adapter.py          # 获取本地ID
  │   ├── moxing_adapter.py         # 参数处理
  │   └── utils.py                  # 工具文件
  ├── models
  │   └── models.py
  ├── omniglot_train_few_shot.py    # 训练脚本
  ├── scripts
  │   ├── run_eval_cpu.sh           # CPU处理器评估的shell脚本
  │   ├── run_eval_gpu.sh           # GPU处理器评估的shell脚本
  │   ├── run_eval.sh               # Ascend评估的shell脚本
  │   ├── run_train_cpu.sh          # 用于CPU训练的shell脚本
  │   ├── run_train_gpu.sh          # 用于GPU上运行分布式训练的shell脚本
  │   └── run_train.sh              # 用于分布式训练的shell脚本 
  ├── requirements.txt              # 需要的包
  └── task_generator.py             # 数据集预处理

```

## [Script Parameters](#contents)

Parameters for both training and evaluation can be set in config.py

- RelationNet

  ```python

  enable_modelarts: False
  device_target: "Ascend" # Ascend                              # 运行设备  
  DEVICE_ID: 3                                                  # 设备编号 

  model_root: "checkpoints"
  encoder_checkpoint: "omniglot_feature_encoder.ckpt"
  relation_checkpoint: "omniglot_relation_network.ckpt"

  CLASS_NUM: 5                                                  # 类别数目
  SAMPLE_NUM_PER_CLASS: 1                                       # 每类样本的数目
  FEATURE_DIM: 64                                               # 特征维度
  RELATION_DIM: 8                                               # 关系维度
  BATCH_NUM_PER_CLASS: 19                                       # 每批样本数目
  EPISODE: 300000                                               # 训练批数
  TEST_EPISODE: 100                                             # 测试批数
  LEARNING_RATE: 0.001                                          # 学习率
  PRINT_FREQUENCY: 100                                          # 打印频率
  TEST_FREQUENCY: 300                                           # 测试频率

  file_name: "net"                                              # 导出文件名称  
  file_format: "MINDIR"                                         # 导出文件格式  
  image_height: 28                                              # 样本数据图像的高度 
  image_width: 28                                               # 样本数据图像的宽度
  ```

For more configuration details, please refer the script `config.py`.

## [Training Process](#contents)

### Training

- running on Ascend

  ```bash
  python omniglot_train_few_shot.py > omniglot_train_few_shot.log 2>&1 &
  ```

  After training, you'll get some checkpoint files under the script folder by default. The loss value will be achieved as follows:

  ```bash
  # grep "loss" train.log
  Training...
  episode:100    loss:0.10610510    Time of train one episode:62.903ms
  episode:200    loss:0.07776772    Time of train one episode:62.420ms
  episode:300    loss:0.06913537    Time of train one episode:67.289ms
  ```

- running on GPU

  For running on GPU, please change `device_target` from `Ascend` to `GPU` in configuration file default_config.yaml

  ```bash
  export CUDA_VISIBLE_DEVICES=0
  python omniglot_train_few_shot.py > omniglot_train_few_shot.log 2>&1 &
  或
  bash run_train_gpu.sh 8 0,1,2,3,4,5,6,7  
  ```

- running on CPU

   For running on CPU,  please change `device_target` from `Ascend` to `CPU` in configuration file default_config.yaml

  ```bash
  python omniglot_train_few_shot.py  > omniglot_train_few_shot.log 2>&1 &
  或
  bash run_train_cpu.sh
  ```

  All the shell command above will run in the background, you can view the results through the file `train/omniglot_train_few_shot.log`.

### Distributed Training

- running on GPU

For running on GPU, please change `device_target` from `Ascend` to `GPU` in configuration file default_config.yaml

  ```bash
  bash scripts/run_train_gpu.sh 8 0,1,2,3,4,5,6,7
  ```

  The above shell command will run distribute training in the background. You can view the results through the file `train/omniglot_train_few_shot.log`.

## [Evaluation Process](#contents)

### Evaluation

- evaluation on target dataset when running on Ascend

 ```bash  
  python eval.py > eval.log 2>&1 &
  OR
  bash run_eval.sh
  ```

- evaluation on target dataset when running on GPU

  For running on GPU, please change `device_target` from `Ascend` to `GPU` in configuration file default_config.yaml

  ```bash
  python eval.py > eval.log 2>&1 &
  OR  
  bash run_eval_gpu.sh
  ```

- evaluation on target dataset when running on CPU

  For running on CPU, please change `device_target` from `Ascend` to `CPU` in configuration file default_config.yaml

  ```bash
  python eval.py > eval.log 2>&1 &
  OR
  bash run_eval_cpu.sh
  ```  

  The above shell command will run in the background. You can view the results through the file `eval/eval.log`. The accuracy of the test dataset will be as follows:

  ```bash
  Avg Accuracy = 99.4%
  ```

## [Export Process](#contents)

### [Export](#content)

  ```shell
  python export.py
  ```

# [Model Description](#contents)

## [Performance](#contents)

### Training Performance

|         Parameters         |                                            Ascend                                            |
|:--------------------------:|:--------------------------------------------------------------------------------------------:|
|       Model Version        |                                             RelationNet                                             |
|          Resource          |                              Ascend 910；CPU 24cores；Memory 96G;                              |
|       uploaded Date        |                                 07/09/2024 (month/day/year)                                  |
|     MindSpore Version      |                                            2.2.0                                             |
|          Dataset           |                           Omniglot;                            |
|    Training Parameters     |  sample_per_class=1;batch_per_class=19;class_num=5;lr=1e-3; |
|         Optimizer          |                                             Adam                                             |
|       Loss Function        |                                    MSE                                     |
|          outputs           |                                         probability                                          |
|           Speed            |                   1pc：62.903ms/episode;                   |
|         Total time         |                                       1pc: 182.45 mins                                        |
|       Parameters (M)       |                                             223k                                             |
| Checkpoint for Fine tuning |                                      0.887M (.ckpt file)                                      |
|    Model for inference     |                            0.901M (.onnx file),  0.21M(.air file)                             |

### Evaluation Performance


| Parameters |              Ascend               |
|:-------:|:---------------------------------:|
| Model Version |               RelationNet                |
| Resource  | Ascend 910；CPU 24cores；Memory 96G; |
| uploaded Date |    07/09/2024 (month/day/year)    |
| MindSpore Version |               2.2.0              |
| Dataset |        Omniglot         |
| outputs |            probability            |
| Accuracy |      1pc: 99.4%       |
| Model for inference |        0.901M (.onnx file)         |

# [Description of Random Situation](#contents)

In train.py, we use init_random_seed() function in utiles.py sets a random number seed.

# [ModelZoo Homepage](#contents)

 Please check the official [homepage](https://gitee.com/mindspore/models).
