# Data for Fine-tuning LLMs

**Please note: the fine-tuning process and fine-tuned model [^1] is not necessary for DAIRstega! In other words, the base LLMs of DAIRstega can be any open-source LLMs itself, 
or fine-tuned LLMs using any domain knowledge. The generated stegos are similar to the covers generated by the corresponding LLMs.
The reason for fine-tuning in this paper is to show that DAIRstega has excellent generation performance in many discourses and fields.**
[^1]: https://github.com/WangYH-BUPT/DAIRstega/tree/master/ft-model

We obtained a large amount of data from Wikipedia, Twitter, GPT4, and the publicly available data [^2] for fine-tuning LLMs.
The final dataset contains 57,414 texts and nearly 100 different discourse characteristics to fine-tune the language models of all schemes.
[^2]: https://github.com/tloen/alpaca-lora/tree/main
