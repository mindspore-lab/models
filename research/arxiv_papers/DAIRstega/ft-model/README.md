# Fine-tuned model

**Please note: the fine-tuned model and fine-tuning process [^1] are not necessary for DAIRstega! In other words, the base LLMs of DAIRstega can be any open-source LLMs itself, or fine-tuned LLMs using any domain knowledge. The generated stegos are similar to the covers generated by the corresponding LLMs. The reason for fine-tuning in this paper is to show that DAIRstega has excellent generation performance in many discourses and fields.**
[^1]: https://github.com/WangYH-BUPT/DAIRstega/tree/master/finetune_data

The model files in this directory are fine-tuned using LoRA, where the rank `r=8` (expandable), and the fine-tuning modules are `Q` and `V` (modules such as `K` and `O` can also be fine-tuned).
