## Adaptive Knowledge Distillation

#### **1. 项目简介 (Project Overview)**

本项目旨在探索并实现一种**自适应知识蒸馏（Adaptive Knowledge Distillation, AKD）**框架，以提升学生模型在视觉任务上的性能。与传统的静态知识蒸馏方法不同，本方法引入**上下文感知教师（Context-Aware Teacher）**模块，使教师模型的“教学”过程能够根据学生模型的训练情况进行动态调整，从而提供更具针对性和更有效的知识。

该框架的核心思想在于：

- **动态化知识传递：** 教师模型的知识输出不再是固定的，而是根据当前的训练批次和学生模型的表现进行实时调整。
- **增强教师指导：** 引入一个**上下文感知模块（Contextual Module）**，通过实时分析师生模型差异，动态地重新加权教师的软标签输出，以更好地指导学生模型。

#### **2. 技术路线与方法 (Technical Approach)**

**2.1. Baseline 模型选择 (Baseline Model Selection)** 本项目的基线实现将基于MindSpore框架，并采用以下模型组合进行验证：

- **教师模型：** 选择预训练的强大模型，如**ViT-B/16 (Vision Transformer)** 或 **Swin-T (Transformer-based models)**，它们在复杂的视觉任务上表现卓越，适合作为知识输出源。
- **学生模型：** 选用参数量小、推理速度快的轻量级模型，如**ResNet-18** 或 **MobileNetV3-Small**，以满足边缘设备部署的需求。

**2.2. 优化**

传统的知识蒸馏技术使用一个固定的平衡因子 $\alpha$ 来平衡硬标签交叉熵损失和软标签蒸馏损失。然而，一个静态的 $\alpha$ 并非最优，因为硬监督和软监督之间的最佳权衡在训练过程中是变化的。本项目的核心创新点在于：

- **可学习的 $\alpha$：** 将 $\alpha$ 作为可学习参数，在训练期间自动优化。
- **上下文感知模块（CAM）：** 引入一个基于**多层感知机（MLP）**和**注意力机制（Attention）**的CAM，以自适应地重新加权类别级别的教师输出，从而更有效地指导学生模型。

该方法在CIFAR-10数据集上进行了实验，使用ResNet-50作为教师模型，ResNet-18作为学生模型。实验结果表明，与固定权重的基准知识蒸馏方法相比，本方法在准确率上取得了更好的表现，并带来了更稳定的收敛。

#### **2. 数据集与环境**

- **数据集：** 本项目使用 CIFAR-10 数据集。
- **框架：** 华为 MindSpore Ascend。

#### 附录

论文已发表在 $arxiv$ 预印本平台，论文链接：http://arxiv.org/abs/2509.05319
