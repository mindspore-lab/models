{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc9da1-fbb2-46c9-8232-d2bcf643c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CMT-Mamba知识蒸馏半监督学习框架\n",
    "数据集: CIFAR10\n",
    "教师模型: CMT-Small with Mamba\n",
    "学生模型: ResNet18\n",
    "训练方式: 半监督学习\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as transforms\n",
    "import mindspore.dataset.vision as vision\n",
    "from mindspore.train.callback import ModelCheckpoint, LossMonitor, TimeMonitor\n",
    "import sys\n",
    "import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 导入MindCV相关模块\n",
    "from mindcv.models import create_model\n",
    "from mindcv.data import create_dataset, create_transforms, create_loader\n",
    "from mindcv.loss import create_loss\n",
    "from mindcv.scheduler import create_scheduler\n",
    "\n",
    "# 设置随机种子确保实验可重复\n",
    "set_seed(42)\n",
    "\n",
    "# 定义命令行参数\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='半监督知识蒸馏学习 CMT-Mamba → ResNet')\n",
    "\n",
    "# 模型参数\n",
    "parser.add_argument('--teacher_model', type=str, default='cmt_small', help='教师模型名称')\n",
    "parser.add_argument('--student_model', type=str, default='resnet18', help='学生模型名称')\n",
    "parser.add_argument('--use_mamba', action='store_true', default=True, help='教师模型是否使用Mamba模块')\n",
    "parser.add_argument('--pretrained_teacher', type=str, default='', help='预训练教师模型路径')\n",
    "\n",
    "# 数据集参数\n",
    "parser.add_argument('--dataset', type=str, default='cifar10', help='数据集名称')\n",
    "parser.add_argument('--data_dir', type=str, default='cifar-10-batches-bin', help='数据集路径')\n",
    "parser.add_argument('--labeled_ratio', type=float, default=0.1, help='有标签数据比例')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='批量大小')\n",
    "parser.add_argument('--num_workers', type=int, default=8, help='数据加载器工作线程数')\n",
    "\n",
    "# 训练参数\n",
    "parser.add_argument('--epochs', type=int, default=200, help='训练轮数')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='学习率')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='SGD动量')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='权重衰减')\n",
    "parser.add_argument('--T', type=float, default=2.0, help='蒸馏温度')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='蒸馏损失权重')\n",
    "parser.add_argument('--beta', type=float, default=0.3, help='一致性损失权重')\n",
    "parser.add_argument('--ema_decay', type=float, default=0.999, help='指数移动平均衰减率')\n",
    "\n",
    "# 设备参数\n",
    "parser.add_argument('--device_target', type=str, default='Ascend', choices=['Ascend', 'GPU', 'CPU'], \n",
    "                    help='运行设备')\n",
    "parser.add_argument('--device_id', type=int, default=0, help='设备ID')\n",
    "parser.add_argument('--amp_level', type=str, default='O2', help='混合精度级别')\n",
    "\n",
    "# 保存路径参数\n",
    "parser.add_argument('--save_dir', type=str, default='./checkpoints', help='模型保存路径')\n",
    "parser.add_argument('--save_interval', type=int, default=10, help='模型保存间隔')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 配置运行环境\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target, device_id=args.device_id)\n",
    "\n",
    "# 创建数据增强\n",
    "def create_weak_augmentation():\n",
    "    \"\"\"创建弱数据增强\"\"\"\n",
    "    return [\n",
    "        # 添加Resize操作将CIFAR10图像放大到模型期望的尺寸\n",
    "        vision.Resize(224),  # CMT模型期望的输入尺寸\n",
    "        vision.RandomCrop(224, padding=28),  # 相应调整RandomCrop尺寸\n",
    "        vision.RandomHorizontalFlip(prob=0.5),\n",
    "        vision.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "def create_strong_augmentation():\n",
    "    \"\"\"创建强数据增强 (RandAugment)\"\"\"\n",
    "    return [\n",
    "        # 添加Resize操作将CIFAR10图像放大到模型期望的尺寸\n",
    "        vision.Resize(224),  # CMT模型期望的输入尺寸\n",
    "        vision.RandomCrop(224, padding=28),  # 相应调整RandomCrop尺寸\n",
    "        vision.RandomHorizontalFlip(prob=0.5),\n",
    "        # RandAugment替代方案：增强多种变换组合\n",
    "        vision.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        vision.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=5),\n",
    "        vision.RandomErasing(prob=0.2),\n",
    "        vision.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "# 自定义EMA模型更新器\n",
    "class EMA:\n",
    "    def __init__(self, model, shadow_model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.shadow_model = shadow_model\n",
    "        self.decay = decay\n",
    "        self.shadow_params = [p.clone() for p in shadow_model.get_parameters()]\n",
    "        self.backup_params = []\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"更新Shadow模型参数\"\"\"\n",
    "        model_params = list(self.model.get_parameters())\n",
    "        for i, param in enumerate(self.shadow_params):\n",
    "            param.assign_value((self.decay * param + (1 - self.decay) * model_params[i]))\n",
    "            \n",
    "    def apply_shadow(self):\n",
    "        \"\"\"应用Shadow参数到模型\"\"\"\n",
    "        model_params = list(self.model.get_parameters())\n",
    "        self.backup_params = [p.clone() for p in model_params]\n",
    "        for i, param in enumerate(model_params):\n",
    "            param.assign_value(self.shadow_params[i])\n",
    "            \n",
    "    def restore(self):\n",
    "        \"\"\"恢复模型原始参数\"\"\"\n",
    "        model_params = list(self.model.get_parameters())\n",
    "        for i, param in enumerate(model_params):\n",
    "            param.assign_value(self.backup_params[i])\n",
    "\n",
    "\n",
    "# 定义CMT-Mamba知识蒸馏半监督训练流程\n",
    "def train_distill():\n",
    "    # 设置日志记录\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_dir = os.path.join(\"logs\", f\"cmt_mamba_distill_{current_time}\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    log_file = os.path.join(log_dir, \"training.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 记录训练配置\n",
    "    logging.info(f\"启动CMT-Mamba知识蒸馏半监督训练\")\n",
    "    logging.info(f\"配置参数: {vars(args)}\")\n",
    "    \n",
    "    # 加载CIFAR10数据集\n",
    "    cifar10_dataset = create_dataset(\n",
    "        name=args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split='train',\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    # 手动进行有标签/无标签分割\n",
    "    dataset_np = []\n",
    "    print(\"加载并转换CIFAR10数据集...\")\n",
    "    for img, label in cifar10_dataset:\n",
    "        dataset_np.append((img.asnumpy(), label.asnumpy().item()))\n",
    "    \n",
    "    print(f\"数据集大小: {len(dataset_np)}\")\n",
    "    rng = np.random.RandomState(42)\n",
    "    rng.shuffle(dataset_np)\n",
    "    \n",
    "    # 按类别分割\n",
    "    class_indices = [[] for _ in range(10)]  # CIFAR10有10个类别\n",
    "    for i, (img, label) in enumerate(dataset_np):\n",
    "        class_indices[label].append(i)\n",
    "    \n",
    "    # 对每个类别进行分层抽样\n",
    "    labeled_indices = []\n",
    "    unlabeled_indices = []\n",
    "    for indices in class_indices:\n",
    "        n_labeled = max(1, int(len(indices) * args.labeled_ratio))\n",
    "        labeled_indices.extend(indices[:n_labeled])\n",
    "        unlabeled_indices.extend(indices[n_labeled:])\n",
    "    \n",
    "    # 创建有标签和无标签数据集\n",
    "    labeled_data = [(dataset_np[i][0], dataset_np[i][1]) for i in labeled_indices]\n",
    "    unlabeled_data = [(dataset_np[i][0], dataset_np[i][1]) for i in unlabeled_indices]\n",
    "    \n",
    "    print(f\"有标签数据量: {len(labeled_data)}, 无标签数据量: {len(unlabeled_data)}\")\n",
    "    \n",
    "    # 创建数据转换\n",
    "    weak_transform = create_weak_augmentation()\n",
    "    strong_transform = create_strong_augmentation()\n",
    "    \n",
    "    # 创建有标签数据加载器\n",
    "    def labeled_generator():\n",
    "        indices = list(range(len(labeled_data)))\n",
    "        while True:\n",
    "            rng.shuffle(indices)\n",
    "            for idx in indices:\n",
    "                yield labeled_data[idx]\n",
    "    \n",
    "    labeled_ds = ds.GeneratorDataset(\n",
    "        source=labeled_generator(),\n",
    "        column_names=[\"image\", \"label\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    labeled_ds = labeled_ds.map(operations=weak_transform, input_columns=[\"image\"])\n",
    "    labeled_ds = labeled_ds.batch(args.batch_size)\n",
    "    \n",
    "    # 创建无标签数据加载器\n",
    "    def unlabeled_generator():\n",
    "        indices = list(range(len(unlabeled_data)))\n",
    "        while True:\n",
    "            rng.shuffle(indices)\n",
    "            for idx in indices:\n",
    "                img, _ = unlabeled_data[idx]\n",
    "                yield img, img  # 弱增强、强增强的原始图像\n",
    "    \n",
    "    unlabeled_ds = ds.GeneratorDataset(\n",
    "        source=unlabeled_generator(),\n",
    "        column_names=[\"weak_image\", \"strong_image\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    unlabeled_ds = unlabeled_ds.map(operations=weak_transform, input_columns=[\"weak_image\"])\n",
    "    unlabeled_ds = unlabeled_ds.map(operations=strong_transform, input_columns=[\"strong_image\"])\n",
    "    unlabeled_ds = unlabeled_ds.batch(args.batch_size)\n",
    "    \n",
    "    # 测试集预处理\n",
    "    test_transform = [\n",
    "        vision.Resize(224),  # 确保测试图像也调整到正确尺寸\n",
    "        vision.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    # 创建测试数据集\n",
    "    test_ds = create_dataset(\n",
    "        name=args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split='test',\n",
    "        download=False\n",
    "    )\n",
    "    test_ds = test_ds.map(operations=test_transform, input_columns=[\"image\"])\n",
    "    test_ds = test_ds.batch(args.batch_size)\n",
    "    \n",
    "    # 创建模型\n",
    "    print(\"创建教师模型(CMT-Small)和学生模型(ResNet18)...\")\n",
    "\n",
    "    # 创建教师模型 - 使用CMT-Small with Mamba\n",
    "    teacher_model = create_model(\n",
    "        args.teacher_model,\n",
    "        num_classes=10,\n",
    "        in_channels=3,\n",
    "        use_mamba=args.use_mamba,\n",
    "    )\n",
    "\n",
    "    # 创建学生模型 - 使用ResNet18\n",
    "    student_model = create_model(\n",
    "        args.student_model,\n",
    "        num_classes=10,\n",
    "        in_channels=3,\n",
    "    )\n",
    "\n",
    "    # 检查是否存在预训练的教师模型\n",
    "    if not args.pretrained_teacher:\n",
    "        logging.info(\"未找到预训练教师模型，开始使用有标签数据训练教师模型...\")\n",
    "        \n",
    "        # 创建教师模型优化器\n",
    "        teacher_optimizer = nn.Momentum(\n",
    "            params=teacher_model.trainable_params(),\n",
    "            learning_rate=args.lr,\n",
    "            momentum=args.momentum,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "        \n",
    "        # 仅使用有标签数据训练教师模型\n",
    "        train_teacher(teacher_model, teacher_optimizer, labeled_ds, test_ds, epochs=100, labeled_size=len(labeled_data))\n",
    "        \n",
    "        # 保存训练好的教师模型\n",
    "        ms.save_checkpoint(teacher_model, \"teacher_pretrained.ckpt\")\n",
    "        logging.info(\"教师模型预训练完成，已保存检查点\")\n",
    "    \n",
    "    # 创建优化器\n",
    "    lr = nn.cosine_decay_lr(\n",
    "        min_lr=args.lr * 0.01,\n",
    "        max_lr=args.lr,\n",
    "        total_step=args.epochs * (len(labeled_data) // args.batch_size),\n",
    "        step_per_epoch=len(labeled_data) // args.batch_size,\n",
    "        decay_epoch=args.epochs\n",
    "    )\n",
    "    \n",
    "    optimizer = nn.Momentum(\n",
    "        params=student_model.trainable_params(),\n",
    "        learning_rate=lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    \n",
    "    # 创建EMA更新器\n",
    "    teacher_ema = EMA(student_model, teacher_model, decay=args.ema_decay)\n",
    "    \n",
    "    # 修改DistillationTrainStep类\n",
    "    class DistillationTrainStep(nn.Cell):\n",
    "        def __init__(self, student_model, teacher_model, optimizer, args):\n",
    "            super(DistillationTrainStep, self).__init__()\n",
    "            self.student_model = student_model\n",
    "            self.teacher_model = teacher_model\n",
    "            self.optimizer = optimizer\n",
    "            self.args = args\n",
    "            # 显式指定网络参数，确保梯度计算与优化器参数匹配\n",
    "            self.weights = self.student_model.trainable_params()\n",
    "            self.grad_fn = ops.value_and_grad(self.forward, None, self.weights, has_aux=False)\n",
    "        \n",
    "        def forward(self, labeled_imgs, labels, unlabeled_weak, unlabeled_strong):\n",
    "            # 教师模型推理\n",
    "            teacher_labeled_logits = self.teacher_model(labeled_imgs)\n",
    "            teacher_unlabeled_logits = self.teacher_model(unlabeled_weak)\n",
    "            \n",
    "            # 学生模型推理\n",
    "            student_labeled_logits = self.student_model(labeled_imgs)\n",
    "            student_unlabeled_strong_logits = self.student_model(unlabeled_strong)\n",
    "            \n",
    "            # 计算蒸馏损失\n",
    "            distill_loss = self._distill_loss_fn(student_labeled_logits, teacher_labeled_logits, labels)\n",
    "            \n",
    "            # 计算半监督一致性损失\n",
    "            # 生成伪标签\n",
    "            pseudo_labels = ops.softmax(teacher_unlabeled_logits, axis=1)\n",
    "            max_probs, targets = ops.max(pseudo_labels, axis=1)\n",
    "            mask = (max_probs > 0.95).astype(ms.float32)  # 高置信度伪标签的掩码\n",
    "            \n",
    "            # 计算一致性损失\n",
    "            consistency_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='none')(\n",
    "                student_unlabeled_strong_logits, \n",
    "                targets\n",
    "            )\n",
    "            consistency_loss = (consistency_loss * mask).mean()\n",
    "            \n",
    "            # 计算总损失\n",
    "            total_loss = distill_loss + self.args.beta * consistency_loss\n",
    "            \n",
    "            return total_loss\n",
    "        \n",
    "        def _distill_loss_fn(self, student_logits, teacher_logits, labels):\n",
    "            # 监督损失\n",
    "            ce_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "            hard_loss = ce_loss(student_logits, labels)\n",
    "            \n",
    "            # 蒸馏软标签损失\n",
    "            T = self.args.T\n",
    "            soft_student = ops.softmax(student_logits / T, axis=1)\n",
    "            soft_teacher = ops.softmax(teacher_logits / T, axis=1)\n",
    "            \n",
    "            kl_div = ops.kl_div(\n",
    "                ops.log(soft_student + 1e-10),\n",
    "                soft_teacher,\n",
    "                reduction='batchmean'\n",
    "            ) * (T * T)\n",
    "            \n",
    "            # 组合损失\n",
    "            return (1 - self.args.alpha) * hard_loss + self.args.alpha * kl_div\n",
    "        \n",
    "        def construct(self, labeled_imgs, labels, unlabeled_weak, unlabeled_strong):\n",
    "            loss, grads = self.grad_fn(labeled_imgs, labels, unlabeled_weak, unlabeled_strong)\n",
    "            self.optimizer(grads)\n",
    "            return loss\n",
    "    \n",
    "    # 创建训练网络\n",
    "    net = DistillationTrainStep(student_model, teacher_model, optimizer, args)\n",
    "    \n",
    "    # 创建评估函数\n",
    "    def eval_model():\n",
    "        top1_correct = 0\n",
    "        top5_correct = 0\n",
    "        total = 0\n",
    "        student_model.set_train(False)\n",
    "        \n",
    "        logging.info(\"开始评估模型...\")\n",
    "        for data in tqdm(test_ds.create_dict_iterator(), desc=\"Evaluating\"):\n",
    "            images = data[\"image\"]\n",
    "            labels = data[\"label\"]\n",
    "            outputs = student_model(images)\n",
    "            \n",
    "            # 计算TOP1准确率\n",
    "            _, top1_pred = ops.max(outputs, axis=1)\n",
    "            top1_pred = top1_pred.astype(ms.int32)\n",
    "            labels = labels.astype(ms.int32)\n",
    "            \n",
    "            # 计算TOP5准确率\n",
    "            top5_preds = ops.topk(outputs, 5)[1]\n",
    "            top5_preds = top5_preds.astype(ms.int32)\n",
    "            \n",
    "            # 使用广播比较每个样本的TOP5预测是否包含真实标签\n",
    "            top5_correct_per_sample = ops.equal(\n",
    "                top5_preds, \n",
    "                ops.reshape(labels, (-1, 1)).repeat(5, axis=1)\n",
    "            )\n",
    "            top5_correct_mask = ops.cast(ops.any(top5_correct_per_sample, 1), ms.float32)\n",
    "            \n",
    "            # 累计正确预测\n",
    "            top1_correct_mask = ops.cast(top1_pred == labels, ms.float32)\n",
    "            \n",
    "            total += labels.shape[0]\n",
    "            top1_correct += top1_correct_mask.sum().asnumpy().item()\n",
    "            top5_correct += top5_correct_mask.sum().asnumpy().item()\n",
    "        \n",
    "        top1_acc = top1_correct / total\n",
    "        top5_acc = top5_correct / total\n",
    "        \n",
    "        logging.info(f\"评估结果 - TOP1准确率: {top1_acc:.4f}, TOP5准确率: {top5_acc:.4f}\")\n",
    "        return top1_acc, top5_acc\n",
    "    \n",
    "    # 创建模型保存目录\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    \n",
    "    # 开始训练\n",
    "    print(\"开始知识蒸馏半监督训练...\")\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    labeled_iter = labeled_ds.create_dict_iterator()\n",
    "    unlabeled_iter = unlabeled_ds.create_dict_iterator()\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        # 设置训练模式\n",
    "        student_model.set_train(True)\n",
    "        teacher_model.set_train(False)  # 教师模型始终处于评估模式\n",
    "        \n",
    "        # 训练一个epoch\n",
    "        total_loss = 0.0\n",
    "        steps = min(len(labeled_data) // args.batch_size, len(unlabeled_data) // args.batch_size)\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch+1}/{args.epochs} 开始训练...\")\n",
    "        epoch_progress = tqdm(range(steps), desc=f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        \n",
    "        for step in epoch_progress:\n",
    "            try:\n",
    "                labeled_batch = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = labeled_ds.create_dict_iterator()\n",
    "                labeled_batch = next(labeled_iter)\n",
    "            \n",
    "            try:\n",
    "                unlabeled_batch = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = unlabeled_ds.create_dict_iterator()\n",
    "                unlabeled_batch = next(unlabeled_iter)\n",
    "            \n",
    "            labeled_imgs = labeled_batch[\"image\"]\n",
    "            labels = labeled_batch[\"label\"]\n",
    "            weak_imgs = unlabeled_batch[\"weak_image\"]\n",
    "            strong_imgs = unlabeled_batch[\"strong_image\"]\n",
    "            \n",
    "            loss = net(labeled_imgs, labels, weak_imgs, strong_imgs)\n",
    "            total_loss += loss.asnumpy().item()\n",
    "            \n",
    "            # 更新进度条\n",
    "            epoch_progress.set_postfix(loss=f\"{loss.asnumpy().item():.4f}\")\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                logging.info(f\"Epoch: {epoch+1}/{args.epochs}, Step: {step+1}/{steps}, Loss: {loss.asnumpy().item():.4f}\")\n",
    "        \n",
    "        # 评估模型\n",
    "        avg_loss = total_loss / steps\n",
    "        logging.info(f\"Epoch: {epoch+1}/{args.epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 使用学生模型评估\n",
    "        top1_acc, top5_acc = eval_model()\n",
    "        logging.info(f\"Epoch: {epoch+1}/{args.epochs}, TOP1 Accuracy: {top1_acc:.4f}, TOP5 Accuracy: {top5_acc:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型 (基于TOP1准确率)\n",
    "        if top1_acc > best_acc:\n",
    "            best_acc = top1_acc\n",
    "            ms.save_checkpoint(student_model, os.path.join(args.save_dir, f\"{args.student_model}_best.ckpt\"))\n",
    "            logging.info(f\"已保存最佳模型，TOP1准确率: {best_acc:.4f}, TOP5准确率: {top5_acc:.4f}\")\n",
    "        \n",
    "        # 每隔一定轮数保存检查点\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            ms.save_checkpoint(student_model, os.path.join(args.save_dir, f\"{args.student_model}_epoch{epoch+1}.ckpt\"))\n",
    "            logging.info(f\"已保存第{epoch+1}轮检查点\")\n",
    "    \n",
    "    print(f\"训练完成！最佳TOP1准确率: {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "def train_teacher(model, optimizer, train_ds, test_ds, epochs, labeled_size):\n",
    "    \"\"\"训练教师模型的函数\"\"\"\n",
    "    # 创建动态学习率\n",
    "    total_steps = epochs * (labeled_size // args.batch_size)\n",
    "    lr = nn.cosine_decay_lr(\n",
    "        min_lr=0.0,\n",
    "        max_lr=args.lr,\n",
    "        total_step=total_steps,\n",
    "        step_per_epoch=labeled_size // args.batch_size,\n",
    "        decay_epoch=epochs\n",
    "    )\n",
    "    dynamic_lr = ms.Parameter(ms.Tensor(lr, ms.float32))\n",
    "    \n",
    "    # 创建新的优化器，使用动态学习率\n",
    "    optimizer = nn.Momentum(\n",
    "        params=model.trainable_params(),\n",
    "        learning_rate=dynamic_lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    \n",
    "    # 定义训练步骤\n",
    "    class TeacherTrainStep(nn.Cell):\n",
    "        def __init__(self, network, optimizer):\n",
    "            super(TeacherTrainStep, self).__init__()\n",
    "            self.network = network\n",
    "            self.optimizer = optimizer\n",
    "            self.grad_fn = ops.value_and_grad(self.forward, None, self.network.trainable_params())\n",
    "            self.loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "        \n",
    "        def forward(self, images, labels):\n",
    "            logits = self.network(images)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return loss\n",
    "        \n",
    "        def construct(self, images, labels):\n",
    "            loss, grads = self.grad_fn(images, labels)\n",
    "            self.optimizer(grads)\n",
    "            return loss\n",
    "    \n",
    "    # 创建训练网络\n",
    "    train_step = TeacherTrainStep(model, optimizer)\n",
    "    \n",
    "    # 定义评估函数\n",
    "    def evaluate():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.set_train(False)\n",
    "        for data in test_ds.create_dict_iterator():\n",
    "            images = data[\"image\"]\n",
    "            labels = data[\"label\"]\n",
    "            outputs = model(images)\n",
    "            _, predicted = ops.max(outputs, axis=1)\n",
    "            predicted = predicted.astype(ms.int32)\n",
    "            labels = labels.astype(ms.int32)\n",
    "            total += labels.shape[0]\n",
    "            correct += (predicted == labels).astype(ms.float32).sum().asnumpy().item()\n",
    "        return correct / total\n",
    "    \n",
    "    logging.info(\"开始训练教师模型...\")\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        model.set_train(True)\n",
    "        total_loss = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        # 计算每个epoch的总步数\n",
    "        total_steps = labeled_size // args.batch_size\n",
    "        \n",
    "        # 使用tqdm显示进度\n",
    "        train_iter = tqdm(range(total_steps), desc=f\"Teacher Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        data_iter = train_ds.create_dict_iterator()\n",
    "        for _ in train_iter:\n",
    "            try:\n",
    "                data = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = train_ds.create_dict_iterator()\n",
    "                data = next(data_iter)\n",
    "            \n",
    "            images = data[\"image\"]\n",
    "            labels = data[\"label\"]\n",
    "            loss = train_step(images, labels)\n",
    "            total_loss += loss.asnumpy().item()\n",
    "            steps += 1\n",
    "            \n",
    "            # 更新进度条\n",
    "            train_iter.set_postfix(loss=f\"{loss.asnumpy().item():.4f}\")\n",
    "            \n",
    "            # 每个epoch训练固定步数\n",
    "            if steps >= total_steps:\n",
    "                break\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / steps\n",
    "        \n",
    "        # 评估模型\n",
    "        acc = evaluate()\n",
    "        \n",
    "        # 记录训练信息\n",
    "        logging.info(f\"Teacher Epoch: {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            ms.save_checkpoint(model, \"teacher_best.ckpt\")\n",
    "            logging.info(f\"保存最佳教师模型，准确率: {acc:.4f}\")\n",
    "    \n",
    "    # 训练结束后加载最佳模型\n",
    "    param_dict = ms.load_checkpoint(\"teacher_best.ckpt\")\n",
    "    ms.load_param_into_net(model, param_dict)\n",
    "    logging.info(f\"教师模型训练完成，最佳准确率: {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_distill() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4ca36-1810-4f9a-94bd-6733e5862181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CMT-Mamba知识蒸馏半监督学习框架\n",
    "数据集: CIFAR10\n",
    "教师模型: CMT-Small with Mamba\n",
    "学生模型: ResNet18\n",
    "训练方式: 半监督学习\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import context\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as transforms\n",
    "import mindspore.dataset.vision as vision\n",
    "from mindspore.train.callback import ModelCheckpoint, LossMonitor, TimeMonitor\n",
    "import sys\n",
    "import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from typing import Optional, Callable, Tuple, Any\n",
    "\n",
    "# 导入MindCV相关模块\n",
    "from mindcv.models import create_model\n",
    "from mindcv.data import create_dataset, create_transforms, create_loader\n",
    "from mindcv.loss import create_loss\n",
    "from mindcv.scheduler import create_scheduler\n",
    "\n",
    "# 设置随机种子确保实验可重复\n",
    "set_seed(42)\n",
    "\n",
    "# 定义命令行参数\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='半监督知识蒸馏学习 CMT-Mamba → ResNet')\n",
    "\n",
    "# 模型参数\n",
    "parser.add_argument('--teacher_model', type=str, default='cmt_small', help='教师模型名称')\n",
    "parser.add_argument('--student_model', type=str, default='resnet18', help='学生模型名称')\n",
    "parser.add_argument('--use_mamba', action='store_true', default=True, help='教师模型是否使用Mamba模块')\n",
    "parser.add_argument('--pretrained_teacher', type=str, default='', help='预训练教师模型路径')\n",
    "\n",
    "# 数据集参数\n",
    "parser.add_argument('--dataset', type=str, default='cifar10', choices=['cifar10', 'cifar100', 'cub200'], \n",
    "                    help='数据集名称')\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='类别数量(cifar10为10，cifar100为100，cub200为200)')\n",
    "parser.add_argument('--data_dir', type=str, default='', help='数据集路径')\n",
    "parser.add_argument('--labeled_ratio', type=float, default=0.1, help='有标签数据比例')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='批量大小')\n",
    "parser.add_argument('--num_workers', type=int, default=2, help='数据加载器工作线程数')\n",
    "\n",
    "# 训练参数\n",
    "parser.add_argument('--epochs', type=int, default=200, help='训练轮数')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='学习率')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='SGD动量')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4, help='权重衰减')\n",
    "parser.add_argument('--T', type=float, default=2.0, help='蒸馏温度')\n",
    "parser.add_argument('--alpha', type=float, default=0.5, help='蒸馏损失权重')\n",
    "parser.add_argument('--beta', type=float, default=0.3, help='一致性损失权重')\n",
    "parser.add_argument('--ema_decay', type=float, default=0.999, help='指数移动平均衰减率')\n",
    "\n",
    "# 设备参数\n",
    "parser.add_argument('--device_target', type=str, default='Ascend', choices=['Ascend', 'GPU', 'CPU'], \n",
    "                    help='运行设备')\n",
    "parser.add_argument('--device_id', type=int, default=0, help='设备ID')\n",
    "parser.add_argument('--amp_level', type=str, default='O2', help='混合精度级别')\n",
    "\n",
    "# 保存路径参数\n",
    "parser.add_argument('--save_dir', type=str, default='./checkpoints', help='模型保存路径')\n",
    "parser.add_argument('--save_interval', type=int, default=10, help='模型保存间隔')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 配置运行环境\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target, device_id=args.device_id)\n",
    "\n",
    "# 创建数据增强\n",
    "def create_weak_augmentation():\n",
    "    \"\"\"创建弱数据增强\"\"\"\n",
    "    # 根据数据集选择合适的归一化参数\n",
    "    if args.dataset == 'cifar10':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2023, 0.1994, 0.2010]\n",
    "    else:  # cifar100\n",
    "        mean = [0.5071, 0.4867, 0.4408]\n",
    "        std = [0.2675, 0.2565, 0.2761]\n",
    "        \n",
    "    return [\n",
    "        # 添加Resize操作将图像放大到模型期望的尺寸\n",
    "        vision.Resize(224),  # CMT模型期望的输入尺寸\n",
    "        vision.RandomCrop(224, padding=28),  # 相应调整RandomCrop尺寸\n",
    "        vision.RandomHorizontalFlip(prob=0.5),\n",
    "        vision.Normalize(mean=mean, std=std),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "def create_strong_augmentation():\n",
    "    \"\"\"创建强数据增强 (RandAugment)\"\"\"\n",
    "    # 根据数据集选择合适的归一化参数\n",
    "    if args.dataset == 'cifar10':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2023, 0.1994, 0.2010]\n",
    "    else:  # cifar100\n",
    "        mean = [0.5071, 0.4867, 0.4408]\n",
    "        std = [0.2675, 0.2565, 0.2761]\n",
    "        \n",
    "    return [\n",
    "        # 添加Resize操作将图像放大到模型期望的尺寸\n",
    "        vision.Resize(224),  # CMT模型期望的输入尺寸\n",
    "        vision.RandomCrop(224, padding=28),  # 相应调整RandomCrop尺寸\n",
    "        vision.RandomHorizontalFlip(prob=0.5),\n",
    "        # RandAugment替代方案：增强多种变换组合\n",
    "        vision.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        vision.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=5),\n",
    "        vision.RandomErasing(prob=0.2),\n",
    "        vision.Normalize(mean=mean, std=std),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "# 创建一个函数来处理测试数据\n",
    "def process_test_batch(batch_data):\n",
    "    \"\"\"统一处理测试批次数据\"\"\"\n",
    "    if args.dataset == 'cifar10':\n",
    "        images, labels = batch_data\n",
    "    else:  # cifar100\n",
    "        images, fine_labels, _ = batch_data  # 解包三个返回值\n",
    "        labels = fine_labels\n",
    "    \n",
    "    # 应用数据转换\n",
    "    # 根据数据集选择合适的归一化参数\n",
    "    if args.dataset == 'cifar10':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2023, 0.1994, 0.2010]\n",
    "    else:  # cifar100\n",
    "        mean = [0.5071, 0.4867, 0.4408]\n",
    "        std = [0.2675, 0.2565, 0.2761]\n",
    "        \n",
    "    test_transform = [\n",
    "        vision.Resize(224),  # 确保测试图像也调整到正确尺寸\n",
    "        vision.Normalize(mean=mean, std=std),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    \n",
    "    # 如果输入是字典类型，则提取图像和标签\n",
    "    if isinstance(batch_data, dict):\n",
    "        images = batch_data.get(\"image\")\n",
    "        labels = batch_data.get(\"label\")\n",
    "    else:\n",
    "        # 已经处理过的元组\n",
    "        pass\n",
    "    \n",
    "    # 应用转换\n",
    "    for op in test_transform:\n",
    "        images = op(images)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# 自定义EMA模型更新器\n",
    "class EMA:\n",
    "    def __init__(self, model, shadow_model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.shadow_model = shadow_model\n",
    "        self.decay = decay\n",
    "        self.shadow_params = [p.clone() for p in shadow_model.get_parameters()]\n",
    "        self.backup_params = []\n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"更新Shadow模型参数\"\"\"\n",
    "        model_params = list(self.model.get_parameters())\n",
    "        for i, param in enumerate(self.shadow_params):\n",
    "            param.assign_value((self.decay * param + (1 - self.decay) * model_params[i]))\n",
    "            \n",
    "    def apply_shadow(self):\n",
    "        \"\"\"应用Shadow参数到模型\"\"\"\n",
    "        model_params = list(self.model.get_parameters())\n",
    "        self.backup_params = [p.clone() for p in model_params]\n",
    "        for i, param in enumerate(model_params):\n",
    "            param.assign_value(self.shadow_params[i])\n",
    "            \n",
    "    def restore(self):\n",
    "        \"\"\"恢复模型原始参数\"\"\"\n",
    "        model_params = list(self.model.get_parameters())\n",
    "        for i, param in enumerate(model_params):\n",
    "            param.assign_value(self.backup_params[i])\n",
    "\n",
    "# 添加CUB200Dataset类\n",
    "class CUB200Dataset:\n",
    "    \"\"\"CUB_200_2011数据集加载器\"\"\"\n",
    "    \n",
    "    def __init__(self, root, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        初始化CUB_200_2011数据集\n",
    "        \n",
    "        Args:\n",
    "            root: 数据集根目录\n",
    "            split: 'train'或'test'\n",
    "            transform: 图像变换\n",
    "        \"\"\"\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # 读取图像列表和对应的类别\n",
    "        self.image_paths = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # 读取图像ID和图像路径的映射\n",
    "        image_path_file = os.path.join(self.root, 'images.txt')\n",
    "        image_paths = {}\n",
    "        with open(image_path_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_id, image_path = line.strip().split()\n",
    "                image_paths[image_id] = image_path\n",
    "        \n",
    "        # 读取图像ID和类别的映射\n",
    "        image_class_file = os.path.join(self.root, 'image_class_labels.txt')\n",
    "        image_classes = {}\n",
    "        with open(image_class_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_id, class_id = line.strip().split()\n",
    "                # 类别ID从1开始，转换为从0开始\n",
    "                image_classes[image_id] = int(class_id) - 1\n",
    "        \n",
    "        # 读取训练/测试分割\n",
    "        split_file = os.path.join(self.root, 'train_test_split.txt')\n",
    "        train_test_split = {}\n",
    "        with open(split_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_id, is_train = line.strip().split()\n",
    "                train_test_split[image_id] = int(is_train)\n",
    "        \n",
    "        # 根据分割情况构建数据集\n",
    "        for image_id in image_paths:\n",
    "            # 1表示训练集，0表示测试集\n",
    "            is_train = train_test_split[image_id] == 1\n",
    "            if (self.split == 'train' and is_train) or (self.split == 'test' and not is_train):\n",
    "                self.image_paths.append(os.path.join(self.root, 'images', image_paths[image_id]))\n",
    "                self.targets.append(image_classes[image_id])\n",
    "        \n",
    "        # 获取类别名称\n",
    "        self.classes = []\n",
    "        with open(os.path.join(self.root, 'classes.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                class_id, class_name = line.strip().split()\n",
    "                self.classes.append(class_name)\n",
    "        \n",
    "        print(f\"CUB200数据集加载完成，{split}集包含 {len(self.image_paths)} 张图像\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.image_paths[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        # 加载图像\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        \n",
    "        # 应用变换\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# 修改命令行参数，添加cub200选项\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='半监督知识蒸馏学习 CMT-Mamba → ResNet')\n",
    "    \n",
    "    # 模型参数\n",
    "    parser.add_argument('--teacher_model', type=str, default='cmt_small', help='教师模型名称')\n",
    "    parser.add_argument('--student_model', type=str, default='resnet18', help='学生模型名称')\n",
    "    parser.add_argument('--use_mamba', action='store_true', default=True, help='教师模型是否使用Mamba模块')\n",
    "    parser.add_argument('--pretrained_teacher', type=str, default='', help='预训练教师模型路径')\n",
    "    \n",
    "    # 数据集参数\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10', choices=['cifar10', 'cifar100', 'cub200'], \n",
    "                        help='数据集名称')\n",
    "    parser.add_argument('--num_classes', type=int, default=10, help='类别数量(cifar10为10，cifar100为100，cub200为200)')\n",
    "    parser.add_argument('--data_dir', type=str, default='', help='数据集路径')\n",
    "    parser.add_argument('--labeled_ratio', type=float, default=0.1, help='有标签数据比例')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='批量大小')\n",
    "    parser.add_argument('--num_workers', type=int, default=2, help='数据加载器工作线程数')\n",
    "    \n",
    "    # ... 其他现有参数 ...\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "# 定义CMT-Mamba知识蒸馏半监督训练流程\n",
    "def train_distill():\n",
    "    # 设置日志记录\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_dir = os.path.join(\"logs\", f\"cmt_mamba_distill_{current_time}\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    log_file = os.path.join(log_dir, \"training.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 记录训练配置\n",
    "    logging.info(f\"启动CMT-Mamba知识蒸馏半监督训练\")\n",
    "    logging.info(f\"配置参数: {vars(args)}\")\n",
    "    \n",
    "    # 加载数据集\n",
    "    if args.dataset == 'cub200':\n",
    "        # 为CUB_200_2011数据集创建数据集加载器\n",
    "        print(f\"加载CUB_200_2011数据集...\")\n",
    "        \n",
    "        # 创建CUB200数据集\n",
    "        train_dataset = CUB200Dataset(\n",
    "            root=args.data_dir,\n",
    "            split='train',\n",
    "            transform=None  # 暂不应用转换，稍后会应用\n",
    "        )\n",
    "        test_dataset = CUB200Dataset(\n",
    "            root=args.data_dir,\n",
    "            split='test',\n",
    "            transform=None\n",
    "        )\n",
    "        \n",
    "        # 将数据集转换为内存中的numpy数组，便于后续处理\n",
    "        dataset_np = []\n",
    "        for idx in range(len(train_dataset)):\n",
    "            img, label = train_dataset[idx]\n",
    "            # 转换为numpy数组\n",
    "            img_np = np.array(img)\n",
    "            dataset_np.append((img_np, label))\n",
    "    else:\n",
    "        # 原有的CIFAR数据集处理逻辑\n",
    "        cifar_dataset = create_dataset(\n",
    "            name=args.dataset,\n",
    "            root=args.data_dir,\n",
    "            split='train',\n",
    "            download=False\n",
    "        )\n",
    "        \n",
    "        # 手动进行有标签/无标签分割\n",
    "        dataset_np = []\n",
    "        print(f\"加载并转换{args.dataset.upper()}数据集...\")\n",
    "        for data in cifar_dataset:\n",
    "            if args.dataset == 'cifar10':\n",
    "                img, label = data\n",
    "            else:  # cifar100\n",
    "                # 解包三个返回值：图像、细粒度标签和粗粒度标签\n",
    "                img, fine_label, _ = data\n",
    "                label = fine_label  # 我们只使用细粒度标签(100个类别)\n",
    "            \n",
    "            dataset_np.append((img.asnumpy(), label.asnumpy().item()))\n",
    "    \n",
    "    print(f\"数据集大小: {len(dataset_np)}\")\n",
    "    rng = np.random.RandomState(42)\n",
    "    rng.shuffle(dataset_np)\n",
    "    \n",
    "    # 按类别分割\n",
    "    class_indices = [[] for _ in range(args.num_classes)]  # 根据数据集类别数创建索引列表\n",
    "    for i, (img, label) in enumerate(dataset_np):\n",
    "        class_indices[label].append(i)\n",
    "    \n",
    "    # 对每个类别进行分层抽样\n",
    "    labeled_indices = []\n",
    "    unlabeled_indices = []\n",
    "    for indices in class_indices:\n",
    "        n_labeled = max(1, int(len(indices) * args.labeled_ratio))\n",
    "        labeled_indices.extend(indices[:n_labeled])\n",
    "        unlabeled_indices.extend(indices[n_labeled:])\n",
    "    \n",
    "    # 创建有标签和无标签数据集\n",
    "    labeled_data = [(dataset_np[i][0], dataset_np[i][1]) for i in labeled_indices]\n",
    "    unlabeled_data = [(dataset_np[i][0], dataset_np[i][1]) for i in unlabeled_indices]\n",
    "    \n",
    "    print(f\"有标签数据量: {len(labeled_data)}, 无标签数据量: {len(unlabeled_data)}\")\n",
    "    \n",
    "    # 创建数据转换\n",
    "    weak_transform = create_weak_augmentation()\n",
    "    strong_transform = create_strong_augmentation()\n",
    "    \n",
    "    # 创建有标签数据加载器\n",
    "    def labeled_generator():\n",
    "        indices = list(range(len(labeled_data)))\n",
    "        while True:\n",
    "            rng.shuffle(indices)\n",
    "            for idx in indices:\n",
    "                yield labeled_data[idx]\n",
    "    \n",
    "    labeled_ds = ds.GeneratorDataset(\n",
    "        source=labeled_generator(),\n",
    "        column_names=[\"image\", \"label\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    labeled_ds = labeled_ds.map(operations=weak_transform, input_columns=[\"image\"])\n",
    "    labeled_ds = labeled_ds.batch(args.batch_size)\n",
    "    \n",
    "    # 创建无标签数据加载器\n",
    "    def unlabeled_generator():\n",
    "        indices = list(range(len(unlabeled_data)))\n",
    "        while True:\n",
    "            rng.shuffle(indices)\n",
    "            for idx in indices:\n",
    "                img, _ = unlabeled_data[idx]\n",
    "                yield img, img  # 弱增强、强增强的原始图像\n",
    "    \n",
    "    unlabeled_ds = ds.GeneratorDataset(\n",
    "        source=unlabeled_generator(),\n",
    "        column_names=[\"weak_image\", \"strong_image\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    unlabeled_ds = unlabeled_ds.map(operations=weak_transform, input_columns=[\"weak_image\"])\n",
    "    unlabeled_ds = unlabeled_ds.map(operations=strong_transform, input_columns=[\"strong_image\"])\n",
    "    unlabeled_ds = unlabeled_ds.batch(args.batch_size)\n",
    "    \n",
    "    # 测试集预处理\n",
    "    # 根据数据集选择合适的归一化参数\n",
    "    if args.dataset == 'cifar10':\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2023, 0.1994, 0.2010]\n",
    "    else:  # cifar100\n",
    "        mean = [0.5071, 0.4867, 0.4408]\n",
    "        std = [0.2675, 0.2565, 0.2761]\n",
    "        \n",
    "    test_transform = [\n",
    "        vision.Resize(224),  # 确保测试图像也调整到正确尺寸\n",
    "        vision.Normalize(mean=mean, std=std),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    \n",
    "    # 创建测试数据集\n",
    "    if args.dataset == 'cub200':\n",
    "        # 已经在前面创建了test_dataset，现在应用转换\n",
    "        def test_generator():\n",
    "            for idx in range(len(test_dataset)):\n",
    "                img, label = test_dataset[idx]\n",
    "                img_np = np.array(img)\n",
    "                \n",
    "                # 应用测试转换\n",
    "                for op in test_transform:\n",
    "                    img_np = op(img_np)\n",
    "                \n",
    "                yield img_np, label\n",
    "        \n",
    "        test_ds = ds.GeneratorDataset(\n",
    "            source=test_generator(),\n",
    "            column_names=[\"image\", \"label\"],\n",
    "            shuffle=False\n",
    "        )\n",
    "        test_ds = test_ds.batch(args.batch_size)\n",
    "    else:\n",
    "        # 原有的CIFAR测试集处理逻辑\n",
    "        test_ds = create_dataset(\n",
    "            name=args.dataset,\n",
    "            root=args.data_dir,\n",
    "            split='test',\n",
    "            download=False\n",
    "        )\n",
    "        \n",
    "        # 对测试集应用转换\n",
    "        test_ds = test_ds.map(operations=test_transform, input_columns=[\"image\"])\n",
    "        test_ds = test_ds.batch(args.batch_size)\n",
    "    \n",
    "    # 创建模型\n",
    "    print(\"创建教师模型(CMT-Small)和学生模型(ResNet18)...\")\n",
    "\n",
    "    # 创建教师模型 - 使用CMT-Small with Mamba\n",
    "    teacher_model = create_model(\n",
    "        args.teacher_model,\n",
    "        num_classes=args.num_classes,\n",
    "        in_channels=3,\n",
    "        use_mamba=args.use_mamba,\n",
    "    )\n",
    "\n",
    "    # 创建学生模型 - 使用ResNet18\n",
    "    student_model = create_model(\n",
    "        args.student_model,\n",
    "        num_classes=args.num_classes,\n",
    "        in_channels=3,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # 检查是否存在预训练的教师模型\n",
    "    if not args.pretrained_teacher:\n",
    "        logging.info(\"未找到预训练教师模型，开始使用有标签数据训练教师模型...\")\n",
    "        \n",
    "        # 创建教师模型优化器\n",
    "        teacher_optimizer = nn.Momentum(\n",
    "            params=teacher_model.trainable_params(),\n",
    "            learning_rate=args.lr,\n",
    "            momentum=args.momentum,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "        \n",
    "        # 仅使用有标签数据训练教师模型\n",
    "        train_teacher(teacher_model, teacher_optimizer, labeled_ds, test_ds, epochs=100, labeled_size=len(labeled_data))\n",
    "        \n",
    "        # 保存训练好的教师模型\n",
    "        ms.save_checkpoint(teacher_model, \"teacher_pretrained.ckpt\")\n",
    "        logging.info(\"教师模型预训练完成，已保存检查点\")\n",
    "    \n",
    "    # 创建优化器\n",
    "    lr = nn.cosine_decay_lr(\n",
    "        min_lr=args.lr * 0.01,\n",
    "        max_lr=args.lr,\n",
    "        total_step=args.epochs * (len(labeled_data) // args.batch_size),\n",
    "        step_per_epoch=len(labeled_data) // args.batch_size,\n",
    "        decay_epoch=args.epochs\n",
    "    )\n",
    "    \n",
    "    optimizer = nn.Momentum(\n",
    "        params=student_model.trainable_params(),\n",
    "        learning_rate=lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    \n",
    "    # 创建EMA更新器\n",
    "    teacher_ema = EMA(student_model, teacher_model, decay=args.ema_decay)\n",
    "    \n",
    "    # 修改DistillationTrainStep类\n",
    "    class DistillationTrainStep(nn.Cell):\n",
    "        def __init__(self, student_model, teacher_model, optimizer, args):\n",
    "            super(DistillationTrainStep, self).__init__()\n",
    "            self.student_model = student_model\n",
    "            self.teacher_model = teacher_model\n",
    "            self.optimizer = optimizer\n",
    "            self.args = args\n",
    "            # 显式指定网络参数，确保梯度计算与优化器参数匹配\n",
    "            self.weights = self.student_model.trainable_params()\n",
    "            self.grad_fn = ops.value_and_grad(self.forward, None, self.weights, has_aux=False)\n",
    "        \n",
    "        def forward(self, labeled_imgs, labels, unlabeled_weak, unlabeled_strong):\n",
    "            # 教师模型推理\n",
    "            teacher_labeled_logits = self.teacher_model(labeled_imgs)\n",
    "            teacher_unlabeled_logits = self.teacher_model(unlabeled_weak)\n",
    "            \n",
    "            # 学生模型推理\n",
    "            student_labeled_logits = self.student_model(labeled_imgs)\n",
    "            student_unlabeled_strong_logits = self.student_model(unlabeled_strong)\n",
    "            \n",
    "            # 计算蒸馏损失\n",
    "            distill_loss = self._distill_loss_fn(student_labeled_logits, teacher_labeled_logits, labels)\n",
    "            \n",
    "            # 计算半监督一致性损失\n",
    "            # 生成伪标签\n",
    "            pseudo_labels = ops.softmax(teacher_unlabeled_logits, axis=1)\n",
    "            max_probs, targets = ops.max(pseudo_labels, axis=1)\n",
    "            mask = (max_probs > 0.95).astype(ms.float32)  # 高置信度伪标签的掩码\n",
    "            \n",
    "            # 计算一致性损失\n",
    "            consistency_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='none')(\n",
    "                student_unlabeled_strong_logits, \n",
    "                targets\n",
    "            )\n",
    "            consistency_loss = (consistency_loss * mask).mean()\n",
    "            \n",
    "            # 计算总损失\n",
    "            total_loss = distill_loss + self.args.beta * consistency_loss\n",
    "            \n",
    "            return total_loss\n",
    "        \n",
    "        def _distill_loss_fn(self, student_logits, teacher_logits, labels):\n",
    "            # 监督损失\n",
    "            ce_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "            hard_loss = ce_loss(student_logits, labels)\n",
    "            \n",
    "            # 蒸馏软标签损失\n",
    "            T = self.args.T\n",
    "            soft_student = ops.softmax(student_logits / T, axis=1)\n",
    "            soft_teacher = ops.softmax(teacher_logits / T, axis=1)\n",
    "            \n",
    "            kl_div = ops.kl_div(\n",
    "                ops.log(soft_student + 1e-10),\n",
    "                soft_teacher,\n",
    "                reduction='batchmean'\n",
    "            ) * (T * T)\n",
    "            \n",
    "            # 组合损失\n",
    "            return (1 - self.args.alpha) * hard_loss + self.args.alpha * kl_div\n",
    "        \n",
    "        def construct(self, labeled_imgs, labels, unlabeled_weak, unlabeled_strong):\n",
    "            loss, grads = self.grad_fn(labeled_imgs, labels, unlabeled_weak, unlabeled_strong)\n",
    "            self.optimizer(grads)\n",
    "            return loss\n",
    "    \n",
    "    # 创建训练网络\n",
    "    net = DistillationTrainStep(student_model, teacher_model, optimizer, args)\n",
    "    \n",
    "    # 定义评估函数\n",
    "    def evaluate():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.set_train(False)\n",
    "        for data in test_ds.create_tuple_iterator():\n",
    "            if args.dataset == 'cifar10':\n",
    "                images, labels = data\n",
    "            else:  # cifar100\n",
    "                images, fine_labels, _ = data  # 解包三个返回值\n",
    "                labels = fine_labels\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = ops.max(outputs, axis=1)\n",
    "            predicted = predicted.astype(ms.int32)\n",
    "            labels = labels.astype(ms.int32)\n",
    "            total += labels.shape[0]\n",
    "            correct += (predicted == labels).astype(ms.float32).sum().asnumpy().item()\n",
    "        return correct / total\n",
    "    \n",
    "    # 创建模型保存目录\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    \n",
    "    # 开始训练\n",
    "    print(\"开始知识蒸馏半监督训练...\")\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    labeled_iter = labeled_ds.create_dict_iterator()\n",
    "    unlabeled_iter = unlabeled_ds.create_dict_iterator()\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        # 设置训练模式\n",
    "        student_model.set_train(True)\n",
    "        teacher_model.set_train(False)  # 教师模型始终处于评估模式\n",
    "        \n",
    "        # 训练一个epoch\n",
    "        total_loss = 0.0\n",
    "        steps = min(len(labeled_data) // args.batch_size, len(unlabeled_data) // args.batch_size)\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch+1}/{args.epochs} 开始训练...\")\n",
    "        epoch_progress = tqdm(range(steps), desc=f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        \n",
    "        for step in epoch_progress:\n",
    "            try:\n",
    "                labeled_batch = next(labeled_iter)\n",
    "            except StopIteration:\n",
    "                labeled_iter = labeled_ds.create_dict_iterator()\n",
    "                labeled_batch = next(labeled_iter)\n",
    "            \n",
    "            try:\n",
    "                unlabeled_batch = next(unlabeled_iter)\n",
    "            except StopIteration:\n",
    "                unlabeled_iter = unlabeled_ds.create_dict_iterator()\n",
    "                unlabeled_batch = next(unlabeled_iter)\n",
    "            \n",
    "            labeled_imgs = labeled_batch[\"image\"]\n",
    "            labels = labeled_batch[\"label\"]\n",
    "            weak_imgs = unlabeled_batch[\"weak_image\"]\n",
    "            strong_imgs = unlabeled_batch[\"strong_image\"]\n",
    "            \n",
    "            loss = net(labeled_imgs, labels, weak_imgs, strong_imgs)\n",
    "            total_loss += loss.asnumpy().item()\n",
    "            \n",
    "            # 更新进度条\n",
    "            epoch_progress.set_postfix(loss=f\"{loss.asnumpy().item():.4f}\")\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                logging.info(f\"Epoch: {epoch+1}/{args.epochs}, Step: {step+1}/{steps}, Loss: {loss.asnumpy().item():.4f}\")\n",
    "        \n",
    "        # 评估模型\n",
    "        avg_loss = total_loss / steps\n",
    "        logging.info(f\"Epoch: {epoch+1}/{args.epochs}, Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 使用学生模型评估\n",
    "        top1_acc = evaluate()\n",
    "        logging.info(f\"Epoch: {epoch+1}/{args.epochs}, TOP1 Accuracy: {top1_acc:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型 (基于TOP1准确率)\n",
    "        if top1_acc > best_acc:\n",
    "            best_acc = top1_acc\n",
    "            ms.save_checkpoint(student_model, os.path.join(args.save_dir, f\"{args.student_model}_best.ckpt\"))\n",
    "            logging.info(f\"已保存最佳模型，TOP1准确率: {best_acc:.4f}\")\n",
    "        \n",
    "        # 每隔一定轮数保存检查点\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            ms.save_checkpoint(student_model, os.path.join(args.save_dir, f\"{args.student_model}_epoch{epoch+1}.ckpt\"))\n",
    "            logging.info(f\"已保存第{epoch+1}轮检查点\")\n",
    "    \n",
    "    print(f\"训练完成！最佳TOP1准确率: {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "def train_teacher(model, optimizer, train_ds, test_ds, epochs, labeled_size):\n",
    "    \"\"\"训练教师模型的函数\"\"\"\n",
    "    # 创建动态学习率\n",
    "    total_steps = epochs * (labeled_size // args.batch_size)\n",
    "    lr = nn.cosine_decay_lr(\n",
    "        min_lr=0.0,\n",
    "        max_lr=args.lr,\n",
    "        total_step=total_steps,\n",
    "        step_per_epoch=labeled_size // args.batch_size,\n",
    "        decay_epoch=epochs\n",
    "    )\n",
    "    dynamic_lr = ms.Parameter(ms.Tensor(lr, ms.float32))\n",
    "    \n",
    "    # 创建新的优化器，使用动态学习率\n",
    "    optimizer = nn.Momentum(\n",
    "        params=model.trainable_params(),\n",
    "        learning_rate=dynamic_lr,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    \n",
    "    # 定义训练步骤\n",
    "    class TeacherTrainStep(nn.Cell):\n",
    "        def __init__(self, network, optimizer):\n",
    "            super(TeacherTrainStep, self).__init__()\n",
    "            self.network = network\n",
    "            self.optimizer = optimizer\n",
    "            self.grad_fn = ops.value_and_grad(self.forward, None, self.network.trainable_params())\n",
    "            self.loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "        \n",
    "        def forward(self, images, labels):\n",
    "            logits = self.network(images)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return loss\n",
    "        \n",
    "        def construct(self, images, labels):\n",
    "            loss, grads = self.grad_fn(images, labels)\n",
    "            self.optimizer(grads)\n",
    "            return loss\n",
    "    \n",
    "    # 创建训练网络\n",
    "    train_step = TeacherTrainStep(model, optimizer)\n",
    "    \n",
    "    # 定义评估函数\n",
    "    def evaluate():\n",
    "        \"\"\"在训练教师模型时使用的独立评估函数\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.set_train(False)\n",
    "        \n",
    "        # 避免使用process_test_batch，直接处理测试数据\n",
    "        for data in test_ds.create_tuple_iterator():\n",
    "            # 根据数据集类型获取图像和标签\n",
    "            if args.dataset == 'cifar10':\n",
    "                images, labels = data\n",
    "            else:  # cifar100\n",
    "                images, fine_labels, _ = data  # 解包三个返回值\n",
    "                labels = fine_labels\n",
    "                \n",
    "            # 直接使用张量数据进行预测\n",
    "            outputs = model(images)\n",
    "            _, predicted = ops.max(outputs, axis=1)\n",
    "            predicted = predicted.astype(ms.int32)\n",
    "            labels = labels.astype(ms.int32)\n",
    "            total += labels.shape[0]\n",
    "            correct += (predicted == labels).astype(ms.float32).sum().asnumpy().item()\n",
    "            \n",
    "        return correct / total\n",
    "    \n",
    "    logging.info(\"开始训练教师模型...\")\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(epochs):\n",
    "        model.set_train(True)\n",
    "        total_loss = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        # 计算每个epoch的总步数\n",
    "        total_steps = labeled_size // args.batch_size\n",
    "        \n",
    "        # 使用tqdm显示进度\n",
    "        train_iter = tqdm(range(total_steps), desc=f\"Teacher Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        data_iter = train_ds.create_dict_iterator()\n",
    "        for _ in train_iter:\n",
    "            try:\n",
    "                data = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = train_ds.create_dict_iterator()\n",
    "                data = next(data_iter)\n",
    "            \n",
    "            images, labels = data[\"image\"], data[\"label\"]\n",
    "            loss = train_step(images, labels)\n",
    "            total_loss += loss.asnumpy().item()\n",
    "            steps += 1\n",
    "            \n",
    "            # 更新进度条\n",
    "            train_iter.set_postfix(loss=f\"{loss.asnumpy().item():.4f}\")\n",
    "            \n",
    "            # 每个epoch训练固定步数\n",
    "            if steps >= total_steps:\n",
    "                break\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / steps\n",
    "        \n",
    "        # 评估模型\n",
    "        acc = evaluate()\n",
    "        \n",
    "        # 记录训练信息\n",
    "        logging.info(f\"Teacher Epoch: {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            ms.save_checkpoint(model, \"teacher_best.ckpt\")\n",
    "            logging.info(f\"保存最佳教师模型，准确率: {acc:.4f}\")\n",
    "    \n",
    "    # 训练结束后加载最佳模型\n",
    "    param_dict = ms.load_checkpoint(\"teacher_best.ckpt\")\n",
    "    ms.load_param_into_net(model, param_dict)\n",
    "    logging.info(f\"教师模型训练完成，最佳准确率: {best_acc:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_distill() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
