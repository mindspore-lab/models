{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd0232-1726-49eb-9da3-4d5971f06984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#静态：model==0；\n",
    "\"\"\"\n",
    "MindSpore implementation of `ResNet` with Mamba integration.\n",
    "Refer to Deep Residual Learning for Image Recognition.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Optional, Type, Union\n",
    "\n",
    "import mindspore.common.initializer as init\n",
    "from mindspore import Tensor, nn, ops\n",
    "\n",
    "from .helpers import build_model_with_cfg\n",
    "from .layers.pooling import GlobalAvgPooling\n",
    "from .registry import register_model\n",
    "\n",
    "__all__ = [\n",
    "    \"ResNet\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"resnet152\",\n",
    "    \"resnext50_32x4d\",\n",
    "    \"resnext101_32x4d\",\n",
    "    \"resnext101_64x4d\",\n",
    "    \"resnext152_64x4d\",\n",
    "]\n",
    "\n",
    "\n",
    "def _cfg(url=\"\", **kwargs):\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"num_classes\": 1000,\n",
    "        \"first_conv\": \"conv1\",\n",
    "        \"classifier\": \"classifier\",\n",
    "        **kwargs,\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    \"resnet18\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet18-1e65cd21.ckpt\"),\n",
    "    \"resnet34\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet34-f297d27e.ckpt\"),\n",
    "    \"resnet50\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet50-e0733ab8.ckpt\"),\n",
    "    \"resnet101\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet101-689c5e77.ckpt\"),\n",
    "    \"resnet152\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet152-beb689d8.ckpt\"),\n",
    "    \"resnext50_32x4d\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext50_32x4d-af8aba16.ckpt\"),\n",
    "    \"resnext101_32x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext101_32x4d-3c1e9c51.ckpt\"\n",
    "    ),\n",
    "    \"resnext101_64x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext101_64x4d-8929255b.ckpt\"\n",
    "    ),\n",
    "    \"resnext152_64x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext152_64x4d-3aba275c.ckpt\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Cell):\n",
    "    \"\"\"define the basic block of resnet\"\"\"\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "        assert groups == 1, \"BasicBlock only supports groups=1\"\n",
    "        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, pad_mode=\"pad\")\n",
    "        self.bn1 = norm(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3,\n",
    "                               stride=1, padding=1, pad_mode=\"pad\")\n",
    "        self.bn2 = norm(channels)\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Cell):\n",
    "    \"\"\"集成Mamba的Bottleneck\"\"\"\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = False,  # 新增参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "\n",
    "        width = int(channels * (base_width / 64.0)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n",
    "        self.bn1 = norm(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, pad_mode=\"pad\", group=groups)\n",
    "        self.bn2 = norm(width)\n",
    "        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n",
    "                               kernel_size=1, stride=1)\n",
    "        self.bn3 = norm(channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # 选择性添加Mamba\n",
    "        self.use_mamba = use_mamba\n",
    "        if use_mamba:\n",
    "            self.mamba = MambaBlock(dim=channels * self.expansion)\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # 在BN3之后，残差连接之前应用Mamba\n",
    "        # 这是Mamba能发挥最大价值的位置\n",
    "        if self.use_mamba:\n",
    "            out = self.mamba(out)\n",
    "        \n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Cell):\n",
    "    \"\"\"精简高效的Mamba块：专注于空间关系建模和训练稳定性\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # 内部维度设计\n",
    "        self.d_inner = max(dim // 2, 64)\n",
    "        \n",
    "        # 输入投影\n",
    "        self.in_proj = nn.Conv2d(dim, self.d_inner, kernel_size=1, \n",
    "                                 weight_init=init.HeNormal())\n",
    "        self.in_norm = nn.BatchNorm2d(self.d_inner, gamma_init='zeros')\n",
    "        \n",
    "        # 空间处理 - 使用较大的卷积核\n",
    "        self.spatial = nn.Conv2d(self.d_inner, self.d_inner, kernel_size=5,\n",
    "                                stride=1, padding=2, pad_mode=\"pad\")\n",
    "        self.spatial_norm = nn.BatchNorm2d(self.d_inner)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Conv2d(self.d_inner, dim, kernel_size=1)\n",
    "        self.out_norm = nn.BatchNorm2d(dim, gamma_init='zeros')\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.scale = 0.1  # 固定的中等强度影响\n",
    "    \n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.in_proj(x)\n",
    "        out = self.in_norm(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.spatial(out)\n",
    "        out = self.spatial_norm(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_norm(out)\n",
    "        \n",
    "        # 使用固定缩放系数\n",
    "        out = identity + self.scale * out\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaBottleneck(nn.Cell):\n",
    "    \"\"\"自适应Mamba Bottleneck：根据网络位置动态调整Mamba影响\"\"\"\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = True,\n",
    "        layer_index: int = 0,  # 添加层索引参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "\n",
    "        width = int(channels * (base_width / 64.0)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n",
    "        self.bn1 = norm(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, pad_mode=\"pad\", group=groups)\n",
    "        self.bn2 = norm(width)\n",
    "        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n",
    "                               kernel_size=1, stride=1)\n",
    "        self.bn3 = norm(channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # 根据层深度选择性添加Mamba\n",
    "        # 浅层用更保守的配置，深层更激进\n",
    "        self.use_mamba = use_mamba\n",
    "        if use_mamba:\n",
    "            self.mamba = MambaBlock(dim=channels * self.expansion)\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # 应用Mamba处理\n",
    "        if self.use_mamba:\n",
    "            out = self.mamba(out)\n",
    "        \n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Cell):\n",
    "    \"\"\"优化的ResNet-Mamba集成架构\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        in_channels: int = 3,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = True,\n",
    "        cifar_mode: bool = False,  # 添加CIFAR模式参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "        \n",
    "        # 关键设计: 只在深层网络使用Mamba，那里更需要长距离关系建模\n",
    "        self.use_mamba_in_layer = [False, False, True, True] if use_mamba else [False, False, False, False]\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.groups = groups\n",
    "        self.base_width = base_width\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # CIFAR模式使用更小的卷积核和去除最大池化\n",
    "        if cifar_mode:\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, kernel_size=3, stride=1, \n",
    "                                  padding=1, pad_mode=\"pad\")\n",
    "            self.max_pool = nn.Identity()  # 不进行池化\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, kernel_size=7, stride=2, \n",
    "                                  padding=3, pad_mode=\"pad\")\n",
    "            self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "            \n",
    "        self.bn1 = norm(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 构建网络层\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], use_mamba=self.use_mamba_in_layer[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, use_mamba=self.use_mamba_in_layer[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, use_mamba=self.use_mamba_in_layer[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, use_mamba=self.use_mamba_in_layer[3])\n",
    "        \n",
    "        self.pool = GlobalAvgPooling()\n",
    "        self.num_features = 512 * block.expansion\n",
    "        self.classifier = nn.Dense(self.num_features, num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Initialize weights for cells.\"\"\"\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, nn.Dense):\n",
    "                cell.weight.set_data(\n",
    "                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n",
    "                )\n",
    "                if cell.bias is not None:\n",
    "                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape, cell.bias.dtype))\n",
    "            elif isinstance(cell, nn.BatchNorm2d):\n",
    "                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape, cell.gamma.dtype))\n",
    "                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape, cell.beta.dtype))\n",
    "            elif isinstance(cell, nn.Conv2d):\n",
    "                cell.weight.set_data(\n",
    "                    init.initializer(init.HeNormal(), cell.weight.shape, cell.weight.dtype)\n",
    "                )\n",
    "                if cell.bias is not None:\n",
    "                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape, cell.bias.dtype))\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        channels: int,\n",
    "        blocks_num: int,\n",
    "        stride: int = 1,\n",
    "        use_mamba: bool = False,\n",
    "    ) -> nn.SequentialCell:\n",
    "        layers = []\n",
    "        # 第一个block通常不使用Mamba，因为它处理下采样\n",
    "        down_sample = None\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            down_sample = nn.SequentialCell([\n",
    "                nn.Conv2d(self.in_channels, channels * block.expansion, kernel_size=1, stride=stride),\n",
    "                self.norm(channels * block.expansion)\n",
    "            ])\n",
    "        \n",
    "        # 第一个block，不使用Mamba\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels,\n",
    "                channels,\n",
    "                stride=stride,\n",
    "                down_sample=down_sample,\n",
    "                groups=self.groups,\n",
    "                base_width=self.base_width,\n",
    "                norm=self.norm,\n",
    "                use_mamba=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.in_channels = channels * block.expansion\n",
    "        \n",
    "        # 对剩余blocks，根据密度决定是否使用Mamba\n",
    "        for i in range(1, blocks_num):\n",
    " \n",
    "            use_mamba_here = use_mamba and i >= 1\n",
    "            \n",
    "            if use_mamba_here:\n",
    "\n",
    "                layers.append(\n",
    "                    block(\n",
    "                        self.in_channels,\n",
    "                        channels,\n",
    "                        groups=self.groups,\n",
    "                        base_width=self.base_width,\n",
    "                        norm=self.norm,\n",
    "                        use_mamba=True\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers.append(\n",
    "                    block(\n",
    "                        self.in_channels,\n",
    "                        channels,\n",
    "                        groups=self.groups,\n",
    "                        base_width=self.base_width,\n",
    "                        norm=self.norm,\n",
    "                        use_mamba=False\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        return nn.SequentialCell(layers)\n",
    "\n",
    "    def forward_features(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Network forward feature extraction.\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x: Tensor) -> Tensor:\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _create_resnet(pretrained=False, **kwargs):\n",
    "    return build_model_with_cfg(ResNet, pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 18 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet18\"]\n",
    "    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 34 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet34\"]\n",
    "    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet50(\n",
    "    pretrained: bool = False,\n",
    "    num_classes: int = 1000,\n",
    "    in_channels: int = 3,\n",
    "    cifar_mode: bool = False,  # 添加CIFAR模式参数\n",
    "    use_mamba: bool = True,    # Mamba开关\n",
    "    **kwargs\n",
    ") -> ResNet:\n",
    "    \"\"\"Get 50 layers ResNet model.\n",
    "    \n",
    "    Args:\n",
    "        pretrained: Whether to download and load the pre-trained model. Default: False.\n",
    "        num_classes: The number of classification. Default: 1000.\n",
    "        in_channels: The input channels. Default: 3.\n",
    "        cifar_mode: Whether to use CIFAR optimized architecture. Default: False.\n",
    "        use_mamba: Whether to use Mamba blocks. Default: True.\n",
    "        \n",
    "    Returns:\n",
    "        ResNet network.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet50\"]\n",
    "    model_args = dict(\n",
    "        block=Bottleneck,\n",
    "        layers=[3, 4, 6, 3],\n",
    "        num_classes=num_classes,\n",
    "        in_channels=in_channels,\n",
    "        cifar_mode=cifar_mode,\n",
    "        use_mamba=use_mamba,\n",
    "        **kwargs\n",
    "    )\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet101\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet152(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 152 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet152\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 50 layers ResNeXt model with 32 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext50_32x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], groups=32, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext101_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNeXt model with 32 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext101_32x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=32, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext101_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNeXt model with 64 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext101_64x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=64, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext152_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    default_cfg = default_cfgs[\"resnext152_64x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], groups=64, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2bb62-f290-41fb-9293-e8ce7cc5b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#动态：mode==1\n",
    "\"\"\"\n",
    "MindSpore implementation of `ResNet` with Mamba integration.\n",
    "Refer to Deep Residual Learning for Image Recognition.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Optional, Type, Union\n",
    "\n",
    "import mindspore.common.initializer as init\n",
    "from mindspore import Tensor, nn, ops\n",
    "\n",
    "from .helpers import build_model_with_cfg\n",
    "from .layers.pooling import GlobalAvgPooling\n",
    "from .registry import register_model\n",
    "\n",
    "__all__ = [\n",
    "    \"ResNet\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"resnet152\",\n",
    "    \"resnext50_32x4d\",\n",
    "    \"resnext101_32x4d\",\n",
    "    \"resnext101_64x4d\",\n",
    "    \"resnext152_64x4d\",\n",
    "]\n",
    "\n",
    "\n",
    "def _cfg(url=\"\", **kwargs):\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"num_classes\": 1000,\n",
    "        \"first_conv\": \"conv1\",\n",
    "        \"classifier\": \"classifier\",\n",
    "        **kwargs,\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    \"resnet18\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet18-1e65cd21.ckpt\"),\n",
    "    \"resnet34\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet34-f297d27e.ckpt\"),\n",
    "    \"resnet50\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet50-e0733ab8.ckpt\"),\n",
    "    \"resnet101\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet101-689c5e77.ckpt\"),\n",
    "    \"resnet152\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet152-beb689d8.ckpt\"),\n",
    "    \"resnext50_32x4d\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext50_32x4d-af8aba16.ckpt\"),\n",
    "    \"resnext101_32x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext101_32x4d-3c1e9c51.ckpt\"\n",
    "    ),\n",
    "    \"resnext101_64x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext101_64x4d-8929255b.ckpt\"\n",
    "    ),\n",
    "    \"resnext152_64x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext152_64x4d-3aba275c.ckpt\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Cell):\n",
    "    \"\"\"define the basic block of resnet\"\"\"\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "        assert groups == 1, \"BasicBlock only supports groups=1\"\n",
    "        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, pad_mode=\"pad\")\n",
    "        self.bn1 = norm(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3,\n",
    "                               stride=1, padding=1, pad_mode=\"pad\")\n",
    "        self.bn2 = norm(channels)\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Cell):\n",
    "    \"\"\"集成Mamba的Bottleneck\"\"\"\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = False,  # 新增参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "\n",
    "        width = int(channels * (base_width / 64.0)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n",
    "        self.bn1 = norm(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, pad_mode=\"pad\", group=groups)\n",
    "        self.bn2 = norm(width)\n",
    "        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n",
    "                               kernel_size=1, stride=1)\n",
    "        self.bn3 = norm(channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # 选择性添加Mamba\n",
    "        self.use_mamba = use_mamba\n",
    "        if use_mamba:\n",
    "            self.mamba = MambaBlock(dim=channels * self.expansion)\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # 在BN3之后，残差连接之前应用Mamba\n",
    "        # 这是Mamba能发挥最大价值的位置\n",
    "        if self.use_mamba:\n",
    "            out = self.mamba(out)\n",
    "        \n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Cell):\n",
    "    \"\"\"精简高效的Mamba块：专注于空间关系建模和训练稳定性\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # CIFAR数据集尺寸较小，我们可以使用更大的内部维度\n",
    "        self.d_inner = max(dim // 2, 64)  # 增大内部维度以捕获更多信息\n",
    "        \n",
    "        # 输入投影\n",
    "        self.in_proj = nn.Conv2d(dim, self.d_inner, kernel_size=1, \n",
    "                                 weight_init=init.HeNormal())\n",
    "        self.in_norm = nn.BatchNorm2d(self.d_inner, gamma_init='zeros')\n",
    "        \n",
    "        # 空间处理 - 使用更大的卷积核以捕获更广泛的空间关系\n",
    "        self.spatial = nn.Conv2d(self.d_inner, self.d_inner, kernel_size=5,  # 增大卷积核\n",
    "                                stride=1, padding=2, pad_mode=\"pad\")  # 调整padding\n",
    "        self.spatial_norm = nn.BatchNorm2d(self.d_inner)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Conv2d(self.d_inner, dim, kernel_size=1)\n",
    "        self.out_norm = nn.BatchNorm2d(dim, gamma_init='zeros')\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 保持自适应影响机制\n",
    "        self.epoch_counter = 0\n",
    "        self.warmup_epochs = 5\n",
    "        self.base_scale = 0.3\n",
    "    \n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        # 渐进式影响控制\n",
    "        if self.training:\n",
    "            self.epoch_counter = min(self.epoch_counter + 1, self.warmup_epochs)\n",
    "        \n",
    "        # 计算自适应缩放系数\n",
    "        current_scale = self.base_scale * min(1.0, self.epoch_counter / self.warmup_epochs)\n",
    "        \n",
    "        # 简化处理路径\n",
    "        out = self.in_proj(x)\n",
    "        out = self.in_norm(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # 关键的空间处理\n",
    "        out = self.spatial(out)\n",
    "        out = self.spatial_norm(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_norm(out)\n",
    "        \n",
    "        # 自适应残差连接\n",
    "        out = identity + current_scale * out\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaBottleneck(nn.Cell):\n",
    "    \"\"\"自适应Mamba Bottleneck：根据网络位置动态调整Mamba影响\"\"\"\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = True,\n",
    "        layer_index: int = 0,  # 添加层索引参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "\n",
    "        width = int(channels * (base_width / 64.0)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n",
    "        self.bn1 = norm(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, pad_mode=\"pad\", group=groups)\n",
    "        self.bn2 = norm(width)\n",
    "        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n",
    "                               kernel_size=1, stride=1)\n",
    "        self.bn3 = norm(channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # 根据层深度选择性添加Mamba\n",
    "        # 浅层用更保守的配置，深层更激进\n",
    "        self.use_mamba = use_mamba\n",
    "        if use_mamba:\n",
    "            self.mamba = MambaBlock(dim=channels * self.expansion)\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # 应用Mamba处理\n",
    "        if self.use_mamba:\n",
    "            out = self.mamba(out)\n",
    "        \n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Cell):\n",
    "    \"\"\"优化的ResNet-Mamba集成架构\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        in_channels: int = 3,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = True,\n",
    "        cifar_mode: bool = False,  # 添加CIFAR模式参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "        \n",
    "        # 关键设计: 只在深层网络使用Mamba，那里更需要长距离关系建模\n",
    "        self.use_mamba_in_layer = [False, False, True, True] if use_mamba else [False, False, False, False]\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.groups = groups\n",
    "        self.base_width = base_width\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # CIFAR模式使用更小的卷积核和去除最大池化\n",
    "        if cifar_mode:\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, kernel_size=3, stride=1, \n",
    "                                  padding=1, pad_mode=\"pad\")\n",
    "            self.max_pool = nn.Identity()  # 不进行池化\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, kernel_size=7, stride=2, \n",
    "                                  padding=3, pad_mode=\"pad\")\n",
    "            self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "            \n",
    "        self.bn1 = norm(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 构建网络层\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], use_mamba=self.use_mamba_in_layer[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, use_mamba=self.use_mamba_in_layer[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, use_mamba=self.use_mamba_in_layer[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, use_mamba=self.use_mamba_in_layer[3])\n",
    "        \n",
    "        self.pool = GlobalAvgPooling()\n",
    "        self.num_features = 512 * block.expansion\n",
    "        self.classifier = nn.Dense(self.num_features, num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Initialize weights for cells.\"\"\"\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, nn.Dense):\n",
    "                cell.weight.set_data(\n",
    "                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n",
    "                )\n",
    "                if cell.bias is not None:\n",
    "                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape, cell.bias.dtype))\n",
    "            elif isinstance(cell, nn.BatchNorm2d):\n",
    "                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape, cell.gamma.dtype))\n",
    "                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape, cell.beta.dtype))\n",
    "            elif isinstance(cell, nn.Conv2d):\n",
    "                cell.weight.set_data(\n",
    "                    init.initializer(init.HeNormal(), cell.weight.shape, cell.weight.dtype)\n",
    "                )\n",
    "                if cell.bias is not None:\n",
    "                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape, cell.bias.dtype))\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        channels: int,\n",
    "        blocks_num: int,\n",
    "        stride: int = 1,\n",
    "        use_mamba: bool = False,\n",
    "    ) -> nn.SequentialCell:\n",
    "        layers = []\n",
    "        # 第一个block通常不使用Mamba，因为它处理下采样\n",
    "        down_sample = None\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            down_sample = nn.SequentialCell([\n",
    "                nn.Conv2d(self.in_channels, channels * block.expansion, kernel_size=1, stride=stride),\n",
    "                self.norm(channels * block.expansion)\n",
    "            ])\n",
    "        \n",
    "        # 第一个block，不使用Mamba\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels,\n",
    "                channels,\n",
    "                stride=stride,\n",
    "                down_sample=down_sample,\n",
    "                groups=self.groups,\n",
    "                base_width=self.base_width,\n",
    "                norm=self.norm,\n",
    "                use_mamba=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.in_channels = channels * block.expansion\n",
    "        \n",
    "        # 对剩余blocks，根据密度决定是否使用Mamba\n",
    "        for i in range(1, blocks_num):\n",
    "            use_mamba_here = use_mamba and i >= 1\n",
    "            \n",
    "            if use_mamba_here:\n",
    "\n",
    "                layers.append(\n",
    "                    block(\n",
    "                        self.in_channels,\n",
    "                        channels,\n",
    "                        groups=self.groups,\n",
    "                        base_width=self.base_width,\n",
    "                        norm=self.norm,\n",
    "                        use_mamba=True\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers.append(\n",
    "                    block(\n",
    "                        self.in_channels,\n",
    "                        channels,\n",
    "                        groups=self.groups,\n",
    "                        base_width=self.base_width,\n",
    "                        norm=self.norm,\n",
    "                        use_mamba=False\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        return nn.SequentialCell(layers)\n",
    "\n",
    "    def forward_features(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Network forward feature extraction.\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x: Tensor) -> Tensor:\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _create_resnet(pretrained=False, **kwargs):\n",
    "    return build_model_with_cfg(ResNet, pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 18 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet18\"]\n",
    "    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 34 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet34\"]\n",
    "    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet50(\n",
    "    pretrained: bool = False,\n",
    "    num_classes: int = 1000,\n",
    "    in_channels: int = 3,\n",
    "    cifar_mode: bool = False,  # 添加CIFAR模式参数\n",
    "    use_mamba: bool = True,    # Mamba开关\n",
    "    **kwargs\n",
    ") -> ResNet:\n",
    "    \"\"\"Get 50 layers ResNet model.\n",
    "    \n",
    "    Args:\n",
    "        pretrained: Whether to download and load the pre-trained model. Default: False.\n",
    "        num_classes: The number of classification. Default: 1000.\n",
    "        in_channels: The input channels. Default: 3.\n",
    "        cifar_mode: Whether to use CIFAR optimized architecture. Default: False.\n",
    "        use_mamba: Whether to use Mamba blocks. Default: True.\n",
    "        \n",
    "    Returns:\n",
    "        ResNet network.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet50\"]\n",
    "    model_args = dict(\n",
    "        block=Bottleneck,\n",
    "        layers=[3, 4, 6, 3],\n",
    "        num_classes=num_classes,\n",
    "        in_channels=in_channels,\n",
    "        cifar_mode=cifar_mode,\n",
    "        use_mamba=use_mamba,\n",
    "        **kwargs\n",
    "    )\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet101\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet152(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 152 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet152\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 50 layers ResNeXt model with 32 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext50_32x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], groups=32, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext101_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNeXt model with 32 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext101_32x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=32, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext101_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNeXt model with 64 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext101_64x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=64, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext152_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    default_cfg = default_cfgs[\"resnext152_64x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], groups=64, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ccb6c-728c-425f-b963-d1d2f4f84014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新减少损伤\n",
    "\"\"\"\n",
    "MindSpore implementation of `ResNet` with Mamba integration.\n",
    "Refer to Deep Residual Learning for Image Recognition.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Optional, Type, Union\n",
    "\n",
    "import mindspore.common.initializer as init\n",
    "from mindspore import Tensor, nn, ops\n",
    "\n",
    "from .helpers import build_model_with_cfg\n",
    "from .layers.pooling import GlobalAvgPooling\n",
    "from .registry import register_model\n",
    "\n",
    "__all__ = [\n",
    "    \"ResNet\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"resnet152\",\n",
    "    \"resnext50_32x4d\",\n",
    "    \"resnext101_32x4d\",\n",
    "    \"resnext101_64x4d\",\n",
    "    \"resnext152_64x4d\",\n",
    "]\n",
    "\n",
    "\n",
    "def _cfg(url=\"\", **kwargs):\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"num_classes\": 1000,\n",
    "        \"first_conv\": \"conv1\",\n",
    "        \"classifier\": \"classifier\",\n",
    "        **kwargs,\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    \"resnet18\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet18-1e65cd21.ckpt\"),\n",
    "    \"resnet34\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet34-f297d27e.ckpt\"),\n",
    "    \"resnet50\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet50-e0733ab8.ckpt\"),\n",
    "    \"resnet101\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet101-689c5e77.ckpt\"),\n",
    "    \"resnet152\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnet/resnet152-beb689d8.ckpt\"),\n",
    "    \"resnext50_32x4d\": _cfg(url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext50_32x4d-af8aba16.ckpt\"),\n",
    "    \"resnext101_32x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext101_32x4d-3c1e9c51.ckpt\"\n",
    "    ),\n",
    "    \"resnext101_64x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext101_64x4d-8929255b.ckpt\"\n",
    "    ),\n",
    "    \"resnext152_64x4d\": _cfg(\n",
    "        url=\"https://download.mindspore.cn/toolkits/mindcv/resnext/resnext152_64x4d-3aba275c.ckpt\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Cell):\n",
    "    \"\"\"define the basic block of resnet\"\"\"\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "        assert groups == 1, \"BasicBlock only supports groups=1\"\n",
    "        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, pad_mode=\"pad\")\n",
    "        self.bn1 = norm(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3,\n",
    "                               stride=1, padding=1, pad_mode=\"pad\")\n",
    "        self.bn2 = norm(channels)\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Cell):\n",
    "    \"\"\"集成Mamba的Bottleneck\"\"\"\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = False,  # 新增参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "\n",
    "        width = int(channels * (base_width / 64.0)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n",
    "        self.bn1 = norm(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, pad_mode=\"pad\", group=groups)\n",
    "        self.bn2 = norm(width)\n",
    "        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n",
    "                               kernel_size=1, stride=1)\n",
    "        self.bn3 = norm(channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # 选择性添加Mamba\n",
    "        self.use_mamba = use_mamba\n",
    "        if use_mamba:\n",
    "            self.mamba = MambaBlock(dim=channels * self.expansion)\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # 在BN3之后，残差连接之前应用Mamba\n",
    "        # 这是Mamba能发挥最大价值的位置\n",
    "        if self.use_mamba:\n",
    "            out = self.mamba(out)\n",
    "        \n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Cell):\n",
    "    \"\"\"高效轻量的Mamba块：专注于不干扰原始网络性能\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # 缩小内部维度，减少参数量和计算量\n",
    "        self.d_inner = max(dim // 4, 32)\n",
    "        \n",
    "        # 输入投影\n",
    "        self.in_proj = nn.Conv2d(dim, self.d_inner, kernel_size=1, \n",
    "                                weight_init=init.HeNormal())\n",
    "        self.in_norm = nn.BatchNorm2d(self.d_inner, gamma_init='zeros')\n",
    "        \n",
    "        # 使用小卷积核，更适合CIFAR数据集\n",
    "        self.spatial = nn.Conv2d(self.d_inner, self.d_inner, kernel_size=3,\n",
    "                                stride=1, padding=1, pad_mode=\"pad\")\n",
    "        self.spatial_norm = nn.BatchNorm2d(self.d_inner)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Conv2d(self.d_inner, dim, kernel_size=1)\n",
    "        self.out_norm = nn.BatchNorm2d(dim, gamma_init='zeros')\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 大幅降低缩放系数，使Mamba的影响更轻微\n",
    "        self.scale = 0.01  # 降低到0.01，减少对原始特征的干扰\n",
    "    \n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        # 标准处理路径\n",
    "        out = self.in_proj(x)\n",
    "        out = self.in_norm(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.spatial(out)\n",
    "        out = self.spatial_norm(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_norm(out)\n",
    "        \n",
    "        # 使用极小的缩放系数\n",
    "        out = identity + self.scale * out\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MambaBottleneck(nn.Cell):\n",
    "    \"\"\"自适应Mamba Bottleneck：根据网络位置动态调整Mamba影响\"\"\"\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        down_sample: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = True,\n",
    "        layer_index: int = 0,  # 添加层索引参数\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "\n",
    "        width = int(channels * (base_width / 64.0)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n",
    "        self.bn1 = norm(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, pad_mode=\"pad\", group=groups)\n",
    "        self.bn2 = norm(width)\n",
    "        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n",
    "                               kernel_size=1, stride=1)\n",
    "        self.bn3 = norm(channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        # 根据层深度选择性添加Mamba\n",
    "        # 浅层用更保守的配置，深层更激进\n",
    "        self.use_mamba = use_mamba\n",
    "        if use_mamba:\n",
    "            self.mamba = MambaBlock(dim=channels * self.expansion)\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # 应用Mamba处理\n",
    "        if self.use_mamba:\n",
    "            out = self.mamba(out)\n",
    "        \n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Cell):\n",
    "    \"\"\"优化的ResNet-Mamba集成架构\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        in_channels: int = 3,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        norm: Optional[nn.Cell] = None,\n",
    "        use_mamba: bool = True,\n",
    "        cifar_mode: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm is None:\n",
    "            norm = nn.BatchNorm2d\n",
    "        \n",
    "        # 只在最深的layer4使用Mamba\n",
    "        self.use_mamba_in_layer = [False, False, False, True] if use_mamba else [False, False, False, False]\n",
    "        \n",
    "        self.norm = norm\n",
    "        self.groups = groups\n",
    "        self.base_width = base_width\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # CIFAR模式使用更小的卷积核和去除最大池化\n",
    "        if cifar_mode:\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, kernel_size=3, stride=1, \n",
    "                                  padding=1, pad_mode=\"pad\")\n",
    "            self.max_pool = nn.Identity()  # 不进行池化\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, self.in_channels, kernel_size=7, stride=2, \n",
    "                                  padding=3, pad_mode=\"pad\")\n",
    "            self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "            \n",
    "        self.bn1 = norm(self.in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 构建网络层\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], use_mamba=self.use_mamba_in_layer[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, use_mamba=self.use_mamba_in_layer[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, use_mamba=self.use_mamba_in_layer[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, use_mamba=self.use_mamba_in_layer[3])\n",
    "        \n",
    "        self.pool = GlobalAvgPooling()\n",
    "        self.num_features = 512 * block.expansion\n",
    "        self.classifier = nn.Dense(self.num_features, num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        \"\"\"Initialize weights for cells.\"\"\"\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, nn.Dense):\n",
    "                cell.weight.set_data(\n",
    "                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n",
    "                )\n",
    "                if cell.bias is not None:\n",
    "                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape, cell.bias.dtype))\n",
    "            elif isinstance(cell, nn.BatchNorm2d):\n",
    "                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape, cell.gamma.dtype))\n",
    "                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape, cell.beta.dtype))\n",
    "            elif isinstance(cell, nn.Conv2d):\n",
    "                cell.weight.set_data(\n",
    "                    init.initializer(init.HeNormal(), cell.weight.shape, cell.weight.dtype)\n",
    "                )\n",
    "                if cell.bias is not None:\n",
    "                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape, cell.bias.dtype))\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        channels: int,\n",
    "        blocks_num: int,\n",
    "        stride: int = 1,\n",
    "        use_mamba: bool = False,\n",
    "    ) -> nn.SequentialCell:\n",
    "        layers = []\n",
    "        # 第一个block通常不使用Mamba，因为它处理下采样\n",
    "        down_sample = None\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            down_sample = nn.SequentialCell([\n",
    "                nn.Conv2d(self.in_channels, channels * block.expansion, kernel_size=1, stride=stride),\n",
    "                self.norm(channels * block.expansion)\n",
    "            ])\n",
    "        \n",
    "        # 第一个block，不使用Mamba\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels,\n",
    "                channels,\n",
    "                stride=stride,\n",
    "                down_sample=down_sample,\n",
    "                groups=self.groups,\n",
    "                base_width=self.base_width,\n",
    "                norm=self.norm,\n",
    "                use_mamba=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.in_channels = channels * block.expansion\n",
    "        \n",
    "        # 对剩余blocks，根据密度决定是否使用Mamba\n",
    "        for i in range(1, blocks_num):\n",
    "            use_mamba_here = use_mamba and i >= 1\n",
    "            \n",
    "            if use_mamba_here:\n",
    "                layers.append(\n",
    "                    block(\n",
    "                        self.in_channels,\n",
    "                        channels,\n",
    "                        groups=self.groups,\n",
    "                        base_width=self.base_width,\n",
    "                        norm=self.norm,\n",
    "                        use_mamba=True\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers.append(\n",
    "                    block(\n",
    "                        self.in_channels,\n",
    "                        channels,\n",
    "                        groups=self.groups,\n",
    "                        base_width=self.base_width,\n",
    "                        norm=self.norm,\n",
    "                        use_mamba=False\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        return nn.SequentialCell(layers)\n",
    "\n",
    "    def forward_features(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Network forward feature extraction.\"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x: Tensor) -> Tensor:\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def construct(self, x: Tensor) -> Tensor:\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _create_resnet(pretrained=False, **kwargs):\n",
    "    return build_model_with_cfg(ResNet, pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 18 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet18\"]\n",
    "    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 34 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet34\"]\n",
    "    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet50(\n",
    "    pretrained: bool = False,\n",
    "    num_classes: int = 1000,\n",
    "    in_channels: int = 3,\n",
    "    cifar_mode: bool = False,  # 添加CIFAR模式参数\n",
    "    use_mamba: bool = True,    # Mamba开关\n",
    "    **kwargs\n",
    ") -> ResNet:\n",
    "    \"\"\"Get 50 layers ResNet model.\n",
    "    \n",
    "    Args:\n",
    "        pretrained: Whether to download and load the pre-trained model. Default: False.\n",
    "        num_classes: The number of classification. Default: 1000.\n",
    "        in_channels: The input channels. Default: 3.\n",
    "        cifar_mode: Whether to use CIFAR optimized architecture. Default: False.\n",
    "        use_mamba: Whether to use Mamba blocks. Default: True.\n",
    "        \n",
    "    Returns:\n",
    "        ResNet network.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet50\"]\n",
    "    model_args = dict(\n",
    "        block=Bottleneck,\n",
    "        layers=[3, 4, 6, 3],\n",
    "        num_classes=num_classes,\n",
    "        in_channels=in_channels,\n",
    "        cifar_mode=cifar_mode,\n",
    "        use_mamba=use_mamba,\n",
    "        **kwargs\n",
    "    )\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet101\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnet152(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 152 layers ResNet model.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnet152\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels,\n",
    "                      **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 50 layers ResNeXt model with 32 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext50_32x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], groups=32, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext101_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNeXt model with 32 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext101_32x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=32, base_width=4, num_classes=num_classes,\n",
    "                      in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext101_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 101 layers ResNeXt model with 64 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext101_64x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=64, base_width=4, num_classes=num_classes,\n",
    "                       in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def resnext152_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n",
    "    \"\"\"Get 152 layers ResNeXt model with 64 groups of GPConv.\n",
    "    Refer to the base class `models.ResNet` for more details.\n",
    "    \"\"\"\n",
    "    default_cfg = default_cfgs[\"resnext152_64x4d\"]\n",
    "    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], groups=64, base_width=4, num_classes=num_classes,\n",
    "                       in_channels=in_channels, **kwargs)\n",
    "    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
